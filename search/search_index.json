{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Pydoxtools Documentation!","text":"<p>For a short overview over Pydoxtools, checkout the readme on the project page:</p> <p>Readme</p>"},{"location":"#introduction","title":"Introduction","text":"<p>Pydoxtools provides a user-friendly interface for document analysis and manipulation, consisting of four main classes:</p> <ul> <li>pydoxtools.Document</li> <li>pydoxtools.DocumentBag</li> <li>pydoxtools.Pipeline</li> <li>pydoxtools.LLMAgent</li> </ul> <p>Additionally, it offers a collection of operators:</p> <ul> <li>pydoxtools.operators</li> </ul>"},{"location":"#here-are-some-examples-to-get-started","title":"Here are some examples to get started:","text":"<ul> <li>Automatic Documenttext writing in under 100 lines:   example</li> <li>TODO: add more examples...</li> </ul>"},{"location":"#analyzing-documents","title":"Analyzing Documents","text":"<p>Both, Document and DocumentBag utilize pydoxtools.Pipeline to define a sophisticated pipeline for extracting data from individual or multiple documents. You can find a list of all the built-in features for each pipeline here:</p> <p>-&gt; pydoxtools.Document and pydoxtools.DocumentBag</p> <p>To ensure seamless operation, Pydoxtools is designed so that Document and DocumentBag automatically organize information in a logical manner while minimizing memory and CPU usage. This approach makes the library highly compatible with AI and LLMs in automated settings. As a result, it is not possible to configure how documents are loaded using configuration parameters. However, you can easily achieve specific data organization by chaining documents together.</p> <p>TODO:  provide an example</p>"},{"location":"#basic-agent-functionality","title":"Basic Agent Functionality","text":"<p>Pydoxtools provides basic LLM Agent functionality out-of-the-box. It provides an Agent class which implements the basic functionality needed to create AI Agents which make use of pydoxtools Pipelines, especially Document and DocumentBag.</p> <p>The agent can be customized, and complicated applications can be created with just a few lines of code. Some examples are:</p> <ul> <li>Automatic Documenttext writing in under 100 lines:   example</li> <li>TODO: add more examples...</li> </ul>"},{"location":"#building-custom-pipelines-with-llms-large-language-models-and-other-ai-tools","title":"Building Custom Pipelines with LLMs (Large Language Models) and other AI Tools","text":"<p>The Pipeline class allows you to create complex, custom pipelines that come with several built-in features, making them easy to integrate with modern AI tools:</p> <ul> <li>Mix, extend, or (partially) overwrite pipelines</li> <li>Export/import data (yaml, json, python-dict)</li> <li>Configure and optimize pipelines</li> <li>Convert data into pydoxtools.Document and pydoxtools.DocumentBag</li> </ul> <p>To develop a custom pipeline, you can utilize the extensive library of pydoxtools.operators. It is generally recommended to use pydoxtools.Document or pydoxtools.DocumentBag as a base for a new pipeline and only replace small parts to achieve the desired custom functionality.</p>"},{"location":"#visualizing-pipelines","title":"Visualizing Pipelines","text":"<p>Visualizing pipelines can be incredibly helpful when developing your own pipeline on top of a complex one, such as the document pipeline. You can visualize the extraction logic for different file types from the Document class (which is a pydoxtools.Pipeline  itself) as follows:</p> <pre><code>doc = Document(fobj=make_path_absolute(\"./data/demo.docx\"))\n# for the currently loaded file type:\ndoc.logic_graph(image_path=settings._PYDOXTOOLS_DIR / \"docs/images/document_logic_docx.svg\")\n# for the \ndoc.logic_graph(image_path=settings._PYDOXTOOLS_DIR / \"docs/images/document_logic_png.svg\", document_logic_id=\".png\")\n</code></pre> <p>This allows you to generate pipelines for various file types:</p> <ul> <li>docx</li> <li>png   (click on links to open the images!)</li> </ul> <p>You can find pipelines for every supported file type here.</p> <p>This feature is also available for custom pipelines!</p> <p>To learn more, continue to: Reference</p>"},{"location":"DEVELOPMENT/","title":"Development &amp; Contribution","text":"<p>The graph model of the library makes it very easy to extend it with new functionality.</p> <ul> <li>the document can be used as a base-model and overwritten with changes</li> <li>the graph can be changed dynamically</li> <li>new functions can be very easily integrated</li> </ul>"},{"location":"DEVELOPMENT/#installation-from-other-branches","title":"Installation from other branches","text":"<p>In order to install pydoxtools from a development branch \"development_branch\" you can do this:</p> <p>pip install -U \"pydoxtools[etl,inference] @ git+https://github.com/xyntopia/pydoxtools.git@development_branch\"</p>"},{"location":"DEVELOPMENT/#pydoxtools-architecture","title":"Pydoxtools Architecture","text":"<p>--&gt; refer to \"document\"</p>"},{"location":"DEVELOPMENT/#contribution-guidelines","title":"Contribution Guidelines","text":""},{"location":"agent/","title":"pydoxtools.LLMAgent","text":""},{"location":"agent/#pydoxtools.agent.LLMAgent.pre_compute_index","title":"<code>pre_compute_index()</code>","text":"<p>Create an index from our datasource by splitting it up into !</p>"},{"location":"agent/#pydoxtools.agent.LLMAgent.reload_vector_store","title":"<code>reload_vector_store()</code>","text":"<p>sometimes we need to reload the vector store. For example if we have added nwe data from a different thread...</p>"},{"location":"document/","title":"pydoxtools.Document","text":"<p>         Bases: <code>Pipeline</code></p> <p>Basic document pipeline class to analyze documents from all kinds of formats.</p> <p>A list and documentation of all document analysis related functions can be found -&gt;here&lt;-.</p> <p>The Document class is designed for information extraction from documents. It inherits from the pydoxtools.document_base.Pipeline class and uses a predefined extraction pipeline focused on document processing tasks. To load a document, create an instance of the Document class with a file path, a file object, a string, a URL or give it some data directly as a dict:</p> <pre><code>from pydoxtools import Document\ndoc = Document(fobj=Path('./data/demo.docx'))\n</code></pre> <p>Extracted data can be accessed by calling the <code>x</code> method with the specified output in the pipeline:</p> <pre><code>doc.x(\"addresses\")\ndoc.x(\"entities\")\ndoc.x(\"full_text\")\n# etc...\n</code></pre> <p>Most members can also be called as normal class attributes for easier readability:</p> <pre><code>doc.addresses\n</code></pre> <p>Additionally, it is possible to get the data directly in dict, yaml or json form:</p> <pre><code>doc.property_dict(\"addresses\",\"filename\",\"keywords\")\ndoc.yaml(\"addresses\",\"filename\",\"keywords\")\ndoc.json(\"addresses\",\"filename\",\"keywords\")\n</code></pre> <p>To retrieve a list of all available extraction data methods, call the <code>x_funcs()</code> method:</p> <pre><code>doc.x_funcs()\n</code></pre>"},{"location":"document/#pydoxtools.Document--customizing-the-document-pipeline","title":"Customizing the Document Pipeline:","text":"<p>The extraction pipeline can be partially overwritten or completely replaced to customize the document processing. To customize the pipeline, it's recommended to use the basic document pipeline defined in <code>pydoxtools.Document</code> as a starting point and only overwrite parts as needed.</p> <p>Inherited classes can override any part of the graph. To exchange, override, extend or introduce extraction pipelines for specific file types (including the generic one: \"\"), such as .html, .pdf, .txt, etc., follow the example below.</p>"},{"location":"document/#pydoxtools.Document--rules-for-customizing-the-extraction-pipeline","title":"Rules for customizing the extraction pipeline:","text":"<ul> <li>The pipeline is defined as a dictionary of several lists of pydoxtools.operator_base.Operator   nodes.</li> <li>Each pydoxtools.operator_base.Operator defines a set of output &amp; input valus through   the <code>out</code> and <code>input</code> methods.   The <code>input</code> method takes a dictionary   or list of input values and the <code>out</code> method takes a dictionary or list of output values.</li> <li>Operator nodes are configured through method chaining.</li> <li>Arguments can be overwritten by a new pipeline in inherited   documents or document types higher up in the hierarchy. The argument precedence is as follows:</li> </ul> <p><code>python-class-member &lt; extractor-graph-function &lt; configuration</code></p> <ul> <li>the different lists in the dictionary represent a \"hierarchy\" of pipelines which   can be combined in different ways by referencing each other. For example the \"png\" pipeline   references the \"image\" pipeline which in turn references the \"pdf\" pipeline. All pipelines   fall back to the \"*\" pipeline which is the most generic one.</li> </ul> <p>The way this looks is like this:</p> <pre><code>_operators = {\n        # .pdf-specific pipeline\n        \"application/pdf\": [*PDFNodes],\n        # image specific pipeline (does OCR)\n        \"image\": [*OCRNodes],\n        # .png-specific pipeline\n        \".png\": [\"image\", \"application/pdf\"],\n        # base pipeline\n        \"*\": [*BaseNodes],\n        }\n\nHere, the \"image\" pipeline overwrites the \"application/pdf\" pipeline for .png files. The \"application/pdf\"\npipeline on the other hand overwrites the \"*\" pipeline for .pdf files. The \"*\" pipeline doesn't not need\nto be specified, as it will always be the fallback pipeline. This way it is possible\nto dynamically adapt a pipeline to different types of input data. In the document pipeline\nthis is used to dynamically adapt the pipeline to different file types.\n\nWhen customizing, it is possible to derive a new class from [pydoxtools.Document][] and partially\noverwrite its hierarchy for your purposes. For example, if you want to add a new pipeline for Component\nextraction one could do something like the following: This will add a few more nodes\nto the generic pipeline in order to extract product information from documents. This would\nnow already work for all the document types defined in the base [pydoxtools.Document][] class!\n\n```python\nclass DocumentX(pydoxtools.Document):\n_operators = {\n    \"*\": [\n        FunctionOperator(get_products_from_pages).input(\"page_templates\", pages=\"page_set\")\n        .out(product_information=\"product_information\").cache(),\n        FunctionOperator(lambda tables: [componardo.spec_utils.table2specs(t) for t in tables])\n        .input(\"tables\").out(\"raw_specs\")\n        .cache().t(list[componardo.spec_utils.Specification])\n        .docs(\"transform tables into a list of specs\"),\n        FunctionOperator(lambda x: [ComponentExtractor([x]).component])\n        .t(componardo.extract_product.ComponentX)\n        .input(x=\"document_extract\").out(\"products\").cache()\n        .docs(\"Extract products from Documents\"),\n    ]\n}\n```\n</code></pre> <p>When creating a new pipeline for documentation purposes, use a function or class for complex operations and include the documentation there. Lambda functions should not be used in this case.</p>"},{"location":"document/#pydoxtools.document.Document.document_type","title":"<code>document_type</code>  <code>property</code>","text":"<p>This has to be done in a member function and not in the pipeline, because the selection of the pipeline depends on this...</p>"},{"location":"document/#pydoxtools.document.Document.filename","title":"<code>filename: str | None</code>  <code>property</code>","text":"<p>return filename or some other identifier of a file</p>"},{"location":"document/#pydoxtools.document.Document.__init__","title":"<code>__init__(fobj=None, source=None, meta=None, document_type='auto', page_numbers=None, max_pages=None, configuration=None, **kwargs)</code>","text":"<p>Initialize a Document instance.</p> <p>Either fobj or source are required. They can both be given. If either of them isn't specified the other one is inferred automatically.</p> <p>document_type, page_number and max_pages are also not required, but can be used to override the default behaviour. specifically document_tgiype can be used manually specify the pipeline that should be used.</p> <p>Parameters:</p> Name Type Description Default <code>fobj</code> <code>str | bytes | Path | IO | dict | list | set</code> <p>The file object or data to load. Depending on the type of object passed: - If a string or bytes object: the object itself is the document. IN case of a bytes      object, the source helps in determining the filetype through file endings. - If a string representing a URL: the document will be loaded from the URL. - If a pathlib.Path object: load the document from the path. - If a file object: load the document from the file object (e.g., bytestream). - If a python dict object: interprete a \"dict\" as a document - If a python list object: interprete a \"list\" as a document</p> <code>None</code> <code>source</code> <code>str | Path</code> <p>The source of the extracted data (e.g., URL, 'pdfupload', parent-URL, or a path). source is given in addition to fobj it overrides the automatically inferred source. A special case applies if our document is a dataobject from a database. In that case the index key from the database should be used as source. This facilitates downstream tasks immensely where we have to refer back to where the data came from.</p> <p>This also applies for \"explode\" operations on documents where the newly created documents will all try to trace their origin using the \"source\" attribute</p> <code>None</code> <code>document_type</code> <code>str</code> <p>The document type to directly specify the pipeline to be used. If \"auto\" is given it will try to be inferred automatically. For example in some cases we would like to have a string given in fobj not to be loaded as a file but actually be used as raw \"string\" data. In this case we can explicitly specify document_type=\"string\"</p> <code>'auto'</code> <code>meta</code> <code>dict[str, str]</code> <p>Optionally set document metadata, which can be very useful for downstream tasks like building an index.</p> <code>None</code> <code>page_numbers</code> <code>list[int]</code> <p>A list of specific pages to extract from the document (e.g., in a PDF).</p> <code>None</code> <code>max_pages</code> <code>int</code> <p>The maximum number of pages to extract to protect resources.</p> <code>None</code> <code>configuration</code> <code>dict</code> <p>configuration dictionary for the pipeline</p> <code>None</code>"},{"location":"document/#pydoxtools.document.Document.__repr__","title":"<code>__repr__()</code>","text":"<p>Returns:</p> Name Type Description <code>str</code> <p>A string representation of the instance.</p>"},{"location":"document/#pydoxtools.document.Document.document_type_detection","title":"<code>document_type_detection()</code>  <code>cached</code>","text":"<p>This one here is actually important as it detects the type of data that we are going to use for out pipeline. That is also why this is implemented as a member function and can not be pushed in the pipeline itself, because in needs to be run in order to select which pipline we are going to use.</p> <p>detect doc type based on various criteria TODO add a doc-type extractor using for example python-magic</p>"},{"location":"document/#text-extraction-attributes-and-functions","title":"Text extraction attributes and functions","text":"<p>The pydoxtools.Document is built on the pydoxtools.Pipeline class and most of the text extraction functionality makes extensive use of the pipeline features. All attributes and functions that are created by the pipeline are documented here.</p> <p>Pipeline visualizations for the structure of the Document pipelines for different document types can be found here.</p>"},{"location":"document/#data","title":"data","text":"<p>The unprocessed data.</p> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('data')\n# or\n&lt;Document&gt;.data\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#page_set","title":"page_set","text":"<p>A constant value</p> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('page_set')\n# or\n&lt;Document&gt;.page_set\n</code></pre> <p>return type : set[int] | typing.Any</p> <p>supports pipeline flows: : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#full_text","title":"full_text","text":"<p>Full text as a string value</p> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('full_text')\n# or\n&lt;Document&gt;.full_text\n</code></pre> <p>return type : &lt;class 'str'&gt; | typing.Any</p> <p>supports pipeline flows: : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#clean_text","title":"clean_text","text":"<p>Alias for: </p> <ul> <li>full_text-&gt;clean_text (output)</li> </ul> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('clean_text')\n# or\n&lt;Document&gt;.clean_text\n</code></pre> <p>return type : &lt;class 'str'&gt; | typing.Any</p> <p>supports pipeline flows: : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#meta","title":"meta","text":"<p>Metadata of the document</p> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('meta')\n# or\n&lt;Document&gt;.meta\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#page_templates","title":"page_templates","text":"<p>create page templates from pandoc documents. This is a temporary workaround  as we do not have the correct page numbers implemented yet.</p> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('page_templates')\n# or\n&lt;Document&gt;.page_templates\n</code></pre> <p>return type : dict[int, str] | typing.Any</p> <p>supports pipeline flows: : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#file_meta","title":"file_meta","text":"<p>Some fast-to-calculate metadata information about a document</p> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('file_meta')\n# or\n&lt;Document&gt;.file_meta\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#text_box_elements","title":"text_box_elements","text":"<p>Text boxes extracted as a pandas Dataframe with some additional metadata</p> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('text_box_elements')\n# or\n&lt;Document&gt;.text_box_elements\n</code></pre> <p>return type : &lt;class 'pandas.core.frame.DataFrame'&gt; | pandas.core.frame.DataFrame | None | typing.Any</p> <p>supports pipeline flows: : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#text_box_list","title":"text_box_list","text":"<p>Text boxes as a list</p> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('text_box_list')\n# or\n&lt;Document&gt;.text_box_list\n</code></pre> <p>return type : list[str] | typing.Any</p> <p>supports pipeline flows: : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#tables_df","title":"tables_df","text":"<p>A constant value</p> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('tables_df')\n# or\n&lt;Document&gt;.tables_df\n</code></pre> <p>return type : list[pandas.core.frame.DataFrame] | str | list[str] | list[pandas.core.frame.DataFrame] | typing.Any</p> <p>supports pipeline flows: : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#tables_dict","title":"tables_dict","text":"<p>List of Table</p> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('tables_dict')\n# or\n&lt;Document&gt;.tables_dict\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#tables","title":"tables","text":"<p>Alias for: </p> <ul> <li>tables_dict-&gt;tables (output)</li> </ul> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('tables')\n# or\n&lt;Document&gt;.tables\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#addresses","title":"addresses","text":"<p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('addresses')\n# or\n&lt;Document&gt;.addresses\n</code></pre> <p>return type : list[str]</p> <p>supports pipeline flows: : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#page_classifier","title":"page_classifier","text":"<p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('page_classifier')\n# or\n&lt;Document&gt;.page_classifier\n</code></pre> <p>return type : typing.Callable[[list[str]], pandas.core.frame.DataFrame]</p> <p>supports pipeline flows: : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#num_pages","title":"num_pages","text":"<p>No documentation</p> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('num_pages')\n# or\n&lt;Document&gt;.num_pages\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#num_words","title":"num_words","text":"<p>No documentation</p> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('num_words')\n# or\n&lt;Document&gt;.num_words\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#num_sents","title":"num_sents","text":"<p>number of sentences</p> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('num_sents')\n# or\n&lt;Document&gt;.num_sents\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#a_d_ratio","title":"a_d_ratio","text":"<p>Letter/digit ratio of the text</p> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('a_d_ratio')\n# or\n&lt;Document&gt;.a_d_ratio\n</code></pre> <p>return type : &lt;class 'float'&gt;</p> <p>supports pipeline flows: : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#language","title":"language","text":"<p>Detect language of a document, return 'unknown' in case of an error</p> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('language')\n# or\n&lt;Document&gt;.language\n</code></pre> <p>return type : &lt;class 'str'&gt; | typing.Any</p> <p>supports pipeline flows: : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#spacy_model_size","title":"spacy_model_size","text":"<p>Configuration for values:</p> <ul> <li>spacy_model_size = md (default)</li> <li>spacy_model = auto (default)</li> </ul> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('spacy_model_size')\n# or\n&lt;Document&gt;.spacy_model_size\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#spacy_model","title":"spacy_model","text":"<p>Configuration for values:</p> <ul> <li>spacy_model_size = md (default)</li> <li>spacy_model = auto (default)</li> </ul> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('spacy_model')\n# or\n&lt;Document&gt;.spacy_model\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#spacy_doc","title":"spacy_doc","text":"<p>Spacy Document and Language Model for this document</p> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('spacy_doc')\n# or\n&lt;Document&gt;.spacy_doc\n</code></pre> <p>return type : spacy.language.Language | spacy.tokens.doc.Doc</p> <p>supports pipeline flows: : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#spacy_nlp","title":"spacy_nlp","text":"<p>Spacy Document and Language Model for this document</p> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('spacy_nlp')\n# or\n&lt;Document&gt;.spacy_nlp\n</code></pre> <p>return type : spacy.language.Language | spacy.tokens.doc.Doc</p> <p>supports pipeline flows: : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#spacy_vectors","title":"spacy_vectors","text":"<p>Vectors for all tokens calculated by spacy</p> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('spacy_vectors')\n# or\n&lt;Document&gt;.spacy_vectors\n</code></pre> <p>return type : typing.Union[torch.Tensor, typing.Any]</p> <p>supports pipeline flows: : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#spacy_embeddings","title":"spacy_embeddings","text":"<p>Embeddings calculated by a spacy transformer</p> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('spacy_embeddings')\n# or\n&lt;Document&gt;.spacy_embeddings\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#spacy_sents","title":"spacy_sents","text":"<p>List of sentences by spacy nlp framework</p> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('spacy_sents')\n# or\n&lt;Document&gt;.spacy_sents\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#spacy_noun_chunks","title":"spacy_noun_chunks","text":"<p>exracts nounchunks from spacy. Will not be cached because it is allin the spacy doc already</p> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('spacy_noun_chunks')\n# or\n&lt;Document&gt;.spacy_noun_chunks\n</code></pre> <p>return type : typing.List[pydoxtools.document_base.TokenCollection]</p> <p>supports pipeline flows: : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#entities","title":"entities","text":"<p>Extract entities from text</p> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('entities')\n# or\n&lt;Document&gt;.entities\n</code></pre> <p>return type : list[str]</p> <p>supports pipeline flows: : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#url","title":"url","text":"<p>Url of this document</p> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('url')\n# or\n&lt;Document&gt;.url\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#relationships","title":"relationships","text":"<p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('relationships')\n# or\n&lt;Document&gt;.relationships\n</code></pre> <p>return type : &lt;class 'pandas.core.frame.DataFrame'&gt;</p> <p>supports pipeline flows: : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#coreference_method","title":"coreference_method","text":"<p>can be 'fast' or 'accurate'</p> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('coreference_method')\n# or\n&lt;Document&gt;.coreference_method\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#graph_debug_context_size","title":"graph_debug_context_size","text":"<p>can be 'fast' or 'accurate'</p> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('graph_debug_context_size')\n# or\n&lt;Document&gt;.graph_debug_context_size\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#coreferences","title":"coreferences","text":"<p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('coreferences')\n# or\n&lt;Document&gt;.coreferences\n</code></pre> <p>return type : list[list[tuple[int, int]]]</p> <p>supports pipeline flows: : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#graph_nodes","title":"graph_nodes","text":"<p>No documentation</p> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('graph_nodes')\n# or\n&lt;Document&gt;.graph_nodes\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#node_map","title":"node_map","text":"<p>No documentation</p> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('node_map')\n# or\n&lt;Document&gt;.node_map\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#graph_edges","title":"graph_edges","text":"<p>No documentation</p> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('graph_edges')\n# or\n&lt;Document&gt;.graph_edges\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#knowledge_graph","title":"knowledge_graph","text":"<p>No documentation</p> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('knowledge_graph')\n# or\n&lt;Document&gt;.knowledge_graph\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#sents","title":"sents","text":"<p>Alias for: </p> <ul> <li>spacy_sents-&gt;sents (output)</li> </ul> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('sents')\n# or\n&lt;Document&gt;.sents\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#noun_chunks","title":"noun_chunks","text":"<p>Alias for: </p> <ul> <li>spacy_noun_chunks-&gt;noun_chunks (output)</li> </ul> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('noun_chunks')\n# or\n&lt;Document&gt;.noun_chunks\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#vector","title":"vector","text":"<p>Embeddings from spacy</p> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('vector')\n# or\n&lt;Document&gt;.vector\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#sent_vecs","title":"sent_vecs","text":"<p>Vectors for sentences &amp; sentence_ids</p> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('sent_vecs')\n# or\n&lt;Document&gt;.sent_vecs\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#sent_ids","title":"sent_ids","text":"<p>Vectors for sentences &amp; sentence_ids</p> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('sent_ids')\n# or\n&lt;Document&gt;.sent_ids\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#noun_vecs","title":"noun_vecs","text":"<p>Vectors for nouns and corresponding noun ids in order to find them in the spacy document</p> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('noun_vecs')\n# or\n&lt;Document&gt;.noun_vecs\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#noun_ids","title":"noun_ids","text":"<p>Vectors for nouns and corresponding noun ids in order to find them in the spacy document</p> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('noun_ids')\n# or\n&lt;Document&gt;.noun_ids\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#vectorizer_model","title":"vectorizer_model","text":"<p>Choose the embeddings model (huggingface-style) and if we wantto do the vectorization using only the tokenizer. Using only thetokenizer is MUCH faster and uses lower CPU than creating actualcontextual embeddings using the model. BUt is also lower qualitybecause it lacks the context.</p> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('vectorizer_model')\n# or\n&lt;Document&gt;.vectorizer_model\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#vectorizer_only_tokenizer","title":"vectorizer_only_tokenizer","text":"<p>Choose the embeddings model (huggingface-style) and if we wantto do the vectorization using only the tokenizer. Using only thetokenizer is MUCH faster and uses lower CPU than creating actualcontextual embeddings using the model. BUt is also lower qualitybecause it lacks the context.</p> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('vectorizer_only_tokenizer')\n# or\n&lt;Document&gt;.vectorizer_only_tokenizer\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#vectorizer_overlap_ratio","title":"vectorizer_overlap_ratio","text":"<p>Choose the embeddings model (huggingface-style) and if we wantto do the vectorization using only the tokenizer. Using only thetokenizer is MUCH faster and uses lower CPU than creating actualcontextual embeddings using the model. BUt is also lower qualitybecause it lacks the context.</p> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('vectorizer_overlap_ratio')\n# or\n&lt;Document&gt;.vectorizer_overlap_ratio\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#vectorizer","title":"vectorizer","text":"<p>No documentation</p> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('vectorizer')\n# or\n&lt;Document&gt;.vectorizer\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#vec_res","title":"vec_res","text":"<p>Calculate context-based vectors for the entire text</p> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('vec_res')\n# or\n&lt;Document&gt;.vec_res\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#tok_embeddings","title":"tok_embeddings","text":"<p>Get the tokenized text</p> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('tok_embeddings')\n# or\n&lt;Document&gt;.tok_embeddings\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#tokens","title":"tokens","text":"<p>Get the tokenized text</p> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('tokens')\n# or\n&lt;Document&gt;.tokens\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#embedding","title":"embedding","text":"<p>Get an embedding for the entire text</p> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('embedding')\n# or\n&lt;Document&gt;.embedding\n</code></pre> <p>return type : list[float]</p> <p>supports pipeline flows: : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#min_size_text_segment","title":"min_size_text_segment","text":"<p>controls the text segmentation for knowledge basesoverlap is only relevant for large text segmenets that need tobe split up into smaller pieces.</p> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('min_size_text_segment')\n# or\n&lt;Document&gt;.min_size_text_segment\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#max_size_text_segment","title":"max_size_text_segment","text":"<p>controls the text segmentation for knowledge basesoverlap is only relevant for large text segmenets that need tobe split up into smaller pieces.</p> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('max_size_text_segment')\n# or\n&lt;Document&gt;.max_size_text_segment\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#text_segment_overlap","title":"text_segment_overlap","text":"<p>controls the text segmentation for knowledge basesoverlap is only relevant for large text segmenets that need tobe split up into smaller pieces.</p> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('text_segment_overlap')\n# or\n&lt;Document&gt;.text_segment_overlap\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#max_text_segment_num","title":"max_text_segment_num","text":"<p>controls the text segmentation for knowledge basesoverlap is only relevant for large text segmenets that need tobe split up into smaller pieces.</p> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('max_text_segment_num')\n# or\n&lt;Document&gt;.max_text_segment_num\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#text_segments","title":"text_segments","text":"<p>Extract small pieces of text with a few more rules     to make them more usable in an index.</p> <pre><code>For example, it is a good idea to make text-pieces not too small. Text pieces should\nalso not bee too big. So that they encode not too much information\ninto the vector. This makes queries on an index more precise.\nAdditionally, we should try to segment a piece of text into its\nlogical structure and try to preserve text blocks such as paragraphs,\ntables etc... as much as possible.\n\nif we split up large text blocks we will let the individual pieces overlap\njust a little bit in order to preserve some of the context.\n</code></pre> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('text_segments')\n# or\n&lt;Document&gt;.text_segments\n</code></pre> <p>return type : list[str] | typing.Any</p> <p>supports pipeline flows: : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#text_segment_vec_res","title":"text_segment_vec_res","text":"<p>Take a function and apply it elementwise to     an iterable. Return a list or iterator.</p> <pre><code>the \"elements\" argument will be evaluated\nelement-wise. You can specify additional arguments for the\nfunction using *args and **kwargs.\n</code></pre> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('text_segment_vec_res')\n# or\n&lt;Document&gt;.text_segment_vec_res\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#text_segment_vecs","title":"text_segment_vecs","text":"<p>No documentation</p> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('text_segment_vecs')\n# or\n&lt;Document&gt;.text_segment_vecs\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#text_segment_ids","title":"text_segment_ids","text":"<p>No documentation</p> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('text_segment_ids')\n# or\n&lt;Document&gt;.text_segment_ids\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#text_segment_index","title":"text_segment_index","text":"<p>Class extracts an index form a document     TODO: make it flexible which kind of index we can use for this :).</p> <pre><code>--&gt; for example we could also use a scikit-learn knn index or some brute-force method\n    etc....\n</code></pre> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('text_segment_index')\n# or\n&lt;Document&gt;.text_segment_index\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#segment_query","title":"segment_query","text":"<p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('segment_query')\n# or\n&lt;Document&gt;.segment_query\n</code></pre> <p>return type : typing.Callable[..., list[tuple]]</p> <p>supports pipeline flows: : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#noun_index","title":"noun_index","text":"<p>Class extracts an index form a document     TODO: make it flexible which kind of index we can use for this :).</p> <pre><code>--&gt; for example we could also use a scikit-learn knn index or some brute-force method\n    etc....\n</code></pre> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('noun_index')\n# or\n&lt;Document&gt;.noun_index\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#spacy_vectorizer","title":"spacy_vectorizer","text":"<p>No documentation</p> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('spacy_vectorizer')\n# or\n&lt;Document&gt;.spacy_vectorizer\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#noun_query","title":"noun_query","text":"<p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('noun_query')\n# or\n&lt;Document&gt;.noun_query\n</code></pre> <p>return type : typing.Callable[..., list[tuple]]</p> <p>supports pipeline flows: : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#noun_graph","title":"noun_graph","text":"<p>this function buils a \"directed similarity graph\" by taking the similarity of words in a document      and connecting tokens which are similar. This can then be used for further analysis      such as textrank (wordranks, sentence ranks, paragraph ranking) etc...</p> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('noun_graph')\n# or\n&lt;Document&gt;.noun_graph\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#top_k_text_rank_keywords","title":"top_k_text_rank_keywords","text":"<p>Configuration for values:</p> <ul> <li>top_k_text_rank_keywords = 5 (default)</li> </ul> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('top_k_text_rank_keywords')\n# or\n&lt;Document&gt;.top_k_text_rank_keywords\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#textrank_keywords","title":"textrank_keywords","text":"<p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('textrank_keywords')\n# or\n&lt;Document&gt;.textrank_keywords\n</code></pre> <p>return type : set[str]</p> <p>supports pipeline flows: : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#keywords","title":"keywords","text":"<p>Alias for: </p> <ul> <li>textrank_keywords-&gt;keywords (output)</li> </ul> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('keywords')\n# or\n&lt;Document&gt;.keywords\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#sent_index","title":"sent_index","text":"<p>Class extracts an index form a document     TODO: make it flexible which kind of index we can use for this :).</p> <pre><code>--&gt; for example we could also use a scikit-learn knn index or some brute-force method\n    etc....\n</code></pre> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('sent_index')\n# or\n&lt;Document&gt;.sent_index\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#sent_query","title":"sent_query","text":"<p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('sent_query')\n# or\n&lt;Document&gt;.sent_query\n</code></pre> <p>return type : typing.Callable[..., list[tuple]]</p> <p>supports pipeline flows: : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#sent_graph","title":"sent_graph","text":"<p>this function buils a \"directed similarity graph\" by taking the similarity of words in a document      and connecting tokens which are similar. This can then be used for further analysis      such as textrank (wordranks, sentence ranks, paragraph ranking) etc...</p> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('sent_graph')\n# or\n&lt;Document&gt;.sent_graph\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#top_k_text_rank_sentences","title":"top_k_text_rank_sentences","text":"<p>Configuration for values:</p> <ul> <li>top_k_text_rank_sentences = 5 (default)</li> </ul> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('top_k_text_rank_sentences')\n# or\n&lt;Document&gt;.top_k_text_rank_sentences\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#textrank_sents","title":"textrank_sents","text":"<p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('textrank_sents')\n# or\n&lt;Document&gt;.textrank_sents\n</code></pre> <p>return type : set[str]</p> <p>supports pipeline flows: : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#summarizer_model","title":"summarizer_model","text":"<p>Configuration for values:</p> <ul> <li>summarizer_model = sshleifer/distilbart-cnn-12-6 (default)</li> <li>summarizer_token_overlap = 50 (default)</li> <li>summarizer_max_text_len = 200 (default)</li> </ul> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('summarizer_model')\n# or\n&lt;Document&gt;.summarizer_model\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#summarizer_token_overlap","title":"summarizer_token_overlap","text":"<p>Configuration for values:</p> <ul> <li>summarizer_model = sshleifer/distilbart-cnn-12-6 (default)</li> <li>summarizer_token_overlap = 50 (default)</li> <li>summarizer_max_text_len = 200 (default)</li> </ul> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('summarizer_token_overlap')\n# or\n&lt;Document&gt;.summarizer_token_overlap\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#summarizer_max_text_len","title":"summarizer_max_text_len","text":"<p>Configuration for values:</p> <ul> <li>summarizer_model = sshleifer/distilbart-cnn-12-6 (default)</li> <li>summarizer_token_overlap = 50 (default)</li> <li>summarizer_max_text_len = 200 (default)</li> </ul> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('summarizer_max_text_len')\n# or\n&lt;Document&gt;.summarizer_max_text_len\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#slow_summary","title":"slow_summary","text":"<p>No documentation</p> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('slow_summary')\n# or\n&lt;Document&gt;.slow_summary\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#qam_model_id","title":"qam_model_id","text":"<p>Configuration for values:</p> <ul> <li>qam_model_id = deepset/minilm-uncased-squad2 (default)</li> </ul> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('qam_model_id')\n# or\n&lt;Document&gt;.qam_model_id\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#answers","title":"answers","text":"<p>Question Answering Machine Operator</p> <pre><code>The Operator generates a function takes questions and gives back\nanswers on the given text.\n</code></pre> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('answers')\n# or\n&lt;Document&gt;.answers\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#chat_model_id","title":"chat_model_id","text":"<p>In order to use openai-chatgpt, you can use 'gpt-3.5-turbo' or 'gpt-4'.Additionally, we support models used by gpt4all library whichcan be run locally and most are available for commercial purposes. Currently available models are: ['wizardlm-13b-v1.1-superhot-8k.ggmlv3.q4_0', 'ggml-model-gpt4all-falcon-q4_0', 'ous-hermes-13b.ggmlv3.q4_0', 'GPT4All-13B-snoozy.ggmlv3.q4_0', 'orca-mini-7b.ggmlv3.q4_0', 'orca-mini-3b.ggmlv3.q4_0', 'orca-mini-13b.ggmlv3.q4_0', 'wizardLM-13B-Uncensored.ggmlv3.q4_0', 'ggml-replit-code-v1-3', 'ggml-all-MiniLM-L6-v2-f16', 'starcoderbase-3b-ggml', 'starcoderbase-7b-ggml', 'llama-2-7b-chat.ggmlv3.q4_0']</p> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('chat_model_id')\n# or\n&lt;Document&gt;.chat_model_id\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#chat_answers","title":"chat_answers","text":"<p>Use LLMChat on data in our pipeline!</p> <pre><code>model_id: if model_language==\"auto\" we also need to set our model_size\n</code></pre> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('chat_answers')\n# or\n&lt;Document&gt;.chat_answers\n</code></pre> <p>return type : typing.Callable[[list[str], list[str] | str], list[str]]</p> <p>supports pipeline flows: : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#meta_pdf","title":"meta_pdf","text":"<p>Extract metadata from pdf</p> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('meta_pdf')\n# or\n&lt;Document&gt;.meta_pdf\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : PIL.Image.Image, application/pdf, image, image/jpeg, image/png, image/tiff</p>"},{"location":"document/#pages_bbox","title":"pages_bbox","text":"<p>Return the 'mediabox' property of the pdf page which gives the size of the page of a pdf in 72 dpi, which is the standard unit of measurement in pdfs.</p> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('pages_bbox')\n# or\n&lt;Document&gt;.pages_bbox\n</code></pre> <p>return type : &lt;class 'numpy.ndarray'&gt;</p> <p>supports pipeline flows: : PIL.Image.Image, application/pdf, image, image/jpeg, image/png, image/tiff</p>"},{"location":"document/#elements","title":"elements","text":"<p>Extract a list of textelements from pdf: Textlines, Graphics, Figures</p> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('elements')\n# or\n&lt;Document&gt;.elements\n</code></pre> <p>return type : &lt;class 'pandas.core.frame.DataFrame'&gt;</p> <p>supports pipeline flows: : PIL.Image.Image, application/pdf, image, image/jpeg, image/png, image/tiff</p>"},{"location":"document/#image_dpi","title":"image_dpi","text":"<p>The dpi when rendering the document. The standard image generation resolution is set to 216 dpi for pdfs as we want to have sufficient DPI for downstram OCR tasks (e.g. table extraction)</p> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('image_dpi')\n# or\n&lt;Document&gt;.image_dpi\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : PIL.Image.Image, application/pdf, image, image/jpeg, image/png, image/tiff</p>"},{"location":"document/#images","title":"images","text":"<p>No documentation</p> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('images')\n# or\n&lt;Document&gt;.images\n</code></pre> <p>return type : dict[&lt;module 'PIL.Image' from '/home/tom/.cache/pypoetry/virtualenvs/pydoxtools-UuJZOkke-py3.10/lib/python3.10/site-packages/PIL/Image.py'&gt;] | typing.Any</p> <p>supports pipeline flows: : PIL.Image.Image, application/pdf, image, image/jpeg, image/png, image/tiff</p>"},{"location":"document/#line_elements","title":"line_elements","text":"<p>Filter document elements for various criteria</p> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('line_elements')\n# or\n&lt;Document&gt;.line_elements\n</code></pre> <p>return type : &lt;class 'pandas.core.frame.DataFrame'&gt;</p> <p>supports pipeline flows: : PIL.Image.Image, application/pdf, image, image/jpeg, image/png, image/tiff</p>"},{"location":"document/#graphic_elements","title":"graphic_elements","text":"<p>Filter document elements for various criteria</p> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('graphic_elements')\n# or\n&lt;Document&gt;.graphic_elements\n</code></pre> <p>return type : &lt;class 'pandas.core.frame.DataFrame'&gt;</p> <p>supports pipeline flows: : PIL.Image.Image, application/pdf, image, image/jpeg, image/png, image/tiff</p>"},{"location":"document/#image_elements","title":"image_elements","text":"<p>Filter document elements for various criteria</p> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('image_elements')\n# or\n&lt;Document&gt;.image_elements\n</code></pre> <p>return type : &lt;class 'pandas.core.frame.DataFrame'&gt;</p> <p>supports pipeline flows: : PIL.Image.Image, application/pdf, image, image/jpeg, image/png, image/tiff</p>"},{"location":"document/#lists","title":"lists","text":"<p>Extract lines that might be part of a \"list\".</p> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('lists')\n# or\n&lt;Document&gt;.lists\n</code></pre> <p>return type : &lt;class 'pandas.core.frame.DataFrame'&gt; | str | list[str] | list[pandas.core.frame.DataFrame]</p> <p>supports pipeline flows: : PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/markdown, text/rtf</p>"},{"location":"document/#table_box_levels","title":"table_box_levels","text":"<p>produces a list of potential table objects</p> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('table_box_levels')\n# or\n&lt;Document&gt;.table_box_levels\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : PIL.Image.Image, application/pdf, image, image/jpeg, image/png, image/tiff</p>"},{"location":"document/#table_candidates","title":"table_candidates","text":"<p>produces a list of potential table objects</p> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('table_candidates')\n# or\n&lt;Document&gt;.table_candidates\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : PIL.Image.Image, application/pdf, image, image/jpeg, image/png, image/tiff</p>"},{"location":"document/#valid_tables","title":"valid_tables","text":"<p>No documentation</p> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('valid_tables')\n# or\n&lt;Document&gt;.valid_tables\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : PIL.Image.Image, application/pdf, image, image/jpeg, image/png, image/tiff</p>"},{"location":"document/#table_df0","title":"table_df0","text":"<p>Filter valid tables from table candidates by looking if meaningful values can be extracted</p> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('table_df0')\n# or\n&lt;Document&gt;.table_df0\n</code></pre> <p>return type : list[pandas.core.frame.DataFrame]</p> <p>supports pipeline flows: : PIL.Image.Image, application/pdf, image, image/jpeg, image/png, image/tiff</p>"},{"location":"document/#table_areas","title":"table_areas","text":"<p>Areas of all detected tables</p> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('table_areas')\n# or\n&lt;Document&gt;.table_areas\n</code></pre> <p>return type : list[numpy.ndarray]</p> <p>supports pipeline flows: : PIL.Image.Image, application/pdf, image, image/jpeg, image/png, image/tiff</p>"},{"location":"document/#titles","title":"titles","text":"<p>This Operator extracts titels and other interesting text parts     from a visual document. It does this by characterising parts     of the text being \"different\" than the rest using an     Isolation Forest algorithm (anomyla detection).     Features are for example: font size,     position, length etc...</p> <pre><code>#TODO: use this for html and other kinds of text files as well...\n</code></pre> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('titles')\n# or\n&lt;Document&gt;.titles\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : PIL.Image.Image, application/pdf, image, image/jpeg, image/png, image/tiff, text/html</p>"},{"location":"document/#side_titles","title":"side_titles","text":"<p>This Operator extracts titels and other interesting text parts     from a visual document. It does this by characterising parts     of the text being \"different\" than the rest using an     Isolation Forest algorithm (anomyla detection).     Features are for example: font size,     position, length etc...</p> <pre><code>#TODO: use this for html and other kinds of text files as well...\n</code></pre> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('side_titles')\n# or\n&lt;Document&gt;.side_titles\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : PIL.Image.Image, application/pdf, image, image/jpeg, image/png, image/tiff</p>"},{"location":"document/#document_objects","title":"document_objects","text":"<p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('document_objects')\n# or\n&lt;Document&gt;.document_objects\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : PIL.Image.Image, application/pdf, image, image/jpeg, image/png, image/tiff</p>"},{"location":"document/#page_templates_str","title":"page_templates_str","text":"<p>No documentation</p> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('page_templates_str')\n# or\n&lt;Document&gt;.page_templates_str\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : PIL.Image.Image, application/pdf, image, image/jpeg, image/png, image/tiff</p>"},{"location":"document/#table_context","title":"table_context","text":"<p>No documentation</p> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('table_context')\n# or\n&lt;Document&gt;.table_context\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : PIL.Image.Image, application/pdf, image, image/jpeg, image/png, image/tiff</p>"},{"location":"document/#html_keywords_str","title":"html_keywords_str","text":"<p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('html_keywords_str')\n# or\n&lt;Document&gt;.html_keywords_str\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : text/html</p>"},{"location":"document/#main_content_clean_html","title":"main_content_clean_html","text":"<p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('main_content_clean_html')\n# or\n&lt;Document&gt;.main_content_clean_html\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : text/html</p>"},{"location":"document/#summary","title":"summary","text":"<p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('summary')\n# or\n&lt;Document&gt;.summary\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : text/html</p>"},{"location":"document/#goose_article","title":"goose_article","text":"<p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('goose_article')\n# or\n&lt;Document&gt;.goose_article\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : text/html</p>"},{"location":"document/#main_content","title":"main_content","text":"<p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('main_content')\n# or\n&lt;Document&gt;.main_content\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : text/html</p>"},{"location":"document/#schemadata","title":"schemadata","text":"<p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('schemadata')\n# or\n&lt;Document&gt;.schemadata\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : text/html</p>"},{"location":"document/#final_urls","title":"final_urls","text":"<p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('final_urls')\n# or\n&lt;Document&gt;.final_urls\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : text/html</p>"},{"location":"document/#pdf_links","title":"pdf_links","text":"<p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('pdf_links')\n# or\n&lt;Document&gt;.pdf_links\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : text/html</p>"},{"location":"document/#title","title":"title","text":"<p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('title')\n# or\n&lt;Document&gt;.title\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : text/html</p>"},{"location":"document/#short_title","title":"short_title","text":"<p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('short_title')\n# or\n&lt;Document&gt;.short_title\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : text/html</p>"},{"location":"document/#urls","title":"urls","text":"<p>No documentation</p> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('urls')\n# or\n&lt;Document&gt;.urls\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : text/html</p>"},{"location":"document/#main_image","title":"main_image","text":"<p>No documentation</p> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('main_image')\n# or\n&lt;Document&gt;.main_image\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : text/html</p>"},{"location":"document/#html_keywords","title":"html_keywords","text":"<p>No documentation</p> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('html_keywords')\n# or\n&lt;Document&gt;.html_keywords\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : text/html</p>"},{"location":"document/#pandoc_document","title":"pandoc_document","text":"<p>Converts a string or a raw byte string into pandoc intermediate format.</p> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('pandoc_document')\n# or\n&lt;Document&gt;.pandoc_document\n</code></pre> <p>return type : Pandoc(Meta, [Block])</p> <p>supports pipeline flows: : application/epub+zip, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, mediawiki, pandoc, text/markdown, text/rtf</p>"},{"location":"document/#full_text_format","title":"full_text_format","text":"<p>Configuration for values:</p> <ul> <li>full_text_format = markdown (default)</li> </ul> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('full_text_format')\n# or\n&lt;Document&gt;.full_text_format\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : application/epub+zip, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, mediawiki, pandoc, text/markdown, text/rtf</p>"},{"location":"document/#convert_to","title":"convert_to","text":"<p>No documentation</p> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('convert_to')\n# or\n&lt;Document&gt;.convert_to\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : application/epub+zip, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, mediawiki, pandoc, text/markdown, text/rtf</p>"},{"location":"document/#clean_format","title":"clean_format","text":"<p>A constant value</p> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('clean_format')\n# or\n&lt;Document&gt;.clean_format\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : application/epub+zip, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, mediawiki, pandoc, text/markdown, text/rtf</p>"},{"location":"document/#sections","title":"sections","text":"<p>extract sections from a textbox dataframe</p> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('sections')\n# or\n&lt;Document&gt;.sections\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : application/epub+zip, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, mediawiki, pandoc, text/markdown, text/rtf</p>"},{"location":"document/#pandoc_blocks","title":"pandoc_blocks","text":"<p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('pandoc_blocks')\n# or\n&lt;Document&gt;.pandoc_blocks\n</code></pre> <p>return type : list['pandoc.types.Block']</p> <p>supports pipeline flows: : application/epub+zip, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, mediawiki, pandoc, text/markdown, text/rtf</p>"},{"location":"document/#headers","title":"headers","text":"<p>Extract tables, headers and lists from a pandoc document</p> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('headers')\n# or\n&lt;Document&gt;.headers\n</code></pre> <p>return type : str | list[str] | list[pandas.core.frame.DataFrame]</p> <p>supports pipeline flows: : application/epub+zip, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, mediawiki, pandoc, text/markdown, text/rtf</p>"},{"location":"document/#ocr_lang","title":"ocr_lang","text":"<p>Configuration for values:</p> <ul> <li>ocr_lang = auto (default)</li> <li>ocr_on = True (default)</li> </ul> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('ocr_lang')\n# or\n&lt;Document&gt;.ocr_lang\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : PIL.Image.Image, image, image/jpeg, image/png, image/tiff</p>"},{"location":"document/#ocr_on","title":"ocr_on","text":"<p>Configuration for values:</p> <ul> <li>ocr_lang = auto (default)</li> <li>ocr_on = True (default)</li> </ul> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('ocr_on')\n# or\n&lt;Document&gt;.ocr_on\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : PIL.Image.Image, image, image/jpeg, image/png, image/tiff</p>"},{"location":"document/#pil_image","title":"pil_image","text":"<p>No documentation</p> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('pil_image')\n# or\n&lt;Document&gt;.pil_image\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : PIL.Image.Image, image, image/jpeg, image/png, image/tiff</p>"},{"location":"document/#ocr_pdf_file","title":"ocr_pdf_file","text":"<p>Takes an image encoded in bytes and returns a pdf document     which can be used to extract data.</p> <pre><code>TODO: maybe we could add \"lines\" here and detect other thigns such as images,\n      figures  etc...?\n</code></pre> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('ocr_pdf_file')\n# or\n&lt;Document&gt;.ocr_pdf_file\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : PIL.Image.Image, image, image/jpeg, image/png, image/tiff</p>"},{"location":"document/#data_sel","title":"data_sel","text":"<p>select values by key from source data in Document</p> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('data_sel')\n# or\n&lt;Document&gt;.data_sel\n</code></pre> <p>return type : typing.Callable[..., dict]</p> <p>supports pipeline flows: : &lt;class 'dict'&gt;, application/x-yaml</p>"},{"location":"document/#keys","title":"keys","text":"<p>No documentation</p> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('keys')\n# or\n&lt;Document&gt;.keys\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : &lt;class 'dict'&gt;, application/x-yaml</p>"},{"location":"document/#values","title":"values","text":"<p>No documentation</p> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('values')\n# or\n&lt;Document&gt;.values\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : &lt;class 'dict'&gt;, application/x-yaml</p>"},{"location":"document/#items","title":"items","text":"<p>No documentation</p> <p>Can be called using:</p> <pre><code>&lt;Document&gt;.x('items')\n# or\n&lt;Document&gt;.items\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : &lt;class 'dict'&gt;, application/x-yaml</p>"},{"location":"documentbag/","title":"pydoxtools.DocumentBag","text":"<p>         Bases: <code>Pipeline</code></p> <p>This class is a work-in-progress (WIP), use with caution.</p> <p>The DocumentBag class loads and processes a set of documents using a pipeline. It leverages Dask bags for efficient memory usage and large-scale computations on documents.</p> Notes <ul> <li>Dask bags documentation can be found here.</li> <li>Dask dataframes can be used for downstream calculations.</li> <li>This class helps scale LLM &amp; AI inference to larger workloads.</li> <li>It uses iterative Dask bags &amp; dataframes to avoid out-of-memory issues.</li> </ul> Rationale <p>This function is needed to create and process new document bags, instead of using Dask bags directly with arbitrary data. It reduces boilerplate code for creating new documents and traceable datasources.</p>"},{"location":"documentbag/#pydoxtools.document.DocumentBag.__repr__","title":"<code>__repr__()</code>","text":"<p>Returns:</p> Name Type Description <code>str</code> <p>A string representation of the instance.</p>"},{"location":"documentbag/#text-extraction-attributes-and-functions","title":"Text extraction attributes and functions","text":"<p>The pydoxtools.DocumentBag is built on the pydoxtools.Pipeline class and most of the text extraction functionality makes extensive use of the pipeline features. All attributes and functions that are created by the pipeline are documented here.</p> <p>Pipeline visualizations for the structure of the Document pipelines for different document types can be found here.</p>"},{"location":"documentbag/#doc_configuration","title":"doc_configuration","text":"<p>We can pass through a configuration object to Documents that are created in our document bag. Any setting that is supported by Document can be specified here.</p> <p>Can be called using:</p> <pre><code>&lt;DocumentBag&gt;.x('doc_configuration')\n# or\n&lt;DocumentBag&gt;.doc_configuration\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : *, &lt;class 'dask.bag.core.Bag'&gt;, &lt;class 'list'&gt;, &lt;class 'pathlib.Path'&gt;, &lt;class 'pydoxtools.document.DatabaseSource'&gt;</p>"},{"location":"documentbag/#forgiving_extracts","title":"forgiving_extracts","text":"<p>When enabled, if we execute certain batch operations on our document bag, this will not stop the extraction, but rather put an error message in the document.</p> <p>Can be called using:</p> <pre><code>&lt;DocumentBag&gt;.x('forgiving_extracts')\n# or\n&lt;DocumentBag&gt;.forgiving_extracts\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : *, &lt;class 'dask.bag.core.Bag'&gt;, &lt;class 'list'&gt;, &lt;class 'pathlib.Path'&gt;, &lt;class 'pydoxtools.document.DatabaseSource'&gt;</p>"},{"location":"documentbag/#_stats","title":"_stats","text":"<p>A constant value</p> <p>Can be called using:</p> <pre><code>&lt;DocumentBag&gt;.x('_stats')\n# or\n&lt;DocumentBag&gt;._stats\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : *, &lt;class 'dask.bag.core.Bag'&gt;, &lt;class 'list'&gt;, &lt;class 'pathlib.Path'&gt;, &lt;class 'pydoxtools.document.DatabaseSource'&gt;</p>"},{"location":"documentbag/#verbosity","title":"verbosity","text":"<p>Configuration for values:</p> <ul> <li>verbosity = None (default)</li> </ul> <p>Can be called using:</p> <pre><code>&lt;DocumentBag&gt;.x('verbosity')\n# or\n&lt;DocumentBag&gt;.verbosity\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : *, &lt;class 'dask.bag.core.Bag'&gt;, &lt;class 'list'&gt;, &lt;class 'pathlib.Path'&gt;, &lt;class 'pydoxtools.document.DatabaseSource'&gt;</p>"},{"location":"documentbag/#get_dicts","title":"get_dicts","text":"<p>Returns a function closure which returns a bag of the specified     property of the enclosed documents.</p> <p>Can be called using:</p> <pre><code>&lt;DocumentBag&gt;.x('get_dicts')\n# or\n&lt;DocumentBag&gt;.get_dicts\n</code></pre> <p>return type : typing.Callable[[typing.Any], dask.bag.core.Bag]</p> <p>supports pipeline flows: : *, &lt;class 'dask.bag.core.Bag'&gt;, &lt;class 'list'&gt;, &lt;class 'pathlib.Path'&gt;, &lt;class 'pydoxtools.document.DatabaseSource'&gt;</p>"},{"location":"documentbag/#d","title":"d","text":"<p>Alias for: </p> <ul> <li>get_dicts-&gt;d (output)</li> </ul> <p>Can be called using:</p> <pre><code>&lt;DocumentBag&gt;.x('d')\n# or\n&lt;DocumentBag&gt;.d\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : *, &lt;class 'dask.bag.core.Bag'&gt;, &lt;class 'list'&gt;, &lt;class 'pathlib.Path'&gt;, &lt;class 'pydoxtools.document.DatabaseSource'&gt;</p>"},{"location":"documentbag/#bag_apply","title":"bag_apply","text":"<p>Basically it applies a function element-wise     on documents in a dask bag and then creates a new DocumentBag from that. This     works similar to pandas dataframes and series. But with documents     as a basic datatype. And apply functions are also required to     produce data which can be used as a document again (which is a lot).</p> <p>Can be called using:</p> <pre><code>&lt;DocumentBag&gt;.x('bag_apply')\n# or\n&lt;DocumentBag&gt;.bag_apply\n</code></pre> <p>return type : typing.Callable[..., dask.bag.core.Bag]</p> <p>supports pipeline flows: : *, &lt;class 'dask.bag.core.Bag'&gt;, &lt;class 'list'&gt;, &lt;class 'pathlib.Path'&gt;, &lt;class 'pydoxtools.document.DatabaseSource'&gt;</p>"},{"location":"documentbag/#apply","title":"apply","text":"<p>Basically it creates a Documentbag from two sets of     on documents in a dask bag and then creates a new DocumentBag from that. This     works similar to pandas dataframes and series. But with documents     as a basic datatype. And apply functions are also required to     produce data which can be used as a document again (which is a lot).</p> <p>Can be called using:</p> <pre><code>&lt;DocumentBag&gt;.x('apply')\n# or\n&lt;DocumentBag&gt;.apply\n</code></pre> <p>return type : typing.Callable[..., pydoxtools.document.DocumentBag]</p> <p>supports pipeline flows: : *, &lt;class 'dask.bag.core.Bag'&gt;, &lt;class 'list'&gt;, &lt;class 'pathlib.Path'&gt;, &lt;class 'pydoxtools.document.DatabaseSource'&gt;</p>"},{"location":"documentbag/#exploded","title":"exploded","text":"<p>Can be called using:</p> <pre><code>&lt;DocumentBag&gt;.x('exploded')\n# or\n&lt;DocumentBag&gt;.exploded\n</code></pre> <p>return type : typing.Callable[..., pydoxtools.document.DocumentBag]</p> <p>supports pipeline flows: : *, &lt;class 'dask.bag.core.Bag'&gt;, &lt;class 'list'&gt;, &lt;class 'pathlib.Path'&gt;, &lt;class 'pydoxtools.document.DatabaseSource'&gt;</p>"},{"location":"documentbag/#e","title":"e","text":"<p>Alias for: </p> <ul> <li>exploded-&gt;e (output)</li> </ul> <p>Can be called using:</p> <pre><code>&lt;DocumentBag&gt;.x('e')\n# or\n&lt;DocumentBag&gt;.e\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : *, &lt;class 'dask.bag.core.Bag'&gt;, &lt;class 'list'&gt;, &lt;class 'pathlib.Path'&gt;, &lt;class 'pydoxtools.document.DatabaseSource'&gt;</p>"},{"location":"documentbag/#stats","title":"stats","text":"<p>gather a number of statistics from documents as a pandas dataframe</p> <p>Can be called using:</p> <pre><code>&lt;DocumentBag&gt;.x('stats')\n# or\n&lt;DocumentBag&gt;.stats\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : *, &lt;class 'dask.bag.core.Bag'&gt;, &lt;class 'list'&gt;, &lt;class 'pathlib.Path'&gt;, &lt;class 'pydoxtools.document.DatabaseSource'&gt;</p>"},{"location":"documentbag/#document","title":"Document","text":"<p>Get a factory for pre-configured documents. Can be called just like pydoxtools.Document class, but automatically gets assigned the same configuration as all Documents in this bag</p> <p>Can be called using:</p> <pre><code>&lt;DocumentBag&gt;.x('Document')\n# or\n&lt;DocumentBag&gt;.Document\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : *, &lt;class 'dask.bag.core.Bag'&gt;, &lt;class 'list'&gt;, &lt;class 'pathlib.Path'&gt;, &lt;class 'pydoxtools.document.DatabaseSource'&gt;</p>"},{"location":"documentbag/#vectorizer","title":"vectorizer","text":"<p>vectorizes a query, using the document configuration of the Documentbag to determine which model to use.</p> <p>Can be called using:</p> <pre><code>&lt;DocumentBag&gt;.x('vectorizer')\n# or\n&lt;DocumentBag&gt;.vectorizer\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : *, &lt;class 'dask.bag.core.Bag'&gt;, &lt;class 'list'&gt;, &lt;class 'pathlib.Path'&gt;, &lt;class 'pydoxtools.document.DatabaseSource'&gt;</p>"},{"location":"documentbag/#add_to_chroma","title":"add_to_chroma","text":"<p>in order to build an index in chrome db we need a key, text, embeddings and a key. Those come from a daskbag with dictionaries with those keys. pydoxtools will return two functions which will - create the index- query the index</p> <p>Can be called using:</p> <pre><code>&lt;DocumentBag&gt;.x('add_to_chroma')\n# or\n&lt;DocumentBag&gt;.add_to_chroma\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : *, &lt;class 'dask.bag.core.Bag'&gt;, &lt;class 'list'&gt;, &lt;class 'pathlib.Path'&gt;, &lt;class 'pydoxtools.document.DatabaseSource'&gt;</p>"},{"location":"documentbag/#docs","title":"docs","text":"<p>create a bag with one document for each file that was foundFrom this point we can hand off the logic to str(Bag) pipeline.</p> <p>Can be called using:</p> <pre><code>&lt;DocumentBag&gt;.x('docs')\n# or\n&lt;DocumentBag&gt;.docs\n</code></pre> <p>return type : &lt;class 'dask.bag.core.Bag'&gt; | typing.Any</p> <p>supports pipeline flows: : &lt;class 'dask.bag.core.Bag'&gt;, &lt;class 'list'&gt;, &lt;class 'pathlib.Path'&gt;, &lt;class 'pydoxtools.document.DatabaseSource'&gt;</p>"},{"location":"documentbag/#take","title":"take","text":"<p>No documentation</p> <p>Can be called using:</p> <pre><code>&lt;DocumentBag&gt;.x('take')\n# or\n&lt;DocumentBag&gt;.take\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : &lt;class 'dask.bag.core.Bag'&gt;, &lt;class 'list'&gt;, &lt;class 'pathlib.Path'&gt;, &lt;class 'pydoxtools.document.DatabaseSource'&gt;</p>"},{"location":"documentbag/#compute","title":"compute","text":"<p>No documentation</p> <p>Can be called using:</p> <pre><code>&lt;DocumentBag&gt;.x('compute')\n# or\n&lt;DocumentBag&gt;.compute\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : &lt;class 'dask.bag.core.Bag'&gt;, &lt;class 'list'&gt;, &lt;class 'pathlib.Path'&gt;, &lt;class 'pydoxtools.document.DatabaseSource'&gt;</p>"},{"location":"documentbag/#sql","title":"sql","text":"<p>No documentation</p> <p>Can be called using:</p> <pre><code>&lt;DocumentBag&gt;.x('sql')\n# or\n&lt;DocumentBag&gt;.sql\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : &lt;class 'pydoxtools.document.DatabaseSource'&gt;</p>"},{"location":"documentbag/#connection_string","title":"connection_string","text":"<p>No documentation</p> <p>Can be called using:</p> <pre><code>&lt;DocumentBag&gt;.x('connection_string')\n# or\n&lt;DocumentBag&gt;.connection_string\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : &lt;class 'pydoxtools.document.DatabaseSource'&gt;</p>"},{"location":"documentbag/#index_column","title":"index_column","text":"<p>No documentation</p> <p>Can be called using:</p> <pre><code>&lt;DocumentBag&gt;.x('index_column')\n# or\n&lt;DocumentBag&gt;.index_column\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : &lt;class 'pydoxtools.document.DatabaseSource'&gt;</p>"},{"location":"documentbag/#bytes_per_chunk","title":"bytes_per_chunk","text":"<p>Configuration for values:</p> <ul> <li>bytes_per_chunk = 256 MiB (default)</li> </ul> <p>Can be called using:</p> <pre><code>&lt;DocumentBag&gt;.x('bytes_per_chunk')\n# or\n&lt;DocumentBag&gt;.bytes_per_chunk\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : &lt;class 'pydoxtools.document.DatabaseSource'&gt;</p>"},{"location":"documentbag/#dataframe","title":"dataframe","text":"<p>Load a table using dask/pandas read_sql</p> <pre><code>sql: can either be the entire table or an SQL expression\n</code></pre> <p>Can be called using:</p> <pre><code>&lt;DocumentBag&gt;.x('dataframe')\n# or\n&lt;DocumentBag&gt;.dataframe\n</code></pre> <p>return type : &lt;class 'dask.dataframe.core.DataFrame'&gt;</p> <p>supports pipeline flows: : &lt;class 'pydoxtools.document.DatabaseSource'&gt;</p>"},{"location":"documentbag/#bag","title":"bag","text":"<p>create a dask bag with all the filepaths in it</p> <p>Can be called using:</p> <pre><code>&lt;DocumentBag&gt;.x('bag')\n# or\n&lt;DocumentBag&gt;.bag\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : &lt;class 'list'&gt;, &lt;class 'pathlib.Path'&gt;, &lt;class 'pydoxtools.document.DatabaseSource'&gt;</p>"},{"location":"documentbag/#root_path","title":"root_path","text":"<p>Alias for: </p> <ul> <li>source-&gt;root_path (output)</li> </ul> <p>Can be called using:</p> <pre><code>&lt;DocumentBag&gt;.x('root_path')\n# or\n&lt;DocumentBag&gt;.root_path\n</code></pre> <p>return type : typing.Any</p> <p>supports pipeline flows: : &lt;class 'list'&gt;, &lt;class 'pathlib.Path'&gt;</p>"},{"location":"documentbag/#paths","title":"paths","text":"<p>Can be called using:</p> <pre><code>&lt;DocumentBag&gt;.x('paths')\n# or\n&lt;DocumentBag&gt;.paths\n</code></pre> <p>return type : typing.Callable</p> <p>supports pipeline flows: : &lt;class 'list'&gt;, &lt;class 'pathlib.Path'&gt;</p>"},{"location":"documentbag/#file_path_list","title":"file_path_list","text":"<p>No documentation</p> <p>Can be called using:</p> <pre><code>&lt;DocumentBag&gt;.x('file_path_list')\n# or\n&lt;DocumentBag&gt;.file_path_list\n</code></pre> <p>return type : &lt;class 'dask.bag.core.Bag'&gt; | typing.Any</p> <p>supports pipeline flows: : &lt;class 'list'&gt;, &lt;class 'pathlib.Path'&gt;</p>"},{"location":"documentbag/#dir_list","title":"dir_list","text":"<p>No documentation</p> <p>Can be called using:</p> <pre><code>&lt;DocumentBag&gt;.x('dir_list')\n# or\n&lt;DocumentBag&gt;.dir_list\n</code></pre> <p>return type : &lt;class 'dask.bag.core.Bag'&gt; | typing.Any</p> <p>supports pipeline flows: : &lt;class 'list'&gt;, &lt;class 'pathlib.Path'&gt;</p>"},{"location":"examples/","title":"Examples","text":""},{"location":"examples/#ai-article-writing-with-information-retrieval","title":"AI Article Writing with Information Retrieval","text":"<p>A jupyter notebook demonstrating how we develop an app which writes articles based on information it finds in a directory of files. It uses an pydoxtools.LLMAgent for the wwriting part and a pydoxtools.DocumentBag for information retrieval.</p> <p>here is the link to the notebook:</p> <ul> <li>open directly in colab to play around with it: </li> <li>on github: https://github.com/Xyntopia/pydoxtools/blob/main/examples/automated_blog_writing.ipynb</li> </ul> <p>And the same thing in about 100 lines of code:</p> <ul> <li>https://github.com/Xyntopia/pydoxtools/blob/main/examples/automatic_project_writing.py</li> </ul>"},{"location":"examples/#todo-add-examples-in-more-detail","title":"TODO: add examples in more detail","text":"<ul> <li>SQL queries</li> <li>automated blog writing</li> <li>table extraction</li> <li>directory index</li> <li>Pandas-style-NLP</li> </ul>"},{"location":"examples/#checkout-more-examples-here","title":"Checkout more examples here:","text":"<p>https://github.com/Xyntopia/pydoxtools/tree/main/examples</p>"},{"location":"readme_cp/","title":"\ud83c\udfa9\u2728\ud83d\udcc4 pydoxtools (Python Library) \ud83c\udfa9\u2728\ud83d\udcc4","text":"<p>Documentation</p> <p>Pydoxtools is a library that provides a sophisticated interface for reading and writing documents, designed to work with AI models such as GPT, Alpaca, and Huggingface. It offers functionalities such as:</p> <ul> <li>Pipeline management</li> <li>Integration with AI (LLMs and more) models</li> <li>low-resource (PDF) table extraction without configuration and expensive   layout detection algorithms!</li> <li>Document analysis and question-answering</li> <li>Support for most of todays document formats</li> <li>Vector index Creation</li> <li>Entity, address identification and more</li> <li>List and keyword extraction</li> <li>Data normalization, translation, and cleaning</li> </ul> <p>The library allows for the creation of complex extraction pipelines for batch-processing of documents by defining them as a lazily-executed graph.</p>"},{"location":"readme_cp/#installation","title":"Installation","text":""},{"location":"readme_cp/#installing-from-github","title":"Installing from GitHub","text":"<p>While pydoxtools can already be installed through pip, due to the many updates coming in right now, it is currently recommended to use the latest version from GitHub as follows:</p> <pre><code>pip install -U \"pydoxtools[etl,inference] @ git+https://github.com/xyntopia/pydoxtools.git\"\n</code></pre>"},{"location":"readme_cp/#installing-from-pypi","title":"Installing from PyPI","text":"<p>Pydoxtools can also be installed through pip, which will become the recommended method once it becomes more stable:</p> <pre><code>pip install -U pydoxtools[etl,inference]\n</code></pre> <p>For loading additional file formats (docx, odt, epub), OCR and other options, check out the additional &gt; Installation Options &lt;.</p>"},{"location":"readme_cp/#teaser","title":"\ud83d\ude80 Teaser \ud83d\ude80","text":"<p>Experience a new level of convenience and efficiency in handling documents with Pydoxtools, and reimagine your data extraction pipelines!</p> <p>In this teaser, we'll demonstrate how to create a document, extract tables, and ask questions using AI models:</p> <pre><code>import pydoxtools as pdx\n\n# Create a document from various sources: file, string, bytestring, file-like object, or URL\ndoc = pdx.Document(\"https://www.raspberrypi.org/app/uploads/2012/12/quick-start-guide-v1.1.pdf\")\n\n# List available extraction functions\nprint(doc.x_funcs)\n\n# get all tables from a single document:\nprint(doc.tables)\n\n# Extract the first 20 tables that we can find in a directory (this might take a while,\n# make sure, to only choose a small directory for testing purposes)\ndocs = pdx.DocumentBag(\"./my_directory_with_documents\", forgiving_extracts=True)\nprint(docs.bag_apply([\"tables_df\", \"filename\"]).take(20))\n\n# Ask a question about the documents using a local Q&amp;A model\nprint(doc.answers([\"how much ram does it have?\"]))\n# Or only ask about the documents tables (or any other extracted information):\nprint(doc.answers([\"how much ram does it have?\"], \"tables\"))\n\n# To use ChatGPT for question-answering, set the API key as an environment variable:\n# OPENAI_API_KEY=\"sk ....\"\n# Then, ask questions about the document using ChatGPT\nprint(doc.chat_answers([\"What is the target group of this document?\"])[0].content)\nprint(doc.chat_answers([\"Answer if a 5-year old would be able to follow these instructions?\"])[0].content)\n</code></pre> <p>With Pydoxtools, you can easily access and process your documents, perform various extractions, and utilize AI models for more advanced analysis.</p>"},{"location":"readme_cp/#supported-file-formats","title":"Supported File Formats","text":"<p>Pydoxtools already supports loading from a large variety of different sources:</p> <ul> <li>Documents from URLs,</li> <li>pdf, html, docx, doc, odt, markdwn, rtf, epub, mediawiki</li> <li>everything supported by pandoc,</li> <li>images (png, jpg, bmp, tiff etc...),</li> <li>And some \"native-python\" dataformats: PIL.Image.Image, ,  <li>data formats: yaml (json in progress)</li> <li>And more!</li>"},{"location":"readme_cp/#some-features-in-more-detail","title":"Some Features in More Detail","text":""},{"location":"readme_cp/#large-pipelines","title":"Large Pipelines","text":"<p>Pydoxtools' main feature is the ability to mix LLMs and other AI models in large, composable, and customizable pipelines. Using pipelines comes with the slight disadvantage that it can be more challenging to add type hints to the code. However, using pipelines decouples all parts of your code, allowing all operators to work independently. This makes it easy to run the pipeline in a distributed setting for big data and enables easy, lazy evaluation. Additionally, mixing different LLM logics together becomes much easier.</p> <p>Check out how Pydoxtools' <code>Document</code> class mixes pipelines for each individual file type:</p> <ul> <li>Every node in an ellipse can be called as an attribute of the document-analysis pipeline.</li> <li>Every execution path is lazily executed throughout the entire graph.</li> <li>Every node is cached by default (but can be turned off).</li> <li>Every piece of this pipeline can be replaced by a customized version.</li> </ul> <p>As an example, consider this pipeline for *.png images from the repository, which includes OCR, keyword extraction, vectorization, and more:</p> <p></p> <p>Pipelines can be mixed, partially overwritten, and extended, giving you a lot of possibilities to extend and adapt the functionality for your specific use case.</p> <p>To learn more about Pydoxtools' large pipelines feature, please refer to the documentation.</p>"},{"location":"readme_cp/#pipeline-configuration","title":"Pipeline Configuration","text":"<p>Pipelines can be configured. For example the local model used for question answering can be selected like this:</p> <pre><code>doc = Document(fobj=\"./data/PFR-PR23_BAT-110__V1.00_.pdf\"))\n        .config(qam_model_id='bert-large-uncased-whole-word-masking-finetuned-squad')\n</code></pre> <p>where \"qam_model_id\" can be any model from huggingface for question answering.</p> <pre><code>TODO: document how to configure a pipeline\n</code></pre>"},{"location":"readme_cp/#pdf-table-extraction-algorithms","title":"PDF Table Extraction Algorithms","text":"<p>The library features its own sophisticated Table extraction algorithm which is benchmarked against a large pdf table dataset. In contrast to how most \"classical\" table extraction algorithms work, it doesn't require:</p> <ul> <li>extensive configuration</li> <li>no expensive deep neural networks for table area recognition which need a GPU and   a lot of memory/CPU requirements</li> </ul> <p>This makes it possible to run analysis on PDF files with pydoxtools on CPU with very limited resources!</p>"},{"location":"readme_cp/#todo-describe-more-of-the-features-here","title":"TODO: Describe more of the features here...","text":""},{"location":"readme_cp/#use-cases","title":"Use Cases","text":"<ul> <li>create new documents from unstructured information</li> <li>analyze documents using any model from huggingface</li> <li>analyze documents using a custom model</li> <li>download a pdf from URL</li> <li>generate document keywords</li> <li>extract tables</li> <li>download document from URL \"manually\" and then feed to document</li> <li>extract addresses</li> <li>extract addresses and use this information for the qam</li> <li>ingest documents into a vector db</li> </ul>"},{"location":"readme_cp/#installation-options","title":"Installation Options","text":"<p>If you simply want to get going, you can install the following libraries on your system which will do evrything for you:</p> <pre><code>sudo apt-get install tesseract-ocr tesseract-ocr-deu tesseract-ocr-fra tesseract-ocr-eng tesseract-ocr-spa \\\n                     poppler-utils graphviz graphviz-dev \\\nsudo apt-get install pandoc\n# OR (for getting the newest version with all features)\n# cd /tmp\n# wget https://github.com/jgm/pandoc/releases/download/2.19.2/pandoc-2.19.2-1-amd64.deb\n# dpkg -i pandoc-2.19.2-1-amd64.deb\n</code></pre> <p>Below are some explanation what the different</p>"},{"location":"readme_cp/#supporting-docx-odt-epub","title":"Supporting *.docx, *.odt, *.epub","text":"<p>In order to be able to load docx, odt and rtf files, you have to install pandoc. Right now, the python pandoc library does not work with pandoc version &gt; 3.0.0. It is therefore recommended to install a version from here for your OS:</p> <p>https://github.com/jgm/pandoc/releases/tag/2.19.2</p>"},{"location":"readme_cp/#image-ocr-support","title":"Image OCR Support","text":"<p>Pydoxtools can automatically analyze images as well, makin use of OCR. In order to be able to use this, install tesseract on your system:</p> <p>Under linux this looks like the following:</p> <pre><code>sudo apt-get update &amp;&amp; sudo apt-get tesseract-ocr\n# install tesseract languages \n# Display a list of all Tesseract language packs:\n#   apt-cache search tesseract-ocr\n# install all languages:\n# sudo apt install tesseract-ocr-*\n# install only german, french, english, spanish language packs\nsudo apt install tesseract-ocr-deu tesseract-ocr-fra tesseract-ocr-eng tesseract-ocr-spa\n</code></pre>"},{"location":"readme_cp/#pdf-image-rendering","title":"pdf image rendering","text":"<p>For pdf rendering, Pydoxtools makes use of a library \"poppler\" which needs to be installed on your system. Under linux, this looks like the following:</p> <pre><code>sudo apt-get install poppler-utils\n</code></pre>"},{"location":"readme_cp/#graphviz","title":"Graphviz","text":"<p>For visualizing the document logic, you need to install graphviz on your system. Under linux, this looks like the following:</p> <pre><code>sudo apt-get install graphviz graphviz-dev\n</code></pre>"},{"location":"readme_cp/#development","title":"Development","text":"<p>--&gt; see </p>"},{"location":"readme_cp/#license","title":"License","text":"<p>This project is licensed under the terms of MIT license.</p> <p>You can check the compatibility using the following tool in a venv environment in a production setting:</p> <pre><code>pip install pip-licenses\npip-licenses | grep -Ev 'MIT License|BSD License|Apache Software License|Python Software Foundation License|Apache 2.0|MIT|Apache License 2.0|hnswlib|Pillow|new BSD|BSD'\n</code></pre>"},{"location":"readme_cp/#dependencies","title":"Dependencies","text":"<p>Here is a list of Libraries, that this project is based on:</p> <p>list</p>"},{"location":"reference/","title":"Reference","text":""},{"location":"reference/#pydoxtools.document_base.Pipeline","title":"<code>Pipeline</code>","text":"<p>Base class for all document classes in pydoxtools, defining a common pipeline interface and establishing a basic pipeline schema that derived classes can override.</p> <p>The MetaPipelineClassConfiguration acts as a compiler to resolve the pipeline hierarchy, allowing pipelines to inherit, mix, extend, or partially overwrite each other. Each key in the _pipelines dictionary represents a different pipeline version.</p> <p>The pydoxtools.Document class leverages this functionality to build separate pipelines for different file types, as the information processing requirements differ significantly between file types.</p> <p>Attributes:</p> Name Type Description <code>_operators</code> <code>dict[str, list[pydoxtools.operators_base.Operator]]</code> <p>Stores the definition of the pipeline graph, a collection of connected operators/functions that process data from a document.</p> <code>_pipelines</code> <code>dict[str, dict[str, pydoxtools.operators_base.Operator]]</code> <p>Provides access to all operator functions by their \"out-key\" which was defined in _operators.</p> Todo <ul> <li>Use pandera (https://github.com/unionai-oss/pandera) to validate dataframes   exchanged between extractors &amp; loaders   (https://pandera.readthedocs.io/en/stable/pydantic_integration.html)</li> </ul>"},{"location":"reference/#pydoxtools.document_base.Pipeline.configuration","title":"<code>configuration</code>  <code>property</code>","text":"<p>Returns a dictionary of all configuration objects for the current pipeline.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the names and values of all configuration objects   for the current pipeline.</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.pipeline_chooser","title":"<code>pipeline_chooser: str</code>  <code>property</code>","text":"<p>Must be implemented by derived classes to decide which pipeline they should use.</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.uuid","title":"<code>uuid</code>  <code>cached</code> <code>property</code>","text":"<p>Retrieves a universally unique identifier (UUID) for the instance.</p> <p>This method generates a new UUID for the instance using Python's <code>uuid.uuid4()</code> function. The UUID is then cached as a property, ensuring that the same UUID is returned for subsequent accesses.</p> <p>Returns:</p> Type Description <p>uuid.UUID: A unique identifier for the instance.</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.x_funcs","title":"<code>x_funcs: dict[str, Operator]</code>  <code>cached</code> <code>property</code>","text":"<p>get all operators/pipeline nodes and their property names for this specific file type/pipeline</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.__getattr__","title":"<code>__getattr__(extract_name)</code>","text":"<p>Retrieves an extractor result by directly accessing it as an attribute.</p> <p>This method is automatically called for attribute names that aren't defined on class level, allowing for a convenient way to access pipeline operator outputs without needing to call the 'x' method.</p> Example <p>document.addresses instead of document.x('addresses')</p> <p>Parameters:</p> Name Type Description Default <code>extract_name</code> <code>str</code> <p>The name of the extractor result to be accessed.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The result of the extractor after processing the document.</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.__getitem__","title":"<code>__getitem__(extract_name)</code>","text":"<p>Retrieves an extractor result by directly accessing it as an attribute.</p> <p>This method is automatically called for attribute names that aren't defined on class level, allowing for a convenient way to access pipeline operator outputs without needing to call the 'x' method.</p> Example <p>document[\"addresses\"] instead of document.x('addresses')</p> <p>Parameters:</p> Name Type Description Default <code>extract_name</code> <code>str</code> <p>The name of the extractor result to be accessed.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The result of the extractor after processing the document.</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.__getstate__","title":"<code>__getstate__()</code>","text":"<p>return necessary variables for pickling, ensuring that we leave out everything that can potentially have a lambda function in it...</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.__init__","title":"<code>__init__(**configuration)</code>","text":"<p>Initializes the Pipeline instance with cache-related attributes.</p> <p>**configuration: A dictionary of key-value pairs representing the configuration         settings for the pipeline. Each key is a string representing the name         of the configuration setting, and the value is the corresponding value         to be set.</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.__repr__","title":"<code>__repr__()</code>","text":"<p>Returns:</p> Name Type Description <code>str</code> <p>A string representation of the instance.</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.__setstate__","title":"<code>__setstate__(state)</code>","text":"<p>we need to restore _x_func_cache for pickling to work...</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.gather_inputs","title":"<code>gather_inputs(mapped_args, traceable)</code>","text":"<p>Gathers arguments from the pipeline and class, and maps them to the provided keys of kwargs.</p> <p>This method retrieves all required input parameters from _in_mapping, which was declared with \"pipe\". It first checks if the parameter is available as an extractor. If so, it calls the function to get the value. Otherwise, it gets the \"native\" member-variables or other functions if an extractor with that name is not found.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>dict</code> <p>A dictionary containing the keys to be mapped to the corresponding values.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the mapped keys and their corresponding values.</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.get_configuration_names","title":"<code>get_configuration_names(pipeline)</code>  <code>classmethod</code> <code>cached</code>","text":"<p>Returns a list of names of all configuration objects for a given pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline</code> <code>str</code> <p>The name of the pipeline to retrieve configuration objects from.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list[str]</code> <p>A list of strings containing the names of all configuration objects for the   given pipeline.</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.markdown_docs","title":"<code>markdown_docs()</code>  <code>classmethod</code>","text":"<p>Returns a formatted string containing the documentation for each pipeline operation in the class.</p> <p>This class method iterates through the pipeline operations, collects information about their output types and supported pipelines, and formats the documentation accordingly.</p> <p>Returns:</p> Name Type Description <code>str</code> <p>A formatted string containing the documentation for each pipeline operation, including  operation name, usage, return type, and supported pipelines.</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.non_interactive_pipeline","title":"<code>non_interactive_pipeline()</code>","text":"<p>return all non-interactive extractors/pipeline nodes</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.operator_infos","title":"<code>operator_infos(pipeline_type=None)</code>  <code>classmethod</code>","text":"<p>Aggregates the pipeline operations and their corresponding types and metadata.</p> <p>This method iterates through all the pipelines registered in the class, and gathers information about each operation, such as the pipeline types it appears in, the return type of the operation, and the operation's docstring.</p> <p>Returns:</p> Name Type Description <code>output_infos</code> <code>Dict[str, Dict[str, Union[Set, str]]]</code> <p>The aggregated information about pipeline operations, with operation keys as the top-level keys, and metadata such as pipeline types, output types, and descriptions as nested dictionaries.</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.operator_types","title":"<code>operator_types(json_schema=False)</code>  <code>classmethod</code>","text":"<p>This function returns a dictionary of operators with their types which is suitable for declaring a pydantic model.</p> if this is set to True, we make sure that only valid json <p>schema types are included in the model. The typical use case is to expose the pipeline via this model to an http API e.g. through fastapi. In this case we should only allow types that are valid json schema. Therefore, this is set to \"False\" by default.</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.pipeline_graph","title":"<code>pipeline_graph(image_path=None, document_logic_id='*')</code>  <code>classmethod</code>","text":"<p>Generates a visualization of the defined pipelines and optionally saves it as an image.</p> <p>Parameters:</p> Name Type Description Default <code>image_path</code> <code>str | pathlib.Path</code> <p>File path for the generated image. If provided, the                                        generated graph will be saved as an image.</p> <code>None</code> <code>document_logic_id</code> <code>str</code> <p>The document logic ID for which the pipeline graph should                                be generated. Defaults to \"current\".</p> <code>'*'</code> <p>Returns:</p> Name Type Description <code>AGraph</code> <p>A PyGraphviz AGraph object representing the pipeline graph. This object can be     visualized or manipulated using PyGraphviz functions.</p> Notes <p>This method requires the NetworkX and PyGraphviz libraries to be installed.</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.pre_cache","title":"<code>pre_cache()</code>","text":"<p>Pre-caches the results of all extractors that have caching enabled.</p> <p>This method iterates through the defined extractors and calls each one with caching enabled, storing the results for faster access in future calls.</p> <p>Returns:</p> Name Type Description <code>self</code> <p>The instance of the class, allowing for method chaining.</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.run_pipeline","title":"<code>run_pipeline(exclude=None)</code>","text":"<p>Runs all extractors defined in the pipeline for testing or pre-caching purposes.</p> <p>!!IMPORTANT!!!  This function should normally not be used as the pipeline is lazily executed anyway.</p> <p>This method iterates through the defined extractors and calls each one, ensuring that the extractor logic is functioning correctly and caching the results if required.</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.run_pipeline_fast","title":"<code>run_pipeline_fast()</code>","text":"<p>run pipeline, but exclude long-running calculations</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.set_disk_cache_settings","title":"<code>set_disk_cache_settings(enable, ttl=3600 * 24 * 7)</code>","text":"<p>Sets disk cache settings</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.to_dict","title":"<code>to_dict(*args, **kwargs)</code>","text":"<p>Returns a dictionary that accumulates the properties given in args or with a mapping in *kwargs.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>str</code> <p>A variable number of strings, each representing a property name.</p> <code>()</code> <code>**kwargs</code> <code>dict</code> <p>A dictionary mapping property names (values) to custom keys (keys) for the              returned dictionary.</p> <code>{}</code> Note <p>This function currently only supports properties that do not require any arguments, such as \"full_text\". Properties like \"answers\" that return a function requiring arguments cannot be used with this function.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary with the accumulated properties and their values, using either the   property names or custom keys as specified in the input arguments.</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.to_json","title":"<code>to_json(*args, **kwargs)</code>","text":"<p>Returns a dictionary that accumulates the properties given in args or with a mapping in *kwargs, and dumps the output as JSON.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>str</code> <p>A variable number of strings, each representing a property name.</p> <code>()</code> <code>**kwargs</code> <code>dict</code> <p>A dictionary mapping property names (values) to custom keys (keys) for the              returned dictionary.</p> <code>{}</code> Note <p>This function currently only supports properties that do not require any arguments, such as \"full_text\". Properties like \"answers\" that return a function requiring arguments cannot be used with this function.</p> <p>Returns:</p> Name Type Description <code>str</code> <p>A JSON-formatted string representing the accumulated properties and their values, using  either the property names or custom keys as specified in the input arguments.</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.to_yaml","title":"<code>to_yaml(*args, **kwargs)</code>","text":"<p>Returns a dictionary that accumulates the properties given in args or with a mapping in *kwargs, and dumps the output as YAML.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>str</code> <p>A variable number of strings, each representing a property name.</p> <code>()</code> <code>**kwargs</code> <code>dict</code> <p>A dictionary mapping property names (values) to custom keys (keys) for the              returned dictionary.</p> <code>{}</code> Note <p>This function currently only supports properties that do not require any arguments, such as \"full_text\". Properties like \"answers\" that return a function requiring arguments cannot be used with this function.</p> <p>Returns:</p> Name Type Description <code>str</code> <p>A YAML-formatted string representing the accumulated properties and their values, using  either the property names or custom keys as specified in the input arguments.</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.x","title":"<code>x(operator_name, disk_cache=False, traceable=False)</code>","text":"<p>Calls an extractor from the defined pipeline and returns the result.</p> <p>Parameters:</p> Name Type Description Default <code>operator_name</code> <code>str</code> <p>The name of the extractor to be called.</p> required <code>cache</code> <p>if we want to cache the call. We can explicitly tell     the pipeline to cache a call. to make caching more efficient     by only caching the calls we want.</p> required <code>traceable</code> <code>bool</code> <p>Some operators will propagate the source of their information        through the pipeline. This adds traceability. By setting this to        traceable=True we can turn this feature on.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The result of the extractor after processing the document.</p> <p>Raises:</p> Type Description <code>operators.OperatorException</code> <p>If an error occurs while executing the extractor.</p> Notes <p>The extractor's parameters can be overridden using args and *kwargs.</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.x_all","title":"<code>x_all()</code>","text":"<p>Retrieves the results of all extractors defined in the pipeline.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the results of all extractors, with keys as the extractor   names and values as the corresponding results.</p>"},{"location":"examples/automated_blog_writing/","title":"Pydoxtools: Automated, LLM based article writing with information retrieval from a directory of files in fewer than 100 lines of code!","text":"<p>At Xyntopia, we are excited to introduce our latest creation, Pydoxtools - a versatile Python library designed to streamline AI-powered document processing and information retrieval. This library is perfect for users who are new to programming and want to harness the power of AI in their projects.</p> <p>This article showcases an efficient method to automate article writing for your project using ChatGPT and Pydoxtools in fewer than 100 lines of code. The script demonstrates the following key functionalities:</p> <ul> <li>Indexing a directory containing files with PyDoxTools</li> <li>Employing an agent for information retrieval within those files</li> <li>Auto-generating a text based on a set objective</li> </ul> <p>You can execute this notebook or simply refer to our concise script, which executes these steps in less than 100 lines of code:</p> <p>https://github.com/Xyntopia/pydoxtools/blob/main/examples/automatic_project_writing.py</p> <p>or open this notebook in colab:</p> <p>https://colab.research.google.com/github/Xyntopia/pydoxtools/blob/main/examples/automated_blog_writing.ipynb</p>"},{"location":"examples/automated_blog_writing/#costs-and-api-key","title":"Costs and API Key","text":"<p>Please note that ChatGPT is a paid service, and running the script once will cost you about 2-5 cents. To use ChatGPT, you will need to generate an OpenAI API key by registering an account at https://platform.openai.com/account/api-keys. Remember to keep your API key secure and do not share it with anyone.</p> <p>We are working on an implementation which makes use of open source models which can do the same for free, locally on your computer. Additionally, Pydoxtools automatically caches all calls to ChatGPT. So subsequent runs usually turn out to be a little cheaper.</p>"},{"location":"examples/automated_blog_writing/#safeguarding-your-api-key-in-google-colab","title":"Safeguarding Your API Key in Google Colab","text":"<p>When working with sensitive information like API keys, it's crucial to ensure their security. In Google Colab, you can save your API key in a separate file, allowing you to share the notebook without exposing the key. To do this, follow these simple steps:</p> <ol> <li>Execute the cell below to create a new file in your Colab environment. This file will store your API key, and it will be deleted automatically when the Colab runtime is terminated.</li> </ol> <pre><code>!touch /tmp/openai_api_key\n</code></pre> <ol> <li> <p>Click on the following link to open the newly created file by clicking on the following link in colab: /tmp/openai_api_key</p> </li> <li> <p>Copy and paste your API key into the file, then save it.</p> </li> </ol> <p>By following these steps, you can ensure the security of your API key while still being able to share your notebook with others. Happy coding!</p>"},{"location":"examples/automated_blog_writing/#installation","title":"Installation","text":"<p>Follow these simple steps to install and configure Pydoxtools for your projects:</p> <ol> <li>Install the Pydoxtools library by running the following command:</li> </ol> <pre><code>%%capture\n# if we want to install directly from our repository:\n#!pip install -U --force-reinstall --no-deps \"pydoxtools[etl,inference] @ git+https://github.com/xyntopia/pydoxtools.git\"\n!pip install -U pydoxtools[etl,inference]==0.6.3\n</code></pre> <p>After installation, restart the runtime to load the newly installed libraries into Jupyter.</p> <ol> <li>Now we are loading the OPENAI_API_KEY from our file.</li> </ol> <pre><code>#load the key as an environment variable:\nimport os\n# load the key\nwith open('/tmp/openai_api_key') as f:\n  os.environ['OPENAI_API_KEY']=f.read()\n</code></pre> <ol> <li>now we can initialize pydoxtools which will automatically make use of the OPENAI_API_KEY</li> </ol> <pre><code>import logging\n\nimport dask\nfrom chromadb.config import Settings\n\nimport pydoxtools as pdx\nfrom pydoxtools import agent as ag\nfrom pydoxtools.settings import settings\n\nlogger = logging.getLogger(__name__)\nlogging.basicConfig(level=logging.INFO)\nlogging.getLogger(\"pydoxtools.document\").setLevel(logging.INFO)\n</code></pre> <pre><code>WARNING:pydoxtools.extract_pandoc:installed pandoc version 2.5, which doesn't support rtf file format!in order to be able to use rtf, you need to install a pandoc version &gt;= 2.14.2Checkout this link here: https://github.com/jgm/pandoc/releases/tag/2.19.2\n</code></pre>"},{"location":"examples/automated_blog_writing/#configuration","title":"configuration","text":"<p>Pydoxtools can be configured in various ways for this example we are using two settings:</p> <ul> <li>Pydoxtools uses dask in the background to handle large indexes and databases. We could even index terabytes of data this way! For this example though, we are setting the dask scheduler to \"synchronous\" so that we can see everything thats happening locally and make it easy to debug the script.</li> <li>Pydoxtools has a caching mechanism which caches calls to pydoxtoos.Document. This helps during development for much faster execution on subsequent runs (for example the vector index creation or extraction of other information from documents).</li> </ul> <pre><code># pydoxtools.DocumentBag uses a dask scheduler for parallel computing\n# in the background. For easier debugging, we set this to \"synchronous\"\ndask.config.set(scheduler='synchronous')\n# dask.config.set(scheduler='multiprocessing') # can als be used...\n\nsettings.PDX_ENABLE_DISK_CACHE = True  # turn on caching for pydoxtools\n</code></pre>"},{"location":"examples/automated_blog_writing/#download-our-project","title":"download our project","text":"<p>In order for our program to work, we need to provide the AI with information. In this case we are using files in a directory as a source of information! We are simply downloading the \"Pydoxtools\" project from github. Essentialy pydoxtools is writing about itself :-). You could also mount a google drive here or simply load a folder on your computer if you're running this notebook locally on your computer.'</p> <pre><code>!cd /content\n!git clone https://github.com/Xyntopia/pydoxtools.git\n</code></pre> <pre><code>fatal: destination path 'pydoxtools' already exists and is not an empty directory.\n</code></pre>"},{"location":"examples/automated_blog_writing/#index-initialization","title":"Index initialization","text":"<p>In order for an LLM like ChatGPT to retrieve the information it needs to be saved in a \"vectorformat\". This way we can retrieve relevant information using nearest neighbour search. We are using ChromaDB here for this purpose, but there are many other choices available.</p> <pre><code>##### Use chromadb as a vectorstore #####\nchroma_settings = Settings(\n    chroma_db_impl=\"duckdb+parquet\",\n    persist_directory=str(settings.PDX_CACHE_DIR_BASE / \"chromadb\"),\n    anonymized_telemetry=False\n)\n\n# create our source of information. It creates a list of documents\n# in pydoxtools called \"pydoxtools.DocumentBag\" (which itself holds a list of pydoxtools.Document) and\n# here we choose to use pydoxtools itself as an information source!\nroot_dir = \"/content/pydoxtools\"\nds = pdx.DocumentBag(\n    source=root_dir,\n    exclude=[  # ignore some files which make the indexing rather inefficient\n        '.git/', '.idea/', '/node_modules', '/dist',\n        '/__pycache__/', '.pytest_cache/', '.chroma', '.svg', '.lock',\n        \"/site/\"\n    ],\n    forgiving_extracts=True\n)\n\n</code></pre>"},{"location":"examples/automated_blog_writing/#initialize-agent-give-it-a-writing-objective-and-compute-the-index","title":"Initialize agent, give it a writing objective and compute the index","text":"<p>Now that we have everything setup, we can initialize our LLM Agent with the provided information. For the pydoxtools project in this example, computing the index will take about 5-10 minutes. In total there will be about 4000 text snippets in the vector index for the  project after finishing the computation.. When using the pydoxtools cache, subsequent calculations will be much faster (~1 min).</p> <pre><code>final_result = []\n\nagent = ag.LLMAgent(\n    vector_store=chroma_settings,\n    objective=\"Write a blog post, introducing a new library (which was developed by us, \"\n              \"the company 'Xyntopia') to \"\n              \"visitors of our corporate webpage, which might want to use the pydoxtools library but \"\n              \"have no idea about programming. Make sure, the text is about half a page long.\",\n    data_source=ds\n)\nagent.pre_compute_index()\n</code></pre> <pre><code>WARNING:chromadb.api.models.Collection:No embedding_function provided, using default embedding function: DefaultEmbeddingFunction https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n\n\n[                                        ] | 0% Completed | 221.54 us\n\nWARNING:chromadb.api.models.Collection:No embedding_function provided, using default embedding function: DefaultEmbeddingFunction https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n\n\n[                                        ] | 0% Completed | 938.02 ms\n\nWARNING:chromadb.api.models.Collection:No embedding_function provided, using default embedding function: DefaultEmbeddingFunction https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n\n\n[####                                    ] | 10% Completed | 2.89 s\n\nWARNING:chromadb.api.models.Collection:No embedding_function provided, using default embedding function: DefaultEmbeddingFunction https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n\n\n[########                                ] | 20% Completed | 4.53 s\n\nWARNING:chromadb.api.models.Collection:No embedding_function provided, using default embedding function: DefaultEmbeddingFunction https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n\n\n[################                        ] | 40% Completed | 6.79 s\n\nWARNING:chromadb.api.models.Collection:No embedding_function provided, using default embedding function: DefaultEmbeddingFunction https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n\n\n[####################                    ] | 50% Completed | 10.61 s\n\nWARNING:chromadb.api.models.Collection:No embedding_function provided, using default embedding function: DefaultEmbeddingFunction https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n\n\n[########################                ] | 60% Completed | 13.09 s\n\nWARNING:chromadb.api.models.Collection:No embedding_function provided, using default embedding function: DefaultEmbeddingFunction https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n\n\n[############################            ] | 70% Completed | 27.41 s\n\nWARNING:chromadb.api.models.Collection:No embedding_function provided, using default embedding function: DefaultEmbeddingFunction https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n\n\n[################################        ] | 80% Completed | 37.35 s\n\nWARNING:chromadb.api.models.Collection:No embedding_function provided, using default embedding function: DefaultEmbeddingFunction https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n\n\n[####################################    ] | 90% Completed | 51.05 s\n\nWARNING:chromadb.api.models.Collection:No embedding_function provided, using default embedding function: DefaultEmbeddingFunction https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n\n\n[########################################] | 100% Completed | 60.10 s\n</code></pre>"},{"location":"examples/automated_blog_writing/#search-for-relevant-information","title":"Search for relevant Information","text":"<p>The agent is able to store information as question/answer pairs and makes use of that information when executing tasks. In order to get our algorithm running a bit more quickly, we answer a basic question manually, to get the algorithm started more quickly... In a real app you could ask questions like this in a user-dialog.</p> <pre><code>agent.add_question(question=\"Can you please provide the main topic of the project or some primary \"\n                            \"keywords related to the project, \"\n                            \"to help with identifying the relevant files in the directory?\",\n                    answer=\"python library, AI, pipelines\")\n</code></pre> <pre><code>WARNING:chromadb.api.models.Collection:No embedding_function provided, using default embedding function: DefaultEmbeddingFunction https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n</code></pre> <p>Having this information, we ask the agent to come up with a few more questions that it needs to answer before being able to write the article.</p> <pre><code># first, gather some basic information...\nquestions = agent.execute_task(\n  task=\"What additional information do you need to create a first, very short outline as a draft? \" \\\n        \"provide it as a ranked list of questions\", save_task=True)\n</code></pre> <pre><code>print(\"\\n\".join(questions))\n</code></pre> <pre><code>What is the name of the new library developed by Xyntopia?\nWhat is the purpose of the pydoxtools library?\nWhat are some examples of how the pydoxtools library can be used in AI pipelines?\nWhat are some benefits of using the pydoxtools library for non-programmers?\nWhat are some potential drawbacks or limitations of the pydoxtools library?\n</code></pre> <p>Having created this list of questions, we can now ask the agent to research them by itself. It will automatically use the index we computed above for this task.</p> <pre><code># we only use the first 5 provided questions to make it faster ;).\nagent.research_questions(questions[:5], allowed_documents=[\"text/markdown\"])\n</code></pre> <pre><code>Token indices sequence length is longer than the specified maximum sequence length for this model (871 &gt; 512). Running this sequence through the model will result in indexing errors\n</code></pre> <p>After retrieving this information we can tell the agent t write the text. We tell it to automatically make use of the information by setting the \"context_size\" parameter to a value greater than 0. This represents the pieces of stored information that it will use to fulfill the task.</p> <pre><code>txt = agent.execute_task(task=\"Complete the overall objective, formulate the text \"\n                              \"based on answered questions and format it in markdown.\",\n                          context_size=20, max_tokens=1000, formatting=\"txt\")\nfinal_result.append(txt)  # add a first draft to the result\n</code></pre> <p>Having our first draft of the text, lets critize it to improve the quality! Then with this critique create a new list of tasks that we can give to the agent to execute one-by-one. Gradually improving our text. </p> <pre><code>critique = agent.execute_task(task=\"Given this text:\\n\\n```markdown\\n{txt}\\n```\"\n                                    \"\\n\\nlist 5 points of critique about the text\",\n                              context_size=0, max_tokens=1000)\n\ntasks = agent.execute_task(\n    task=\"Given this text:\\n\\n```markdown\\n{txt}\\n```\\n\\n\"\n          f\"and its critique: {critique}\\n\\n\"\n          \"Generate instructions that would make it better. \"\n          \"Sort them by importance and return it as a list of tasks\",\n    context_size=0, max_tokens=1000)\n\nfor t in tasks:\n    task = \"Given this text:\\n\\n\" \\\n            f\"```markdown\\n{txt}\\n```\\n\\n\" \\\n            f\"Make the text better by executing this task: '{t}' \" \\\n            f\"and integrate it into the given text, but keep the overall objective in mind.\"\n    txt = agent.execute_task(task, context_size=10, max_tokens=1000, formatting=\"markdown\")\n    final_result.append([task, txt])\n</code></pre> <pre><code>print(\"\\n\".join(str(t) for t in tasks))\n</code></pre> <pre><code>{'Specify the text length': 'Write a blog post that is approximately half a page long.'}\n{'Define the target audience': 'Introduce the pydoxtools library to visitors of our corporate webpage who are interested in using it but have no programming experience.'}\n{'Mention the name of the library': 'Introduce our new library, pydoxtools, to readers.'}\n{'Explain the benefits of using the library': 'Highlight the advantages of using pydoxtools, such as simplifying documentation generation and improving workflow efficiency.'}\n{'Include a call-to-action or next steps for the reader': 'Encourage readers to try out pydoxtools by providing a link to download the library and offering support resources for beginners.'}\n</code></pre> <pre><code># for debugging, you can see all intermediate results, simply uncomment the variable to check:\n\n#final_result  # for the evolution of the final text\n#agent._debug_queue  # in order to check all requests made to llms and vectorstores etc...\n</code></pre>"},{"location":"examples/automated_blog_writing/#final-text","title":"Final text","text":"<p>After all the processing is finally done, here is the final text:</p> <pre><code>from IPython.display import display, Markdown, Latex\nMarkdown(txt.strip(\"`\").replace(\"markdown\",\"\"))\n</code></pre>"},{"location":"examples/automated_blog_writing/#introduction-to-pydoxtools","title":"Introduction to Pydoxtools","text":"<p>Introduce our new library, pydoxtools, to readers. Pydoxtools is a Python library developed by Xyntopia that provides a sophisticated interface for reading and writing documents, designed to work with AI models such as GPT, Alpaca, and Huggingface. The library aims to simplify the process of building custom pipelines with LLMs and other AI tools, making it easy to integrate modern AI tools and reimagine data extraction pipelines.</p>"},{"location":"examples/automated_blog_writing/#purpose-of-pydoxtools","title":"Purpose of Pydoxtools","text":"<p>The purpose of the pydoxtools library is to provide functionalities such as pipeline management, integration with AI models, low-resource (PDF) table extraction without configuration and expensive layout detection algorithms, document analysis and question-answering, support for most document formats, vector index creation, entity and address identification, list and keyword extraction, data normalization, translation, and cleaning.</p>"},{"location":"examples/automated_blog_writing/#benefits-for-non-programmers","title":"Benefits for Non-Programmers","text":"<p>Pydoxtools is a great tool for non-programmers who want to extract data from documents. It offers low-resource (PDF) table extraction without configuration and expensive layout detection algorithms, document analysis and question-answering, support for most document formats, vector index creation, entity and address identification, list and keyword extraction, data normalization, translation, and cleaning. The library also allows for the creation of custom pipelines with LLMs and other AI tools, making it easy to integrate modern AI tools and reimagine data extraction pipelines. </p> <p>Some benefits of using the pydoxtools library for non-programmers include simplifying documentation generation and improving workflow efficiency.</p>"},{"location":"examples/automated_blog_writing/#usage-in-ai-pipelines","title":"Usage in AI Pipelines","text":"<p>Pydoxtools can be used in AI pipelines for low-resource (PDF) table extraction without configuration and expensive layout detection algorithms, document analysis and question-answering, support for most document formats, vector index creation, entity and address identification, list and keyword extraction, data normalization, translation, and cleaning. The library also allows for the creation of custom pipelines with LLMs and other AI tools, making it easy to integrate modern AI tools and reimagine data extraction pipelines.</p>"},{"location":"examples/automated_blog_writing/#limitations","title":"Limitations","text":"<p>There is no mention of any potential drawbacks or limitations of the pydoxtools library in the provided text. </p>"},{"location":"examples/automated_blog_writing/#target-audience","title":"Target Audience","text":"<p>This blog post is intended for visitors of our corporate webpage who are interested in using the pydoxtools library but have no programming experience.</p> <p>Encourage readers to try out pydoxtools by providing a link to download the library and offering support resources for beginners. If you are interested in using Pydoxtools, you can find more information on our corporate webpage.</p>"},{"location":"examples/automated_blog_writing/#conclusion","title":"Conclusion","text":"<p>Pydoxtools is a powerful and user-friendly Python library that makes it easy to harness the power of AI for document processing and information retrieval. Whether you are new to programming or an experienced developer, Pydoxtools can help you streamline your projects and achieve your goals. Give it a try today and experience the benefits for yourself!</p> <p>Get more information under the following links:</p> <ul> <li>https://pydoxtools.xyntopia.com</li> <li>https://github.com/xyntopia/pydoxtools</li> <li>https://www.xyntopia.com</li> </ul>"}]}