{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Pydoxtools Documentation! Readme Readme Visualization of the Extraction logic The extraction logic for different file types can be visualized bydoing something like this: doc = Document(fobj=make_path_absolute(\"./data/demo.docx\"), document_type=\".docx\") # for the currently loaded file type: doc.logic_graph(image_path=settings._PYDOXTOOLS_DIR / \"docs/images/document_logic_docx.svg\") # for the doc.logic_graph(image_path=settings._PYDOXTOOLS_DIR / \"docs/images/document_logic_png.svg\", document_logic_id=\".png\") This way we can generate the pipelines for different filetypes: docx png (click on links to open the images!) This also works for custom pipelines! In order to learn more continue to: Reference","title":"Home"},{"location":"#welcome-to-pydoxtools-documentation","text":"","title":"Welcome to Pydoxtools Documentation!"},{"location":"#readme","text":"Readme","title":"Readme"},{"location":"#visualization-of-the-extraction-logic","text":"The extraction logic for different file types can be visualized bydoing something like this: doc = Document(fobj=make_path_absolute(\"./data/demo.docx\"), document_type=\".docx\") # for the currently loaded file type: doc.logic_graph(image_path=settings._PYDOXTOOLS_DIR / \"docs/images/document_logic_docx.svg\") # for the doc.logic_graph(image_path=settings._PYDOXTOOLS_DIR / \"docs/images/document_logic_png.svg\", document_logic_id=\".png\") This way we can generate the pipelines for different filetypes: docx png (click on links to open the images!) This also works for custom pipelines! In order to learn more continue to: Reference","title":"Visualization of the Extraction logic"},{"location":"DEVELOPMENT/","text":"Development & Contribution The graph model of the library makes it very easy to extend it with new functionality. the document can be used as a base-model and overwritten with changes the graph can be changed dynamically new functions can be very easily integrated Pydoxtools Architecture --> refer to \"document\" Contribution Guidelines","title":"Development & Contribution"},{"location":"DEVELOPMENT/#development-contribution","text":"The graph model of the library makes it very easy to extend it with new functionality. the document can be used as a base-model and overwritten with changes the graph can be changed dynamically new functions can be very easily integrated","title":"Development &amp; Contribution"},{"location":"DEVELOPMENT/#pydoxtools-architecture","text":"--> refer to \"document\"","title":"Pydoxtools Architecture"},{"location":"DEVELOPMENT/#contribution-guidelines","text":"","title":"Contribution Guidelines"},{"location":"readme_cp/","text":"\ud83d\ude80 pydoxtools \ud83d\ude80 ( WIP ) Documentation Pydoxtools is a library that provides a sophisticated interface for reading and writing documents, designed to work with AI models such as GPT, Alpaca, and Huggingface. It offers functionalities such as: Table extraction Vector Index Creation Document analysis and question-answering Entity, address identification and more List and keyword extraction Data normalization, translation, and cleaning The library allows for the creation of complex extraction pipelines for batch-processing of documents by defining them as a lazily-executed graph. Teaser Experience a new level of convenience and efficiency in handling documents with Pydoxtools, and reimagine your data extraction pipelines! \ud83c\udfa9\u2728\ud83d\udcc4. import pydoxtools as pdx # create a document from a file, string, bytestring, file-like object # or even an url: doc = Document( \"https://www.raspberrypi.org/app/uploads/2012/12/quick-start-guide-v1.1.pdf\", document_type=\".pdf\" ) # extract the table as a pandas dataframe: print(doc.tables_df) # ask a question about the document, using Q&A Models (questionas answered locally!): print(doc.answers([\"how much power does it need?\"])[0][0][0]) # ask a question about the document, using ChatGPT: print(doc.chat_answers([\"who is the target group of this document?\"])[0].content) print(doc.chat_answers([\"Answer if a 5-year old would be able to follow these instructions?\"])[0].content) Large pipelines Pydoxtools main feature are large, composable and customizable pipelines. As a teaser check out this pipeline for *.png images from the repository: Pipelines can be mixed, partially overwritten and extended which gives you a lot of possibilities to extend and adapt the functionality for your specific use-case. Find out more about it in the documentation Installation Pydoxtools can be installed through pip: pip install pydoxtools[etl, inference] It should automatically download models & dependencies where required. In order to get the latest version install it like this: pip install git+https://github.com/xyntopia/pydoxtools' Use Cases analyze documents using any model from huggingface... analyze documents using a custom model download a pdf from URL generate document keywords extract tables download document from URL \"manually\" and then feed to document extract addresses extract addresses and use this information for the qam ingest documents into a vector db Development --> see License This project is licensed under the terms of MIT license. You can check the compatibility using the following tool in a venv environment in a production setting: pip install pip-licenses pip-licenses | grep -Ev 'MIT License|BSD License|Apache Software License|Python Software Foundation License|Apache 2.0|MIT|Apache License 2.0|hnswlib|Pillow|new BSD|BSD' list of libraries, that this project is based on: list","title":"Readme"},{"location":"readme_cp/#pydoxtools","text":"( WIP ) Documentation Pydoxtools is a library that provides a sophisticated interface for reading and writing documents, designed to work with AI models such as GPT, Alpaca, and Huggingface. It offers functionalities such as: Table extraction Vector Index Creation Document analysis and question-answering Entity, address identification and more List and keyword extraction Data normalization, translation, and cleaning The library allows for the creation of complex extraction pipelines for batch-processing of documents by defining them as a lazily-executed graph.","title":"\ud83d\ude80 pydoxtools \ud83d\ude80"},{"location":"readme_cp/#teaser","text":"Experience a new level of convenience and efficiency in handling documents with Pydoxtools, and reimagine your data extraction pipelines! \ud83c\udfa9\u2728\ud83d\udcc4. import pydoxtools as pdx # create a document from a file, string, bytestring, file-like object # or even an url: doc = Document( \"https://www.raspberrypi.org/app/uploads/2012/12/quick-start-guide-v1.1.pdf\", document_type=\".pdf\" ) # extract the table as a pandas dataframe: print(doc.tables_df) # ask a question about the document, using Q&A Models (questionas answered locally!): print(doc.answers([\"how much power does it need?\"])[0][0][0]) # ask a question about the document, using ChatGPT: print(doc.chat_answers([\"who is the target group of this document?\"])[0].content) print(doc.chat_answers([\"Answer if a 5-year old would be able to follow these instructions?\"])[0].content)","title":"Teaser"},{"location":"readme_cp/#large-pipelines","text":"Pydoxtools main feature are large, composable and customizable pipelines. As a teaser check out this pipeline for *.png images from the repository: Pipelines can be mixed, partially overwritten and extended which gives you a lot of possibilities to extend and adapt the functionality for your specific use-case. Find out more about it in the documentation","title":"Large pipelines"},{"location":"readme_cp/#installation","text":"Pydoxtools can be installed through pip: pip install pydoxtools[etl, inference] It should automatically download models & dependencies where required. In order to get the latest version install it like this: pip install git+https://github.com/xyntopia/pydoxtools'","title":"Installation"},{"location":"readme_cp/#use-cases","text":"analyze documents using any model from huggingface... analyze documents using a custom model download a pdf from URL generate document keywords extract tables download document from URL \"manually\" and then feed to document extract addresses extract addresses and use this information for the qam ingest documents into a vector db","title":"Use Cases"},{"location":"readme_cp/#development","text":"--> see","title":"Development"},{"location":"readme_cp/#license","text":"This project is licensed under the terms of MIT license. You can check the compatibility using the following tool in a venv environment in a production setting: pip install pip-licenses pip-licenses | grep -Ev 'MIT License|BSD License|Apache Software License|Python Software Foundation License|Apache 2.0|MIT|Apache License 2.0|hnswlib|Pillow|new BSD|BSD'","title":"License"},{"location":"readme_cp/#list-of-libraries-that-this-project-is-based-on","text":"list","title":"list of libraries, that this project is based on:"},{"location":"reference/","text":"Reference Document Bases: Pipeline This class implements an extensive pipeline using the class for information extraction from documents. In order to load a document, simply open it with the document class:: from pydoxtools import Document doc = Document(fobj=./data/demo.docx) You can then access any extracted data by calling x with the specified member:: doc.x(\"addresses\") doc.x(\"entities\") doc.x(\"full_text\") # etc... Most members are also callable like a normal class member in order to make the code easier to read:: doc.addresses A list of all available extraction data can be called like this:: doc.x_funcs() The document class is backed by a pipeline class with a pre-defined pipeline focusing on document extraction tasks. This extraction pipeline can be overwritten partially or completly replaced. In order to customize the pipeline it is usually best to take the pipeline for basic documents defined in pydoxtools.Document as a starting point and only overwrite the parts that should be customized. inherited classes can override any part of the graph. It is possible to exchange/override/extend or introduce extraction pipelines for individual file types (including the generic one: \" \") such as .html extractors, .pdf, .txt etc.. Strings inside a document class indicate the inclusion of that document type pipeline but with a lower priority this way a directed extraction graph gets built. This only counts for the current class that is being defined though!! Example extension pipeline for an OCR extractor which converts images into text \"image\" code block and supports filetypes: \".png\", \".jpeg\", \".jpg\", \".tif\", \".tiff\":: \"image\": [ OCRExtractor() .pipe(file=\"raw_content\") .out(\"ocr_pdf_file\") .cache(), ], # the first base doc types have priority over the last ones # so here .png > image > .pdf \".png\": [\"image\", \".pdf\"], \".jpeg\": [\"image\", \".pdf\"], \".jpg\": [\"image\", \".pdf\"], \".tif\": [\"image\", \".pdf\"], \".tiff\": [\"image\", \".pdf\"], # the \"*\" gets overwritten by functions above \"*\": [...] Each function (or node) in the extraction pipeline gets fed its input-parameters by the \"pipe\" command. These parameters can be configured on document creation if some of them are declared using the \"config\" command. These arguments can be overwritten by a new pipeline in inherited documents or document types that are higher up in the hierarchy. The argument precedence is hereby as follows:: python-class-member < extractor-graph-function < config document_type cached property detect doc type based on file-ending TODO add a doc-type extractor using for example python-magic filename : str | None cached property TODO: move this into document __init__ ( fobj = None , source = None , page_numbers = None , max_pages = None , mime_type = None , filename = None , document_type = None ) a file object which should be loaded. if it is a string or bytes object: the string itself is the document! if it is a pathlib.Path: load the document from the path if it is a file object: load document from file object (or bytestream etc...) source: Where does the extracted data come from? (Examples: URL, 'pdfupload', parent-URL, or a path)\" page_numbers: list of the specific pages that we would like to extract (for example in a pdf) max_pages: maximum number of pages that we want to extract in order to protect resources config: a dict which describes values for variables in the document logic mime_type: optional mimetype for the document filename: optional filename. Helps sometimes helps in determining the purpose of a document directly specify the document type which specifies the extraction logic that should be used Pipeline This class is the base for all document classes in pydoxtools and defines a common pipeline interface for all. This class also defines a basic extraction schema which derived classes can override x_funcs : dict [ str , Extractor ] cached property get all extractors and their property names for this specific file type __getattr__ ( extract_name ) getattr only gets called for non-existing variable names. So we can automatically avoid name collisions here. document.addresses instead of document.x['addresses'] config ( ** kwargs ) Set a standard configuration for a pipeline non_interactive_x_funcs () return all non-interactive extractors pipeline_graph ( image_path = None , document_logic_id = 'current' ) Generate a visualization of the defined pipelines image_path: file path for a generated image pre_cache () in some situations, for example for caching purposes it would be nice to pre-cache all calculations this is done here by simply calling all functions... run_all_extractors () can be used for testing or pre-caching purposes x ( extract_name , * args , ** kwargs ) call an extractor from our definition TODO: using args and *kwargs the extractors parameters can be overriden","title":"Reference"},{"location":"reference/#reference","text":"","title":"Reference"},{"location":"reference/#pydoxtools.document.Document","text":"Bases: Pipeline This class implements an extensive pipeline using the class for information extraction from documents. In order to load a document, simply open it with the document class:: from pydoxtools import Document doc = Document(fobj=./data/demo.docx) You can then access any extracted data by calling x with the specified member:: doc.x(\"addresses\") doc.x(\"entities\") doc.x(\"full_text\") # etc... Most members are also callable like a normal class member in order to make the code easier to read:: doc.addresses A list of all available extraction data can be called like this:: doc.x_funcs() The document class is backed by a pipeline class with a pre-defined pipeline focusing on document extraction tasks. This extraction pipeline can be overwritten partially or completly replaced. In order to customize the pipeline it is usually best to take the pipeline for basic documents defined in pydoxtools.Document as a starting point and only overwrite the parts that should be customized. inherited classes can override any part of the graph. It is possible to exchange/override/extend or introduce extraction pipelines for individual file types (including the generic one: \" \") such as .html extractors, .pdf, .txt etc.. Strings inside a document class indicate the inclusion of that document type pipeline but with a lower priority this way a directed extraction graph gets built. This only counts for the current class that is being defined though!! Example extension pipeline for an OCR extractor which converts images into text \"image\" code block and supports filetypes: \".png\", \".jpeg\", \".jpg\", \".tif\", \".tiff\":: \"image\": [ OCRExtractor() .pipe(file=\"raw_content\") .out(\"ocr_pdf_file\") .cache(), ], # the first base doc types have priority over the last ones # so here .png > image > .pdf \".png\": [\"image\", \".pdf\"], \".jpeg\": [\"image\", \".pdf\"], \".jpg\": [\"image\", \".pdf\"], \".tif\": [\"image\", \".pdf\"], \".tiff\": [\"image\", \".pdf\"], # the \"*\" gets overwritten by functions above \"*\": [...] Each function (or node) in the extraction pipeline gets fed its input-parameters by the \"pipe\" command. These parameters can be configured on document creation if some of them are declared using the \"config\" command. These arguments can be overwritten by a new pipeline in inherited documents or document types that are higher up in the hierarchy. The argument precedence is hereby as follows:: python-class-member < extractor-graph-function < config","title":"Document"},{"location":"reference/#pydoxtools.document.Document.document_type","text":"detect doc type based on file-ending TODO add a doc-type extractor using for example python-magic","title":"document_type"},{"location":"reference/#pydoxtools.document.Document.filename","text":"TODO: move this into document","title":"filename"},{"location":"reference/#pydoxtools.document.Document.__init__","text":"a file object which should be loaded. if it is a string or bytes object: the string itself is the document! if it is a pathlib.Path: load the document from the path if it is a file object: load document from file object (or bytestream etc...) source: Where does the extracted data come from? (Examples: URL, 'pdfupload', parent-URL, or a path)\" page_numbers: list of the specific pages that we would like to extract (for example in a pdf) max_pages: maximum number of pages that we want to extract in order to protect resources config: a dict which describes values for variables in the document logic mime_type: optional mimetype for the document filename: optional filename. Helps sometimes helps in determining the purpose of a document directly specify the document type which specifies the extraction logic that should be used","title":"__init__()"},{"location":"reference/#pydoxtools.document_base.Pipeline","text":"This class is the base for all document classes in pydoxtools and defines a common pipeline interface for all. This class also defines a basic extraction schema which derived classes can override","title":"Pipeline"},{"location":"reference/#pydoxtools.document_base.Pipeline.x_funcs","text":"get all extractors and their property names for this specific file type","title":"x_funcs"},{"location":"reference/#pydoxtools.document_base.Pipeline.__getattr__","text":"getattr only gets called for non-existing variable names. So we can automatically avoid name collisions here. document.addresses instead of document.x['addresses']","title":"__getattr__()"},{"location":"reference/#pydoxtools.document_base.Pipeline.config","text":"Set a standard configuration for a pipeline","title":"config()"},{"location":"reference/#pydoxtools.document_base.Pipeline.non_interactive_x_funcs","text":"return all non-interactive extractors","title":"non_interactive_x_funcs()"},{"location":"reference/#pydoxtools.document_base.Pipeline.pipeline_graph","text":"Generate a visualization of the defined pipelines image_path: file path for a generated image","title":"pipeline_graph()"},{"location":"reference/#pydoxtools.document_base.Pipeline.pre_cache","text":"in some situations, for example for caching purposes it would be nice to pre-cache all calculations this is done here by simply calling all functions...","title":"pre_cache()"},{"location":"reference/#pydoxtools.document_base.Pipeline.run_all_extractors","text":"can be used for testing or pre-caching purposes","title":"run_all_extractors()"},{"location":"reference/#pydoxtools.document_base.Pipeline.x","text":"call an extractor from our definition TODO: using args and *kwargs the extractors parameters can be overriden","title":"x()"}]}