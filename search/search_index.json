{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Pydoxtools Documentation! Readme Readme Visualization of the Extraction logic The extraction logic for different file types can be visualized bydoing something like this: doc = Document(fobj=make_path_absolute(\"./data/demo.docx\"), document_type=\".docx\") # for the currently loaded file type: doc.logic_graph(image_path=settings._PYDOXTOOLS_DIR / \"docs/images/document_logic_docx.svg\") # for the doc.logic_graph(image_path=settings._PYDOXTOOLS_DIR / \"docs/images/document_logic_png.svg\", document_logic_id=\".png\") This way we can generate the pipelines for different filetypes: docx png (click on links to open the images!) This also works for custom pipelines! In order to learn more continue to: Reference","title":"Home"},{"location":"#welcome-to-pydoxtools-documentation","text":"","title":"Welcome to Pydoxtools Documentation!"},{"location":"#readme","text":"Readme","title":"Readme"},{"location":"#visualization-of-the-extraction-logic","text":"The extraction logic for different file types can be visualized bydoing something like this: doc = Document(fobj=make_path_absolute(\"./data/demo.docx\"), document_type=\".docx\") # for the currently loaded file type: doc.logic_graph(image_path=settings._PYDOXTOOLS_DIR / \"docs/images/document_logic_docx.svg\") # for the doc.logic_graph(image_path=settings._PYDOXTOOLS_DIR / \"docs/images/document_logic_png.svg\", document_logic_id=\".png\") This way we can generate the pipelines for different filetypes: docx png (click on links to open the images!) This also works for custom pipelines! In order to learn more continue to: Reference","title":"Visualization of the Extraction logic"},{"location":"DEVELOPMENT/","text":"Development & Contribution The graph model of the library makes it very easy to extend it with new functionality. the document can be used as a base-model and overwritten with changes the graph can be changed dynamically new functions can be very easily integrated Pydoxtools Architecture --> refer to \"document\" Contribution Guidelines","title":"Development & Contribution"},{"location":"DEVELOPMENT/#development-contribution","text":"The graph model of the library makes it very easy to extend it with new functionality. the document can be used as a base-model and overwritten with changes the graph can be changed dynamically new functions can be very easily integrated","title":"Development &amp; Contribution"},{"location":"DEVELOPMENT/#pydoxtools-architecture","text":"--> refer to \"document\"","title":"Pydoxtools Architecture"},{"location":"DEVELOPMENT/#contribution-guidelines","text":"","title":"Contribution Guidelines"},{"location":"readme_cp/","text":"\ud83d\ude80 pydoxtools (Python Library) \ud83d\ude80 ( WIP ) Documentation Pydoxtools is a library that provides a sophisticated interface for reading and writing documents, designed to work with AI models such as GPT, Alpaca, and Huggingface. It offers functionalities such as: Table extraction Vector Index Creation Document analysis and question-answering Entity, address identification and more List and keyword extraction Data normalization, translation, and cleaning The library allows for the creation of complex extraction pipelines for batch-processing of documents by defining them as a lazily-executed graph. Installation While pydoxtools can already be installed through pip. Due to the many updates coming in right now, it is right now recommended to use the latest version from github as follows: pip install -U \"pydoxtools[etl,inference] @ git+https://github.com/xyntopia/pydoxtools.git\" Pydoxtools can be also be installed through pip which will become the recommended method once it becomes more stable: pip install -U pydoxtools[etl,inference] For loading additional file formats (docx, odt, epub) and images, checkout the additional > Installation Options <. Teaser Experience a new level of convenience and efficiency in handling documents with Pydoxtools, and reimagine your data extraction pipelines! \ud83c\udfa9\u2728\ud83d\udcc4. import pydoxtools as pdx # create a document from a file, string, bytestring, file-like object # or even an url: doc = pdx.Document( \"https://www.raspberrypi.org/app/uploads/2012/12/quick-start-guide-v1.1.pdf\", document_type=\".pdf\" ) You can easily extract a large number of pre-defined information about your document. To get a list of possible operators use print(doc.x_funcs) . # extract tables from the pdf as a pandas dataframe: print(doc.tables_df) Some extraction operations need input when called: # ask a question about the document, using Q&A Models (questionas answered locally!): print(doc.answers([\"how much ram does it have?\"])) others need an API key installed, if it refers to an online service. # ask a question about the document, using ChatGPT (we need the API key for ChatGPT!): # load the API key into an environment variable like this: # # OPENAI_API_KEY=\"sk ....\" # # Do **NOT** use the key in your code. This could potentially cost you a lot of money... print(doc.chat_answers([\"What is the target group of this document?\"])[0].content) print(doc.chat_answers([\"Answer if a 5-year old would be able to follow these instructions?\"])[0].content) Features Large pipelines Pydoxtools main feature is the ability to mix LLMs and other AI models in large, composable and customizable pipelines. As a teaser, check out this pipeline for *.png images from the repository including OCR, keyword extraction, vectorization and more. In this pipeline: Every node in an ellipse can be called as an attribute of the document-analysis pipeline. Every execution-path is lazily executed throughout the entire graph. Every node is cached by default (can be turned off). Every piece of this pipeline can be replaced by a customized version. Pipelines can be mixed, partially overwritten and extended which gives you a lot of possibilities to extend and adapt the functionality for your specific use-case. Find out more about it in the documentation Pipeline configuration Pipelines can be configured. For example the local model used for question answering can be selected like this: doc = Document(fobj=\"./data/PFR-PR23_BAT-110__V1.00_.pdf\")) .config(qam_model_id='bert-large-uncased-whole-word-masking-finetuned-squad') where \"qam_model_id\" can be any model from huggingface for question answering. TODO: document how to configure a pipeline PDF table extraction algorithms The library features its own sophisticated Table extraction algorithm which is benchmarked against a large pdf table dataset. In contrast to most other table extraction frameworks out there it does not require: extensive configuration no expensive deep neural networks which need a GPU This makes it possible to run analysis on PDF files with pydoxtools on CPU with very limited resources! TODO: Describe more of the features here... Use Cases analyze documents using any model from huggingface... analyze documents using a custom model download a pdf from URL generate document keywords extract tables download document from URL \"manually\" and then feed to document extract addresses extract addresses and use this information for the qam ingest documents into a vector db Installation Options Supporting .docx, .odt, *.epub In order to be able to load docx, odt and rtf files, you have to install pandoc. Right now, the python pandoc library does not work with pandoc version > 3.0.0. It is therefore recommended to install a version from here for your OS: https://github.com/jgm/pandoc/releases/tag/2.19.2 Image OCR support Pydoxtools can automatically analyze images as well, makin use of OCR . In order to be able to use this, install tesseract on your system: Under linux this looks like the following: apt-get update && tesseract-ocr # install tesseract languages # Display a list of all Tesseract language packs: # apt-cache search tesseract-ocr # install all languages: # sudo apt install tesseract-ocr-* # install only german, french, english, spanish language packs # sudo apt install tesseract-ocr-deu tesseract-ocr-fra tesseract-ocr-eng tesseract-ocr-spa Development --> see License This project is licensed under the terms of MIT license. You can check the compatibility using the following tool in a venv environment in a production setting: pip install pip-licenses pip-licenses | grep -Ev 'MIT License|BSD License|Apache Software License|Python Software Foundation License|Apache 2.0|MIT|Apache License 2.0|hnswlib|Pillow|new BSD|BSD' list of libraries, that this project is based on: list","title":"Readme"},{"location":"readme_cp/#pydoxtools-python-library","text":"( WIP ) Documentation Pydoxtools is a library that provides a sophisticated interface for reading and writing documents, designed to work with AI models such as GPT, Alpaca, and Huggingface. It offers functionalities such as: Table extraction Vector Index Creation Document analysis and question-answering Entity, address identification and more List and keyword extraction Data normalization, translation, and cleaning The library allows for the creation of complex extraction pipelines for batch-processing of documents by defining them as a lazily-executed graph.","title":"\ud83d\ude80 pydoxtools (Python Library) \ud83d\ude80"},{"location":"readme_cp/#installation","text":"While pydoxtools can already be installed through pip. Due to the many updates coming in right now, it is right now recommended to use the latest version from github as follows: pip install -U \"pydoxtools[etl,inference] @ git+https://github.com/xyntopia/pydoxtools.git\" Pydoxtools can be also be installed through pip which will become the recommended method once it becomes more stable: pip install -U pydoxtools[etl,inference] For loading additional file formats (docx, odt, epub) and images, checkout the additional > Installation Options <.","title":"Installation"},{"location":"readme_cp/#teaser","text":"Experience a new level of convenience and efficiency in handling documents with Pydoxtools, and reimagine your data extraction pipelines! \ud83c\udfa9\u2728\ud83d\udcc4. import pydoxtools as pdx # create a document from a file, string, bytestring, file-like object # or even an url: doc = pdx.Document( \"https://www.raspberrypi.org/app/uploads/2012/12/quick-start-guide-v1.1.pdf\", document_type=\".pdf\" ) You can easily extract a large number of pre-defined information about your document. To get a list of possible operators use print(doc.x_funcs) . # extract tables from the pdf as a pandas dataframe: print(doc.tables_df) Some extraction operations need input when called: # ask a question about the document, using Q&A Models (questionas answered locally!): print(doc.answers([\"how much ram does it have?\"])) others need an API key installed, if it refers to an online service. # ask a question about the document, using ChatGPT (we need the API key for ChatGPT!): # load the API key into an environment variable like this: # # OPENAI_API_KEY=\"sk ....\" # # Do **NOT** use the key in your code. This could potentially cost you a lot of money... print(doc.chat_answers([\"What is the target group of this document?\"])[0].content) print(doc.chat_answers([\"Answer if a 5-year old would be able to follow these instructions?\"])[0].content)","title":"Teaser"},{"location":"readme_cp/#features","text":"","title":"Features"},{"location":"readme_cp/#large-pipelines","text":"Pydoxtools main feature is the ability to mix LLMs and other AI models in large, composable and customizable pipelines. As a teaser, check out this pipeline for *.png images from the repository including OCR, keyword extraction, vectorization and more. In this pipeline: Every node in an ellipse can be called as an attribute of the document-analysis pipeline. Every execution-path is lazily executed throughout the entire graph. Every node is cached by default (can be turned off). Every piece of this pipeline can be replaced by a customized version. Pipelines can be mixed, partially overwritten and extended which gives you a lot of possibilities to extend and adapt the functionality for your specific use-case. Find out more about it in the documentation","title":"Large pipelines"},{"location":"readme_cp/#pipeline-configuration","text":"Pipelines can be configured. For example the local model used for question answering can be selected like this: doc = Document(fobj=\"./data/PFR-PR23_BAT-110__V1.00_.pdf\")) .config(qam_model_id='bert-large-uncased-whole-word-masking-finetuned-squad') where \"qam_model_id\" can be any model from huggingface for question answering. TODO: document how to configure a pipeline","title":"Pipeline configuration"},{"location":"readme_cp/#pdf-table-extraction-algorithms","text":"The library features its own sophisticated Table extraction algorithm which is benchmarked against a large pdf table dataset. In contrast to most other table extraction frameworks out there it does not require: extensive configuration no expensive deep neural networks which need a GPU This makes it possible to run analysis on PDF files with pydoxtools on CPU with very limited resources!","title":"PDF table extraction algorithms"},{"location":"readme_cp/#todo-describe-more-of-the-features-here","text":"","title":"TODO: Describe more of the features here..."},{"location":"readme_cp/#use-cases","text":"analyze documents using any model from huggingface... analyze documents using a custom model download a pdf from URL generate document keywords extract tables download document from URL \"manually\" and then feed to document extract addresses extract addresses and use this information for the qam ingest documents into a vector db","title":"Use Cases"},{"location":"readme_cp/#installation-options","text":"","title":"Installation Options"},{"location":"readme_cp/#supporting-docx-odt-epub","text":"In order to be able to load docx, odt and rtf files, you have to install pandoc. Right now, the python pandoc library does not work with pandoc version > 3.0.0. It is therefore recommended to install a version from here for your OS: https://github.com/jgm/pandoc/releases/tag/2.19.2","title":"Supporting .docx, .odt, *.epub"},{"location":"readme_cp/#image-ocr-support","text":"Pydoxtools can automatically analyze images as well, makin use of OCR . In order to be able to use this, install tesseract on your system: Under linux this looks like the following: apt-get update && tesseract-ocr # install tesseract languages # Display a list of all Tesseract language packs: # apt-cache search tesseract-ocr # install all languages: # sudo apt install tesseract-ocr-* # install only german, french, english, spanish language packs # sudo apt install tesseract-ocr-deu tesseract-ocr-fra tesseract-ocr-eng tesseract-ocr-spa","title":"Image OCR support"},{"location":"readme_cp/#development","text":"--> see","title":"Development"},{"location":"readme_cp/#license","text":"This project is licensed under the terms of MIT license. You can check the compatibility using the following tool in a venv environment in a production setting: pip install pip-licenses pip-licenses | grep -Ev 'MIT License|BSD License|Apache Software License|Python Software Foundation License|Apache 2.0|MIT|Apache License 2.0|hnswlib|Pillow|new BSD|BSD'","title":"License"},{"location":"readme_cp/#list-of-libraries-that-this-project-is-based-on","text":"list","title":"list of libraries, that this project is based on:"},{"location":"reference/","text":"Reference Document Bases: Pipeline This class implements an extensive pipeline using the class for information extraction from documents. In order to load a document, simply open it with the document class:: from pydoxtools import Document doc = Document(fobj=./data/demo.docx) You can then access any extracted data by calling x with the specified member:: doc.x(\"addresses\") doc.x(\"entities\") doc.x(\"full_text\") # etc... Most members are also callable like a normal class member in order to make the code easier to read:: doc.addresses A list of all available extraction data can be called like this:: doc.x_funcs() The document class is backed by a pipeline class with a pre-defined pipeline focusing on document extraction tasks. This extraction pipeline can be overwritten partially or completly replaced. In order to customize the pipeline it is usually best to take the pipeline for basic documents defined in pydoxtools.Document as a starting point and only overwrite the parts that should be customized. inherited classes can override any part of the graph. It is possible to exchange/override/extend or introduce extraction pipelines for individual file types (including the generic one: \" \") such as .html extractors, .pdf, .txt etc.. Strings inside a document class indicate the inclusion of that document type pipeline but with a lower priority this way a directed extraction graph gets built. This only counts for the current class that is being defined though!! Example extension pipeline for an OCR extractor which converts images into text \"image\" code block and supports filetypes: \".png\", \".jpeg\", \".jpg\", \".tif\", \".tiff\":: \"image\": [ OCRExtractor() .pipe(file=\"raw_content\") .out(\"ocr_pdf_file\") .cache(), ], # the first base doc types have priority over the last ones # so here .png > image > .pdf \".png\": [\"image\", \".pdf\"], \".jpeg\": [\"image\", \".pdf\"], \".jpg\": [\"image\", \".pdf\"], \".tif\": [\"image\", \".pdf\"], \".tiff\": [\"image\", \".pdf\"], # the \"*\" gets overwritten by functions above \"*\": [...] Each function (or node) in the extraction pipeline gets connected to other nodes in the pipeline by the \"pipe\" command. These arguments can be overwritten by a new pipeline in inherited documents or document types that are higher up in the hierarchy. The argument precedence is hereby as follows:: python-class-member < extractor-graph-function < configuration when creating a new pipeline for documentation purposes a general rule is: if the operation is too complicated to be self-describing, then use a function or class and put the documentation in there. A Lambda function is not the right tool in this case. document_type property cached detect doc type based on file-ending TODO add a doc-type extractor using for example python-magic filename : str | None property cached TODO: move this into document pipeline __init__ ( fobj = None , source = None , page_numbers = None , max_pages = None , mime_type = None , filename = None , document_type = None ) a file object which should be loaded. if it is a string or bytes object: the string itself is the document! if it is a pathlib.Path: load the document from the path if it is a file object: load document from file object (or bytestream etc...) source: Where does the extracted data come from? (Examples: URL, 'pdfupload', parent-URL, or a path)\" page_numbers: list of the specific pages that we would like to extract (for example in a pdf) max_pages: maximum number of pages that we want to extract in order to protect resources mime_type: optional mimetype for the document filename: optional filename. Helps sometimes helps in determining the purpose of a document directly specify the document type which specifies the extraction logic that should be used Pipeline This class is the base for all document classes in pydoxtools and defines a common pipeline interface for all. This class also defines a basic extraction schema which derived classes can override in order to create a new pipeline, the _extractor variable shoud be overwritten with a pipeline definition this pipeline will get compiled in a function mappint defined in _x_funcs configuration property Get all configuration objects of our pipeline and merge them into a dict. x_funcs : dict [ str , operators . Operator ] property cached get all extractors and their property names for this specific file type __getattr__ ( extract_name ) getattr only gets called for non-existing variable names. So we can automatically avoid name collisions here. document.addresses instead of document.x['addresses'] config ( ** settings ) Set configuration parameters for a pipeline This function loops through all \"operators.Configure\" and assigns the configuration from **settings to them. json ( * args , ** kwargs ) same as property_dict, but dumps output as yaml non_interactive_x_funcs () return all non-interactive extractors pipeline_graph ( image_path = None , document_logic_id = 'current' ) Generate a visualization of the defined pipelines image_path: file path for a generated image pre_cache () in some situations, for example for caching purposes it would be nice to pre-cache all calculations this is done here by simply calling all functions... property_dict ( * args , ** kwargs ) return a dictionary which accumulates the properties given in args or with a mapping in *kwargs where the keys in kwargs are the variable in the returned dictionary, whereas the values are the variable names of the pipeline. Right now, this only works for properties that don#t need any arguments, in pipelines such as \"full_text\". Others, such as \"answers\" return a function which needs arguments itself and can therefore not be used here. run_all_extractors () can be used for testing or pre-caching purposes x ( extract_name , * args , ** kwargs ) call an extractor from our definition TODO: using args and *kwargs the extractors parameters can be overriden yaml ( * args , ** kwargs ) same as property_dict, but dumps output as yaml","title":"Reference"},{"location":"reference/#reference","text":"","title":"Reference"},{"location":"reference/#pydoxtools.document.Document","text":"Bases: Pipeline This class implements an extensive pipeline using the class for information extraction from documents. In order to load a document, simply open it with the document class:: from pydoxtools import Document doc = Document(fobj=./data/demo.docx) You can then access any extracted data by calling x with the specified member:: doc.x(\"addresses\") doc.x(\"entities\") doc.x(\"full_text\") # etc... Most members are also callable like a normal class member in order to make the code easier to read:: doc.addresses A list of all available extraction data can be called like this:: doc.x_funcs() The document class is backed by a pipeline class with a pre-defined pipeline focusing on document extraction tasks. This extraction pipeline can be overwritten partially or completly replaced. In order to customize the pipeline it is usually best to take the pipeline for basic documents defined in pydoxtools.Document as a starting point and only overwrite the parts that should be customized. inherited classes can override any part of the graph. It is possible to exchange/override/extend or introduce extraction pipelines for individual file types (including the generic one: \" \") such as .html extractors, .pdf, .txt etc.. Strings inside a document class indicate the inclusion of that document type pipeline but with a lower priority this way a directed extraction graph gets built. This only counts for the current class that is being defined though!! Example extension pipeline for an OCR extractor which converts images into text \"image\" code block and supports filetypes: \".png\", \".jpeg\", \".jpg\", \".tif\", \".tiff\":: \"image\": [ OCRExtractor() .pipe(file=\"raw_content\") .out(\"ocr_pdf_file\") .cache(), ], # the first base doc types have priority over the last ones # so here .png > image > .pdf \".png\": [\"image\", \".pdf\"], \".jpeg\": [\"image\", \".pdf\"], \".jpg\": [\"image\", \".pdf\"], \".tif\": [\"image\", \".pdf\"], \".tiff\": [\"image\", \".pdf\"], # the \"*\" gets overwritten by functions above \"*\": [...] Each function (or node) in the extraction pipeline gets connected to other nodes in the pipeline by the \"pipe\" command. These arguments can be overwritten by a new pipeline in inherited documents or document types that are higher up in the hierarchy. The argument precedence is hereby as follows:: python-class-member < extractor-graph-function < configuration when creating a new pipeline for documentation purposes a general rule is: if the operation is too complicated to be self-describing, then use a function or class and put the documentation in there. A Lambda function is not the right tool in this case.","title":"Document"},{"location":"reference/#pydoxtools.document.Document.document_type","text":"detect doc type based on file-ending TODO add a doc-type extractor using for example python-magic","title":"document_type"},{"location":"reference/#pydoxtools.document.Document.filename","text":"TODO: move this into document pipeline","title":"filename"},{"location":"reference/#pydoxtools.document.Document.__init__","text":"a file object which should be loaded. if it is a string or bytes object: the string itself is the document! if it is a pathlib.Path: load the document from the path if it is a file object: load document from file object (or bytestream etc...) source: Where does the extracted data come from? (Examples: URL, 'pdfupload', parent-URL, or a path)\" page_numbers: list of the specific pages that we would like to extract (for example in a pdf) max_pages: maximum number of pages that we want to extract in order to protect resources mime_type: optional mimetype for the document filename: optional filename. Helps sometimes helps in determining the purpose of a document directly specify the document type which specifies the extraction logic that should be used","title":"__init__()"},{"location":"reference/#pydoxtools.document_base.Pipeline","text":"This class is the base for all document classes in pydoxtools and defines a common pipeline interface for all. This class also defines a basic extraction schema which derived classes can override in order to create a new pipeline, the _extractor variable shoud be overwritten with a pipeline definition this pipeline will get compiled in a function mappint defined in _x_funcs","title":"Pipeline"},{"location":"reference/#pydoxtools.document_base.Pipeline.configuration","text":"Get all configuration objects of our pipeline and merge them into a dict.","title":"configuration"},{"location":"reference/#pydoxtools.document_base.Pipeline.x_funcs","text":"get all extractors and their property names for this specific file type","title":"x_funcs"},{"location":"reference/#pydoxtools.document_base.Pipeline.__getattr__","text":"getattr only gets called for non-existing variable names. So we can automatically avoid name collisions here. document.addresses instead of document.x['addresses']","title":"__getattr__()"},{"location":"reference/#pydoxtools.document_base.Pipeline.config","text":"Set configuration parameters for a pipeline This function loops through all \"operators.Configure\" and assigns the configuration from **settings to them.","title":"config()"},{"location":"reference/#pydoxtools.document_base.Pipeline.json","text":"same as property_dict, but dumps output as yaml","title":"json()"},{"location":"reference/#pydoxtools.document_base.Pipeline.non_interactive_x_funcs","text":"return all non-interactive extractors","title":"non_interactive_x_funcs()"},{"location":"reference/#pydoxtools.document_base.Pipeline.pipeline_graph","text":"Generate a visualization of the defined pipelines image_path: file path for a generated image","title":"pipeline_graph()"},{"location":"reference/#pydoxtools.document_base.Pipeline.pre_cache","text":"in some situations, for example for caching purposes it would be nice to pre-cache all calculations this is done here by simply calling all functions...","title":"pre_cache()"},{"location":"reference/#pydoxtools.document_base.Pipeline.property_dict","text":"return a dictionary which accumulates the properties given in args or with a mapping in *kwargs where the keys in kwargs are the variable in the returned dictionary, whereas the values are the variable names of the pipeline. Right now, this only works for properties that don#t need any arguments, in pipelines such as \"full_text\". Others, such as \"answers\" return a function which needs arguments itself and can therefore not be used here.","title":"property_dict()"},{"location":"reference/#pydoxtools.document_base.Pipeline.run_all_extractors","text":"can be used for testing or pre-caching purposes","title":"run_all_extractors()"},{"location":"reference/#pydoxtools.document_base.Pipeline.x","text":"call an extractor from our definition TODO: using args and *kwargs the extractors parameters can be overriden","title":"x()"},{"location":"reference/#pydoxtools.document_base.Pipeline.yaml","text":"same as property_dict, but dumps output as yaml","title":"yaml()"}]}