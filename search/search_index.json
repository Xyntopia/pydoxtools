{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Pydoxtools Documentation!","text":"<p>For a short overview over Pydoxtools, checkout the readme:</p> <p>Readme</p>"},{"location":"#introduction","title":"Introduction","text":"<p>Pydoxtools provides a user-friendly interface for document analysis and manipulation consisting of three main classes:</p> <ul> <li>pydoxtools.Document</li> <li>pydoxtools.DocumentBag</li> <li>pydoxtools.Pipeline</li> </ul> <p>The Pipeline class is the base of it and helps to compose dynamic pipelines which adapt to the underlying data.</p> <p>Notes</p> <p>Support for agents was removed in Pydoxtools &gt;= 0.8.0. The functionality can now be found in another app: (taskyon.space)[https://taskyon.space]</p>"},{"location":"#analyzing-documents","title":"Analyzing Documents","text":"<p>Both, Document and DocumentBag utilize pydoxtools.Pipeline to define a sophisticated pipeline for extracting data from individual or multiple documents. You can find a list of all the built-in features for each pipeline here:</p> <p>-&gt; pydoxtools.Document and pydoxtools.DocumentBag</p> <p>To ensure seamless operation, Pydoxtools is designed so that Document and DocumentBag automatically organize information in a logical manner while minimizing memory and CPU usage. This approach makes the library highly compatible with AI and LLMs in automated settings. As a result, it is not possible to configure how documents are loaded using configuration parameters. However, you can easily achieve specific data organization by chaining documents together.</p> <p>TODO:  provide an example</p>"},{"location":"#building-custom-pipelines-with-llms-large-language-models-and-other-ai-tools","title":"Building Custom Pipelines with LLMs (Large Language Models) and other AI Tools","text":"<p>The Pipeline class allows you to create complex, custom pipelines that come with several built-in features, making them easy to integrate with modern AI tools:</p> <ul> <li>Mix, extend, or (partially) overwrite pipelines</li> <li>Export/import data (yaml, json, python-dict)</li> <li>Configure and optimize pipelines</li> <li>Convert data into pydoxtools.Document and pydoxtools.DocumentBag</li> </ul> <p>To develop a custom pipeline, you can utilize the extensive library of pydoxtools.operators. It is generally recommended to use pydoxtools.Document or pydoxtools.DocumentBag as a base for a new pipeline and only replace small parts to achieve the desired custom functionality.</p>"},{"location":"#visualizing-pipelines","title":"Visualizing Pipelines","text":"<p>Visualizing pipelines can be incredibly helpful when developing your own pipeline on top of a complex one, such as the document pipeline. You can visualize the extraction logic for different file types from the Document class (which is a pydoxtools.Pipeline  itself) as follows:</p> <pre><code>doc = Document(fobj=make_path_absolute(\"./data/demo.docx\"))\n# for the currently loaded file type:\ndoc.logic_graph(image_path=settings._PYDOXTOOLS_DIR / \"docs/images/document_logic_docx.svg\")\n# for the \ndoc.logic_graph(image_path=settings._PYDOXTOOLS_DIR / \"docs/images/document_logic_png.svg\", document_logic_id=\".png\")\n</code></pre> <p>This allows you to generate pipelines for various file types. For example:</p> <ul> <li>docx</li> <li>png   (click on links to open the images!)</li> </ul> <p>You can find pipelines for every supported file type here.</p> <p>This feature is also available for custom pipelines!</p> <p>To learn more, continue to: Reference</p>"},{"location":"DEVELOPMENT/","title":"Development &amp; Contribution","text":"<p>The graph model of the library makes it very easy to extend it with new functionality.</p> <ul> <li>the document can be used as a base-model and overwritten with changes</li> <li>the graph can be changed dynamically</li> <li>new functions can be very easily integrated</li> </ul>"},{"location":"DEVELOPMENT/#installation-from-other-branches","title":"Installation from other branches","text":"<p>In order to install pydoxtools from a development branch \"development_branch\" you can do this:</p> <p>pip install -U \"pydoxtools[etl,inference] @ git+https://github.com/xyntopia/pydoxtools.git@development_branch\"</p>"},{"location":"DEVELOPMENT/#pydoxtools-architecture","title":"Pydoxtools Architecture","text":"<p>--&gt; refer to \"document\"</p>"},{"location":"DEVELOPMENT/#testing","title":"Testing","text":"<p>For unit-testing, the test dataset is needed. You can download it with this commmand:</p> <pre><code>poetry run clone-data\n</code></pre> <p>As of Jan 2024, only one dataset with some proprietary data is available an \"open\" dataset is being worked on. Contact the author of pydoxtools if you would like to have access to the test dataset.</p>"},{"location":"DEVELOPMENT/#contribution-guidelines","title":"Contribution Guidelines","text":""},{"location":"document/","title":"pydoxtools.Document","text":"<p>             Bases: <code>Pipeline</code></p> <p>Basic document pipeline class to analyze documents from all kinds of formats.</p> <p>A list and documentation of all document analysis related functions can be found -&gt;here&lt;-.</p> <p>The Document class is designed for information extraction from documents. It inherits from the pydoxtools.document_base.Pipeline class and uses a predefined extraction pipeline focused on document processing tasks. To load a document, create an instance of the Document class with a file path, a file object, a string, a URL or give it some data directly as a dict:</p> <pre><code>from pydoxtools import Document\ndoc = Document(fobj=Path('./data/demo.docx'))\n</code></pre> <p>Extracted data can be accessed by calling the <code>x</code> method with the specified output in the pipeline:</p> <pre><code>doc.x(\"addresses\")\ndoc.x(\"entities\")\ndoc.x(\"full_text\")\n# etc...\n</code></pre> <p>Most members can also be called as normal class attributes for easier readability:</p> <pre><code>doc.addresses\n</code></pre> <p>Additionally, it is possible to get the data directly in dict, yaml or json form:</p> <pre><code>doc.property_dict(\"addresses\",\"filename\",\"keywords\")\ndoc.yaml(\"addresses\",\"filename\",\"keywords\")\ndoc.json(\"addresses\",\"filename\",\"keywords\")\n</code></pre> <p>To retrieve a list of all available extraction data methods, call the <code>x_funcs()</code> method:</p> <pre><code>doc.x_funcs()\n</code></pre>"},{"location":"document/#pydoxtools.Document--customizing-the-document-pipeline","title":"Customizing the Document Pipeline:","text":"<p>The extraction pipeline can be partially overwritten or completely replaced to customize the document processing. To customize the pipeline, it's recommended to use the basic document pipeline defined in <code>pydoxtools.Document</code> as a starting point and only overwrite parts as needed.</p> <p>Inherited classes can override any part of the graph. To exchange, override, extend or introduce extraction pipelines for specific file types (including the generic one: \"\"), such as .html, .pdf, .txt, etc., follow the example below.</p>"},{"location":"document/#pydoxtools.Document--rules-for-customizing-the-extraction-pipeline","title":"Rules for customizing the extraction pipeline:","text":"<ul> <li>The pipeline is defined as a dictionary of several lists of pydoxtools.operator_base.Operator   nodes.</li> <li>Each pydoxtools.operator_base.Operator defines a set of output &amp; input valus through   the <code>out</code> and <code>input</code> methods.   The <code>input</code> method takes a dictionary   or list of input values and the <code>out</code> method takes a dictionary or list of output values.</li> <li>Operator nodes are configured through method chaining.</li> <li>Arguments can be overwritten by a new pipeline in inherited   documents or document types higher up in the hierarchy. The argument precedence is as follows:</li> </ul> <p><code>python-class-member &lt; extractor-graph-function &lt; configuration</code></p> <ul> <li>the different lists in the dictionary represent a \"hierarchy\" of pipelines which   can be combined in different ways by referencing each other. For example the \"png\" pipeline   references the \"image\" pipeline which in turn references the \"pdf\" pipeline. All pipelines   fall back to the \"*\" pipeline which is the most generic one.</li> </ul> <p>The way this looks is like this:</p> <pre><code>_operators = {\n        # .pdf-specific pipeline\n        \"application/pdf\": [*PDFNodes],\n        # image specific pipeline (does OCR)\n        \"image\": [*OCRNodes],\n        # .png-specific pipeline\n        \".png\": [\"image\", \"application/pdf\"],\n        # base pipeline\n        \"*\": [*BaseNodes],\n        }\n\nHere, the \"image\" pipeline overwrites the \"application/pdf\" pipeline for .png files. The \"application/pdf\"\npipeline on the other hand overwrites the \"*\" pipeline for .pdf files. The \"*\" pipeline doesn't not need\nto be specified, as it will always be the fallback pipeline. This way it is possible\nto dynamically adapt a pipeline to different types of input data. In the document pipeline\nthis is used to dynamically adapt the pipeline to different file types.\n\nWhen customizing, it is possible to derive a new class from [pydoxtools.Document][] and partially\noverwrite its hierarchy for your purposes. For example, if you want to add a new pipeline for Component\nextraction one could do something like the following: This will add a few more nodes\nto the generic pipeline in order to extract product information from documents. This would\nnow already work for all the document types defined in the base [pydoxtools.Document][] class!\n\n```python\nclass DocumentX(pydoxtools.Document):\n_operators = {\n    \"*\": [\n        FunctionOperator(get_products_from_pages).input(\"page_templates\", pages=\"page_set\")\n        .out(product_information=\"product_information\").cache(),\n        FunctionOperator(lambda tables: [componardo.spec_utils.table2specs(t) for t in tables])\n        .input(\"tables\").out(\"raw_specs\")\n        .cache().t(list[componardo.spec_utils.Specification])\n        .docs(\"transform tables into a list of specs\"),\n        FunctionOperator(lambda x: [ComponentExtractor([x]).component])\n        .t(componardo.extract_product.ComponentX)\n        .input(x=\"document_extract\").out(\"products\").cache()\n        .docs(\"Extract products from Documents\"),\n    ]\n}\n```\n</code></pre> <p>When creating a new pipeline for documentation purposes, use a function or class for complex operations and include the documentation there. Lambda functions should not be used in this case.</p>"},{"location":"document/#pydoxtools.Document.document_type","title":"<code>document_type</code>  <code>property</code>","text":"<p>This has to be done in a member function and not in the pipeline, because the selection of the pipeline depends on this...</p>"},{"location":"document/#pydoxtools.Document.filename","title":"<code>filename: str | None</code>  <code>property</code>","text":"<p>return filename or some other identifier of a file</p>"},{"location":"document/#pydoxtools.Document.__init__","title":"<code>__init__(fobj=None, source=None, meta=None, document_type='auto', page_numbers=None, max_pages=None, configuration=None, **kwargs)</code>","text":"<p>Initialize a Document instance.</p> <p>Either fobj or source are required. They can both be given. If either of them isn't specified the other one is inferred automatically.</p> <p>document_type, page_number and max_pages are also not required, but can be used to override the default behaviour. specifically document_tgiype can be used manually specify the pipeline that should be used.</p> <p>Parameters:</p> Name Type Description Default <code>fobj</code> <code>str | bytes | Path | IO | dict | list | set</code> <p>The file object or data to load. Depending on the type of object passed: - If a string or bytes object: the object itself is the document. IN case of a bytes      object, the source helps in determining the filetype through file endings. - If a string representing a URL: the document will be loaded from the URL. - If a pathlib.Path object: load the document from the path. - If a file object: load the document from the file object (e.g., bytestream). - If a python dict object: interprete a \"dict\" as a document - If a python list object: interprete a \"list\" as a document</p> <code>None</code> <code>source</code> <code>str | Path</code> <p>The source of the extracted data (e.g., URL, 'pdfupload', parent-URL, or a path). source is given in addition to fobj it overrides the automatically inferred source. A special case applies if our document is a dataobject from a database. In that case the index key from the database should be used as source. This facilitates downstream tasks immensely where we have to refer back to where the data came from.</p> <p>This also applies for \"explode\" operations on documents where the newly created documents will all try to trace their origin using the \"source\" attribute</p> <code>None</code> <code>document_type</code> <code>str</code> <p>The document type to directly specify the pipeline to be used. If \"auto\" is given it will try to be inferred automatically. For example in some cases we would like to have a string given in fobj not to be loaded as a file but actually be used as raw \"string\" data. In this case we can explicitly specify document_type=\"string\"</p> <code>'auto'</code> <code>meta</code> <code>dict[str, str]</code> <p>Optionally set document metadata, which can be very useful for downstream tasks like building an index.</p> <code>None</code> <code>page_numbers</code> <code>list[int]</code> <p>A list of specific pages to extract from the document (e.g., in a PDF).</p> <code>None</code> <code>max_pages</code> <code>int</code> <p>The maximum number of pages to extract to protect resources.</p> <code>None</code> <code>configuration</code> <code>dict</code> <p>configuration dictionary for the pipeline</p> <code>None</code>"},{"location":"document/#pydoxtools.Document.__repr__","title":"<code>__repr__()</code>","text":"<p>Returns:</p> Name Type Description <code>str</code> <p>A string representation of the instance.</p>"},{"location":"document/#pydoxtools.Document.document_type_detection","title":"<code>document_type_detection()</code>  <code>cached</code>","text":"<p>This one here is actually important as it detects the type of data that we are going to use for out pipeline. That is also why this is implemented as a member function and can not be pushed in the pipeline itself, because in needs to be run in order to select which pipline we are going to use.</p> <p>detect doc type based on various criteria TODO add a doc-type extractor using for example python-magic</p>"},{"location":"document/#text-extraction-attributes-and-functions","title":"Text extraction attributes and functions","text":"<p>The pydoxtools.Document is built on the pydoxtools.Pipeline class and most of the text extraction functionality makes extensive use of the pipeline features. All attributes and functions that are created by the pipeline are documented here.</p> <p>Pipeline visualizations for the structure of the Document pipelines for different document types can be found here.</p>"},{"location":"document/#dg","title":"DG","text":"<p>Alias for: </p> <p>* document_graph-&gt;DG (output)</p> <p>name : <code>&lt;Document&gt;.x('DG') or &lt;Document&gt;.DG</code></p> <p>return type : &lt;class 'networkx.classes.digraph.DiGraph'&gt;</p> <p>supports pipeline flows : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#a_d_ratio","title":"a_d_ratio","text":"<p>Letter/digit ratio of the text</p> <p>name : <code>&lt;Document&gt;.x('a_d_ratio') or &lt;Document&gt;.a_d_ratio</code></p> <p>return type : &lt;class 'float'&gt;</p> <p>supports pipeline flows : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#addresses","title":"addresses","text":"<p>get addresses from text</p> <p>name : <code>&lt;Document&gt;.x('addresses') or &lt;Document&gt;.addresses</code></p> <p>return type : list[str]</p> <p>supports pipeline flows : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#answers","title":"answers","text":"<p>Extract answers from the text using the Huggingface question answering pipeline</p> <p>name : <code>&lt;Document&gt;.x('answers') or &lt;Document&gt;.answers</code></p> <p>return type : typing.Any</p> <p>supports pipeline flows : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#chat_answers","title":"chat_answers","text":"<p>Extract answers from the text using OpenAI Chat GPT and other models.</p> <p>name : <code>&lt;Document&gt;.x('chat_answers') or &lt;Document&gt;.chat_answers</code></p> <p>return type : typing.Callable[[list[str], list[str] | str], list[str]]</p> <p>supports pipeline flows : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#clean_format","title":"clean_format","text":"<p>The format used to convert the document to a clean string for downstream processing tasks</p> <p>name : <code>&lt;Document&gt;.x('clean_format') or &lt;Document&gt;.clean_format</code></p> <p>return type : typing.Any</p> <p>supports pipeline flows : application/epub+zip, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, mediawiki, pandoc, text/markdown, text/rtf</p>"},{"location":"document/#clean_spacy_text","title":"clean_spacy_text","text":"<p>Generate text to be used for spacy. Depending on the 'use_clean_text_for_spacy' option it will use page templates and replace complicated text structures such as tables for better text understanding.</p> <p>name : <code>&lt;Document&gt;.x('clean_spacy_text') or &lt;Document&gt;.clean_spacy_text</code></p> <p>return type : &lt;class 'str'&gt;</p> <p>supports pipeline flows : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#clean_text","title":"clean_text","text":"pipe_type description *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/pdf, application/x-yaml, image, image/jpeg, image/png, image/tiff, text/html Alias for: * full_text-&gt;clean_text (output) application/epub+zip, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, mediawiki, pandoc, text/markdown, text/rtf for some downstream tasks, it is better to have pure text, without any sructural elements in it <p>name : <code>&lt;Document&gt;.x('clean_text') or &lt;Document&gt;.clean_text</code></p> <p>return type : &lt;class 'str'&gt;</p> <p>supports pipeline flows : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#convert_to","title":"convert_to","text":"<p>Generic pandoc converter for other document formats. TODO: better docs</p> <p>name : <code>&lt;Document&gt;.x('convert_to') or &lt;Document&gt;.convert_to</code></p> <p>return type : typing.Callable</p> <p>supports pipeline flows : application/epub+zip, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, mediawiki, pandoc, text/markdown, text/rtf</p>"},{"location":"document/#coreferences","title":"coreferences","text":"<p>Resolve coreferences in the text</p> <p>name : <code>&lt;Document&gt;.x('coreferences') or &lt;Document&gt;.coreferences</code></p> <p>return type : list[list[tuple[int, int]]]</p> <p>supports pipeline flows : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#data","title":"data","text":"pipe_type description PIL.Image.Image Converts the image to a numpy array image, image/jpeg, image/png, image/tiff Converts the image to a numpy array for downstream processing tasks application/x-yaml Load yaml data from a string *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, mediawiki, pandoc, text/html, text/markdown, text/rtf The unprocessed data. <p>name : <code>&lt;Document&gt;.x('data') or &lt;Document&gt;.data</code></p> <p>return type : &lt;class 'numpy.ndarray'&gt; | typing.Any</p> <p>supports pipeline flows : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#data_sel","title":"data_sel","text":"<p>select values by key from source data in Document</p> <p>name : <code>&lt;Document&gt;.x('data_sel') or &lt;Document&gt;.data_sel</code></p> <p>return type : typing.Callable[..., dict]</p> <p>supports pipeline flows : &lt;class 'dict'&gt;, application/x-yaml</p>"},{"location":"document/#do","title":"do","text":"<p>Alias for: </p> <p>* document_objects-&gt;do (output)</p> <p>name : <code>&lt;Document&gt;.x('do') or &lt;Document&gt;.do</code></p> <p>return type : dict[int, pydoxtools.document_base.DocumentElement]</p> <p>supports pipeline flows : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#document_graph","title":"document_graph","text":"<p>Builds a networkx graph from the relations and coreferences</p> <p>name : <code>&lt;Document&gt;.x('document_graph') or &lt;Document&gt;.document_graph</code></p> <p>return type : &lt;class 'networkx.classes.digraph.DiGraph'&gt;</p> <p>supports pipeline flows : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#document_objects","title":"document_objects","text":"pipe_type description PIL.Image.Image, application/pdf, image, image/jpeg, image/png, image/tiff extracts a list of document objects such as tables, text boxes, figures, etc. *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, application/epub+zip, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, mediawiki, pandoc, text/html, text/markdown, text/rtf output a list of document elements which can be referenced by id <p>name : <code>&lt;Document&gt;.x('document_objects') or &lt;Document&gt;.document_objects</code></p> <p>return type : dict[int, pydoxtools.document_base.DocumentElement]</p> <p>supports pipeline flows : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#elements","title":"elements","text":"pipe_type description application/pdf Loads a pdf file and returns a list of basic document elements such as lines, figures, etc. PIL.Image.Image, image, image/jpeg, image/png, image/tiff Loads the pdf file into a list of  *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, application/x-yaml, text/html extracts a list of document objects such as tables, text boxes, figures, etc. application/epub+zip, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, mediawiki, pandoc, text/markdown, text/rtf split a pandoc document into text elements. <p>name : <code>&lt;Document&gt;.x('elements') or &lt;Document&gt;.elements</code></p> <p>return type : &lt;class 'pandas.core.frame.DataFrame'&gt; | list[pydoxtools.document_base.DocumentElement]</p> <p>supports pipeline flows : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#embedded_meta","title":"embedded_meta","text":"pipe_type description application/epub+zip, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, mediawiki, pandoc, text/markdown, text/rtf Alias for: * meta_pandoc-&gt;embedded_meta (output) PIL.Image.Image, application/pdf, image, image/jpeg, image/png, image/tiff Alias for: * meta_pdf-&gt;embedded_meta (output) *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, application/x-yaml, text/html represents the metadata embedded in the file <p>name : <code>&lt;Document&gt;.x('embedded_meta') or &lt;Document&gt;.embedded_meta</code></p> <p>return type : &lt;class 'dict'&gt; | typing.Any</p> <p>supports pipeline flows : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#embedding","title":"embedding","text":"<p>Get a vector (embedding) for the entire text by taking the mean of the contextual embeddings of all tokens</p> <p>name : <code>&lt;Document&gt;.x('embedding') or &lt;Document&gt;.embedding</code></p> <p>return type : list[float]</p> <p>supports pipeline flows : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#entities","title":"entities","text":"<p>Extract entities from text</p> <p>name : <code>&lt;Document&gt;.x('entities') or &lt;Document&gt;.entities</code></p> <p>return type : list[str]</p> <p>supports pipeline flows : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#file_meta","title":"file_meta","text":"<p>Some fast-to-calculate metadata information about a document</p> <p>name : <code>&lt;Document&gt;.x('file_meta') or &lt;Document&gt;.file_meta</code></p> <p>return type : dict[str, typing.Any]</p> <p>supports pipeline flows : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#final_urls","title":"final_urls","text":"<p>Extracts the main content from the html document, removing boilerplate and other noise</p> <p>name : <code>&lt;Document&gt;.x('final_urls') or &lt;Document&gt;.final_urls</code></p> <p>return type : typing.Any</p> <p>supports pipeline flows : text/html</p>"},{"location":"document/#full_text","title":"full_text","text":"pipe_type description text/html Alias for: * main_content-&gt;full_text (output) application/x-yaml Alias for: * raw_content-&gt;full_text (output) application/epub+zip, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, mediawiki, pandoc, text/markdown, text/rtf Converts the document to a string using pandoc &lt;class 'dict'&gt; Dump dict data to a yaml-like string &lt;class 'list'&gt; Dump list data to a yaml-like string PIL.Image.Image, application/pdf, image, image/jpeg, image/png, image/tiff Extracts the full text from the document by grouping text elements * Full text as a string value <p>name : <code>&lt;Document&gt;.x('full_text') or &lt;Document&gt;.full_text</code></p> <p>return type : &lt;class 'str'&gt; | typing.Any</p> <p>supports pipeline flows : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#goose_article","title":"goose_article","text":"<p>Extracts the main content from the html document, removing boilerplate and other noise</p> <p>name : <code>&lt;Document&gt;.x('goose_article') or &lt;Document&gt;.goose_article</code></p> <p>return type : typing.Any</p> <p>supports pipeline flows : text/html</p>"},{"location":"document/#graphic_elements","title":"graphic_elements","text":"<p>Filters the document elements and only keeps the graphic elements</p> <p>name : <code>&lt;Document&gt;.x('graphic_elements') or &lt;Document&gt;.graphic_elements</code></p> <p>return type : list[pydoxtools.document_base.DocumentElement]</p> <p>supports pipeline flows : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#headers","title":"headers","text":"<p>Extracts the headers from the document</p> <p>name : <code>&lt;Document&gt;.x('headers') or &lt;Document&gt;.headers</code></p> <p>return type : list[pydoxtools.document_base.DocumentElement]</p> <p>supports pipeline flows : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#html_keywords","title":"html_keywords","text":"<p>Extracts explicitly given keywords from the html document</p> <p>name : <code>&lt;Document&gt;.x('html_keywords') or &lt;Document&gt;.html_keywords</code></p> <p>return type : set[str]</p> <p>supports pipeline flows : text/html</p>"},{"location":"document/#html_keywords_str","title":"html_keywords_str","text":"<p>Extracts the main content from the html document, removing boilerplate and other noise</p> <p>name : <code>&lt;Document&gt;.x('html_keywords_str') or &lt;Document&gt;.html_keywords_str</code></p> <p>return type : typing.Any</p> <p>supports pipeline flows : text/html</p>"},{"location":"document/#image_elements","title":"image_elements","text":"<p>Filters the document elements and only keeps the image elements</p> <p>name : <code>&lt;Document&gt;.x('image_elements') or &lt;Document&gt;.image_elements</code></p> <p>return type : list[pydoxtools.document_base.DocumentElement]</p> <p>supports pipeline flows : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#images","title":"images","text":"pipe_type description PIL.Image.Image, image, image/jpeg, image/png, image/tiff Access images as a dictionary with page numbers as keys for downstream processing tasks application/pdf Render a pdf into images which can be used for further downstream processing <p>name : <code>&lt;Document&gt;.x('images') or &lt;Document&gt;.images</code></p> <p>return type : dict[int, PIL.Image.Image]</p> <p>supports pipeline flows : PIL.Image.Image, application/pdf, image, image/jpeg, image/png, image/tiff</p>"},{"location":"document/#items","title":"items","text":"<p>Get the items of the dictionary</p> <p>name : <code>&lt;Document&gt;.x('items') or &lt;Document&gt;.items</code></p> <p>return type : typing.Any</p> <p>supports pipeline flows : &lt;class 'dict'&gt;, application/x-yaml</p>"},{"location":"document/#keys","title":"keys","text":"<p>Get the keys of the dictionary</p> <p>name : <code>&lt;Document&gt;.x('keys') or &lt;Document&gt;.keys</code></p> <p>return type : typing.Any</p> <p>supports pipeline flows : &lt;class 'dict'&gt;, application/x-yaml</p>"},{"location":"document/#keywords","title":"keywords","text":"pipe_type description text/html Aggregates the keywords from the html document and found by other algorithms *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/markdown, text/rtf Alias for: * textrank_keywords-&gt;keywords (output) <p>name : <code>&lt;Document&gt;.x('keywords') or &lt;Document&gt;.keywords</code></p> <p>return type : set[str]</p> <p>supports pipeline flows : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#labeled_text_boxes","title":"labeled_text_boxes","text":"<p>Classifies the text elements into addresses, emails, phone numbers, etc. if possible.</p> <p>name : <code>&lt;Document&gt;.x('labeled_text_boxes') or &lt;Document&gt;.labeled_text_boxes</code></p> <p>return type : list[pydoxtools.document_base.DocumentElement]</p> <p>supports pipeline flows : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#language","title":"language","text":"pipe_type description *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, application/epub+zip, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, mediawiki, pandoc, text/markdown, text/rtf Detect language of a document, return 'unknown' in case of an error PIL.Image.Image, application/pdf, image, image/jpeg, image/png, image/tiff Extracts the language of the document text/html Extracts the main content from the html document, removing boilerplate and other noise <p>name : <code>&lt;Document&gt;.x('language') or &lt;Document&gt;.language</code></p> <p>return type : &lt;class 'str'&gt; | typing.Any</p> <p>supports pipeline flows : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#line_elements","title":"line_elements","text":"<p>Filters the document elements and only keeps the text elements</p> <p>name : <code>&lt;Document&gt;.x('line_elements') or &lt;Document&gt;.line_elements</code></p> <p>return type : list[pydoxtools.document_base.DocumentElement]</p> <p>supports pipeline flows : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#lists","title":"lists","text":"pipe_type description PIL.Image.Image, application/pdf, image, image/jpeg, image/png, image/tiff Extracts lists from the document text elements *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, application/epub+zip, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, mediawiki, pandoc, text/html, text/markdown, text/rtf Extracts the lists from the document <p>name : <code>&lt;Document&gt;.x('lists') or &lt;Document&gt;.lists</code></p> <p>return type : &lt;class 'pandas.core.frame.DataFrame'&gt; | list[pydoxtools.document_base.DocumentElement]</p> <p>supports pipeline flows : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#main_content","title":"main_content","text":"<p>Extracts the main content from the html document, removing boilerplate and other noise</p> <p>name : <code>&lt;Document&gt;.x('main_content') or &lt;Document&gt;.main_content</code></p> <p>return type : typing.Any</p> <p>supports pipeline flows : text/html</p>"},{"location":"document/#main_content_clean_html","title":"main_content_clean_html","text":"<p>Extracts the main content from the html document, removing boilerplate and other noise</p> <p>name : <code>&lt;Document&gt;.x('main_content_clean_html') or &lt;Document&gt;.main_content_clean_html</code></p> <p>return type : typing.Any</p> <p>supports pipeline flows : text/html</p>"},{"location":"document/#main_image","title":"main_image","text":"<p>Extracts the main image from the html document</p> <p>name : <code>&lt;Document&gt;.x('main_image') or &lt;Document&gt;.main_image</code></p> <p>return type : typing.Any</p> <p>supports pipeline flows : text/html</p>"},{"location":"document/#meta","title":"meta","text":"<p>Metadata of the document</p> <p>name : <code>&lt;Document&gt;.x('meta') or &lt;Document&gt;.meta</code></p> <p>return type : dict[str, typing.Any]</p> <p>supports pipeline flows : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#meta_pandoc","title":"meta_pandoc","text":"<p>meta information from pandoc document</p> <p>name : <code>&lt;Document&gt;.x('meta_pandoc') or &lt;Document&gt;.meta_pandoc</code></p> <p>return type : typing.Any</p> <p>supports pipeline flows : application/epub+zip, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, mediawiki, pandoc, text/markdown, text/rtf</p>"},{"location":"document/#meta_pdf","title":"meta_pdf","text":"pipe_type description application/pdf Loads a pdf file and returns a list of basic document elements such as lines, figures, etc. PIL.Image.Image, image, image/jpeg, image/png, image/tiff Loads the pdf file into a list of  <p>name : <code>&lt;Document&gt;.x('meta_pdf') or &lt;Document&gt;.meta_pdf</code></p> <p>return type : &lt;class 'dict'&gt;</p> <p>supports pipeline flows : PIL.Image.Image, application/pdf, image, image/jpeg, image/png, image/tiff</p>"},{"location":"document/#noun_chunks","title":"noun_chunks","text":"<p>Alias for: </p> <p>* spacy_noun_chunks-&gt;noun_chunks (output)</p> <p>name : <code>&lt;Document&gt;.x('noun_chunks') or &lt;Document&gt;.noun_chunks</code></p> <p>return type : typing.List[pydoxtools.document_base.TokenCollection]</p> <p>supports pipeline flows : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#noun_graph","title":"noun_graph","text":"<p>Create a graph of similar nouns</p> <p>name : <code>&lt;Document&gt;.x('noun_graph') or &lt;Document&gt;.noun_graph</code></p> <p>return type : typing.Any</p> <p>supports pipeline flows : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#noun_ids","title":"noun_ids","text":"<p>Vectors for nouns and corresponding noun ids in order to find them in the spacy document</p> <p>name : <code>&lt;Document&gt;.x('noun_ids') or &lt;Document&gt;.noun_ids</code></p> <p>return type : typing.Any</p> <p>supports pipeline flows : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#noun_index","title":"noun_index","text":"<p>Create an index for the nouns</p> <p>name : <code>&lt;Document&gt;.x('noun_index') or &lt;Document&gt;.noun_index</code></p> <p>return type : typing.Any</p> <p>supports pipeline flows : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#noun_query","title":"noun_query","text":"<p>Create a query function for the nouns which can be used to do nearest-neighbor queries</p> <p>name : <code>&lt;Document&gt;.x('noun_query') or &lt;Document&gt;.noun_query</code></p> <p>return type : typing.Callable[..., list[tuple]]</p> <p>supports pipeline flows : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#noun_vecs","title":"noun_vecs","text":"<p>Vectors for nouns and corresponding noun ids in order to find them in the spacy document</p> <p>name : <code>&lt;Document&gt;.x('noun_vecs') or &lt;Document&gt;.noun_vecs</code></p> <p>return type : typing.Any</p> <p>supports pipeline flows : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#num_pages","title":"num_pages","text":"pipe_type description *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, application/epub+zip, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, mediawiki, pandoc, text/html, text/markdown, text/rtf Number of pages in the document PIL.Image.Image, application/pdf, image, image/jpeg, image/png, image/tiff Outputs the number of pages in the document <p>name : <code>&lt;Document&gt;.x('num_pages') or &lt;Document&gt;.num_pages</code></p> <p>return type : &lt;class 'int'&gt;</p> <p>supports pipeline flows : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#num_sents","title":"num_sents","text":"<p>number of sentences</p> <p>name : <code>&lt;Document&gt;.x('num_sents') or &lt;Document&gt;.num_sents</code></p> <p>return type : &lt;class 'int'&gt;</p> <p>supports pipeline flows : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#num_words","title":"num_words","text":"<p>Number of words in the document</p> <p>name : <code>&lt;Document&gt;.x('num_words') or &lt;Document&gt;.num_words</code></p> <p>return type : &lt;class 'int'&gt;</p> <p>supports pipeline flows : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#ocr_pdf_file","title":"ocr_pdf_file","text":"<p>Extracts the text from the document using OCR. It does this by creating a pdf which is important in order to keep the positional information of the text elements.</p> <p>name : <code>&lt;Document&gt;.x('ocr_pdf_file') or &lt;Document&gt;.ocr_pdf_file</code></p> <p>return type : typing.Any</p> <p>supports pipeline flows : PIL.Image.Image, image, image/jpeg, image/png, image/tiff</p>"},{"location":"document/#page_classifier","title":"page_classifier","text":"<p>Classifies the pages into different types. This is useful for example for identifiying table of contents, certain chapters etc... . This works as a zero-shot classifier and the classes are not predefined. it can by called like this: </p> <p>Document('somefile.pdf').page_classifier(candidate_labels=['table_of_contents', 'credits', 'license'])</p> <p>name : <code>&lt;Document&gt;.x('page_classifier') or &lt;Document&gt;.page_classifier</code></p> <p>return type : typing.Callable[[list[str]], dict]</p> <p>supports pipeline flows : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#page_set","title":"page_set","text":"pipe_type description *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, application/epub+zip, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, mediawiki, pandoc, text/html, text/markdown, text/rtf A constant value application/pdf Loads a pdf file and returns a list of basic document elements such as lines, figures, etc. PIL.Image.Image, image, image/jpeg, image/png, image/tiff Loads the pdf file into a list of  <p>name : <code>&lt;Document&gt;.x('page_set') or &lt;Document&gt;.page_set</code></p> <p>return type : set[int] | typing.Any</p> <p>supports pipeline flows : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#page_templates","title":"page_templates","text":"<p>Generates a text page while replacing certain elements of the page which can be specified as a list of ElementTypes. It also automatically replaces elements which don't have a textual representation with an identifier. This is often the case with images &amp; figures for example. The Id of the placeholder refers to the index of the DocumentObject. So for example, if we encounter and Identifier:  {Table_22}, we would be able to find it using doc.document_objects[22] or doc.do[22].</p> <p>name : <code>&lt;Document&gt;.x('page_templates') or &lt;Document&gt;.page_templates</code></p> <p>return type : typing.Callable[[list[str]], dict[int, str]]</p> <p>supports pipeline flows : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#page_templates_str","title":"page_templates_str","text":"<p>Outputs a nice text version of the documents with annotated document objects such as page numbers, tables, figures, etc.</p> <p>name : <code>&lt;Document&gt;.x('page_templates_str') or &lt;Document&gt;.page_templates_str</code></p> <p>return type : &lt;class 'str'&gt;</p> <p>supports pipeline flows : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#page_templates_str_minimal","title":"page_templates_str_minimal","text":"<p>No documentation</p> <p>name : <code>&lt;Document&gt;.x('page_templates_str_minimal') or &lt;Document&gt;.page_templates_str_minimal</code></p> <p>return type : &lt;class 'str'&gt;</p> <p>supports pipeline flows : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#pages_bbox","title":"pages_bbox","text":"pipe_type description application/pdf Loads a pdf file and returns a list of basic document elements such as lines, figures, etc. PIL.Image.Image, image, image/jpeg, image/png, image/tiff Loads the pdf file into a list of  <p>name : <code>&lt;Document&gt;.x('pages_bbox') or &lt;Document&gt;.pages_bbox</code></p> <p>return type : &lt;class 'numpy.ndarray'&gt;</p> <p>supports pipeline flows : PIL.Image.Image, application/pdf, image, image/jpeg, image/png, image/tiff</p>"},{"location":"document/#pandoc_document","title":"pandoc_document","text":"<p>Loads the document using the pandoc project https://pandoc.org/ into a pydoxtools list of </p> <p>name : <code>&lt;Document&gt;.x('pandoc_document') or &lt;Document&gt;.pandoc_document</code></p> <p>return type : Pandoc(Meta, [Block])</p> <p>supports pipeline flows : application/epub+zip, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, mediawiki, pandoc, text/markdown, text/rtf</p>"},{"location":"document/#pdf_links","title":"pdf_links","text":"<p>Extracts the main content from the html document, removing boilerplate and other noise</p> <p>name : <code>&lt;Document&gt;.x('pdf_links') or &lt;Document&gt;.pdf_links</code></p> <p>return type : typing.Any</p> <p>supports pipeline flows : text/html</p>"},{"location":"document/#pil_image","title":"pil_image","text":"pipe_type description PIL.Image.Image Alias for: * _fobj-&gt;pil_image (output) image, image/jpeg, image/png, image/tiff Converts the image to a PIL-style image for downstream processing tasks <p>name : <code>&lt;Document&gt;.x('pil_image') or &lt;Document&gt;.pil_image</code></p> <p>return type : &lt;class 'PIL.Image.Image'&gt; | typing.Any</p> <p>supports pipeline flows : PIL.Image.Image, image, image/jpeg, image/png, image/tiff</p>"},{"location":"document/#schemadata","title":"schemadata","text":"<p>Extracts the main content from the html document, removing boilerplate and other noise</p> <p>name : <code>&lt;Document&gt;.x('schemadata') or &lt;Document&gt;.schemadata</code></p> <p>return type : typing.Any</p> <p>supports pipeline flows : text/html</p>"},{"location":"document/#sections","title":"sections","text":"<p>Extracts the sections from the document by grouping text elements</p> <p>name : <code>&lt;Document&gt;.x('sections') or &lt;Document&gt;.sections</code></p> <p>return type : typing.Any</p> <p>supports pipeline flows : application/epub+zip, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, mediawiki, pandoc, text/markdown, text/rtf</p>"},{"location":"document/#segment_query","title":"segment_query","text":"<p>Create a query function for the text segments which can be used to do nearest-neighbor queries</p> <p>name : <code>&lt;Document&gt;.x('segment_query') or &lt;Document&gt;.segment_query</code></p> <p>return type : typing.Callable[..., list[tuple]]</p> <p>supports pipeline flows : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#semantic_relations","title":"semantic_relations","text":"<p>Extract relations from text for building a knowledge graph</p> <p>name : <code>&lt;Document&gt;.x('semantic_relations') or &lt;Document&gt;.semantic_relations</code></p> <p>return type : &lt;class 'pandas.core.frame.DataFrame'&gt;</p> <p>supports pipeline flows : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#sent_graph","title":"sent_graph","text":"<p>Create a graph of similar sentences</p> <p>name : <code>&lt;Document&gt;.x('sent_graph') or &lt;Document&gt;.sent_graph</code></p> <p>return type : typing.Any</p> <p>supports pipeline flows : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#sent_ids","title":"sent_ids","text":"<p>Vectors for sentences &amp; sentence_ids</p> <p>name : <code>&lt;Document&gt;.x('sent_ids') or &lt;Document&gt;.sent_ids</code></p> <p>return type : typing.Any</p> <p>supports pipeline flows : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#sent_index","title":"sent_index","text":"<p>Create an index for the sentences</p> <p>name : <code>&lt;Document&gt;.x('sent_index') or &lt;Document&gt;.sent_index</code></p> <p>return type : typing.Any</p> <p>supports pipeline flows : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#sent_query","title":"sent_query","text":"<p>Create a query function for the sentences which can be used to do nearest-neighbor queries</p> <p>name : <code>&lt;Document&gt;.x('sent_query') or &lt;Document&gt;.sent_query</code></p> <p>return type : typing.Callable[..., list[tuple]]</p> <p>supports pipeline flows : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#sent_vecs","title":"sent_vecs","text":"<p>Vectors for sentences &amp; sentence_ids</p> <p>name : <code>&lt;Document&gt;.x('sent_vecs') or &lt;Document&gt;.sent_vecs</code></p> <p>return type : typing.Any</p> <p>supports pipeline flows : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#sents","title":"sents","text":"<p>Alias for: </p> <p>* spacy_sents-&gt;sents (output)</p> <p>name : <code>&lt;Document&gt;.x('sents') or &lt;Document&gt;.sents</code></p> <p>return type : list[str]</p> <p>supports pipeline flows : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#short_title","title":"short_title","text":"<p>Extracts the main content from the html document, removing boilerplate and other noise</p> <p>name : <code>&lt;Document&gt;.x('short_title') or &lt;Document&gt;.short_title</code></p> <p>return type : typing.Any</p> <p>supports pipeline flows : text/html</p>"},{"location":"document/#side_titles","title":"side_titles","text":"<p>Extracts the titles from the document by detecting unusual font styles</p> <p>name : <code>&lt;Document&gt;.x('side_titles') or &lt;Document&gt;.side_titles</code></p> <p>return type : typing.Any</p> <p>supports pipeline flows : PIL.Image.Image, application/pdf, image, image/jpeg, image/png, image/tiff</p>"},{"location":"document/#slow_summary","title":"slow_summary","text":"<p>Summarize the text using the Huggingface summarization pipeline</p> <p>name : <code>&lt;Document&gt;.x('slow_summary') or &lt;Document&gt;.slow_summary</code></p> <p>return type : typing.Any</p> <p>supports pipeline flows : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#spacy_doc","title":"spacy_doc","text":"<p>Spacy Document and Language Model for this document</p> <p>name : <code>&lt;Document&gt;.x('spacy_doc') or &lt;Document&gt;.spacy_doc</code></p> <p>return type : &lt;class 'spacy.tokens.doc.Doc'&gt;</p> <p>supports pipeline flows : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#spacy_embeddings","title":"spacy_embeddings","text":"<p>Embeddings calculated by a spacy transformer</p> <p>name : <code>&lt;Document&gt;.x('spacy_embeddings') or &lt;Document&gt;.spacy_embeddings</code></p> <p>return type : typing.Any</p> <p>supports pipeline flows : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#spacy_nlp","title":"spacy_nlp","text":"<p>Spacy Document and Language Model for this document</p> <p>name : <code>&lt;Document&gt;.x('spacy_nlp') or &lt;Document&gt;.spacy_nlp</code></p> <p>return type : &lt;class 'spacy.language.Language'&gt;</p> <p>supports pipeline flows : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#spacy_noun_chunks","title":"spacy_noun_chunks","text":"<p>exracts nounchunks from spacy. Will not be cached because it is allin the spacy doc already</p> <p>name : <code>&lt;Document&gt;.x('spacy_noun_chunks') or &lt;Document&gt;.spacy_noun_chunks</code></p> <p>return type : typing.List[pydoxtools.document_base.TokenCollection]</p> <p>supports pipeline flows : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#spacy_sents","title":"spacy_sents","text":"<p>List of sentences by spacy nlp framework</p> <p>name : <code>&lt;Document&gt;.x('spacy_sents') or &lt;Document&gt;.spacy_sents</code></p> <p>return type : list[str]</p> <p>supports pipeline flows : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#spacy_vectorizer","title":"spacy_vectorizer","text":"<p>Create a vectorizer function from spacy library.</p> <p>name : <code>&lt;Document&gt;.x('spacy_vectorizer') or &lt;Document&gt;.spacy_vectorizer</code></p> <p>return type : typing.Any</p> <p>supports pipeline flows : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#spacy_vectors","title":"spacy_vectors","text":"<p>Vectors for all tokens calculated by spacy</p> <p>name : <code>&lt;Document&gt;.x('spacy_vectors') or &lt;Document&gt;.spacy_vectors</code></p> <p>return type : torch.Tensor | typing.Any</p> <p>supports pipeline flows : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#summary","title":"summary","text":"<p>Extracts the main content from the html document, removing boilerplate and other noise</p> <p>name : <code>&lt;Document&gt;.x('summary') or &lt;Document&gt;.summary</code></p> <p>return type : typing.Any</p> <p>supports pipeline flows : text/html</p>"},{"location":"document/#table_areas","title":"table_areas","text":"<p>Areas of all detected tables</p> <p>name : <code>&lt;Document&gt;.x('table_areas') or &lt;Document&gt;.table_areas</code></p> <p>return type : list[numpy.ndarray]</p> <p>supports pipeline flows : PIL.Image.Image, application/pdf, image, image/jpeg, image/png, image/tiff</p>"},{"location":"document/#table_box_levels","title":"table_box_levels","text":"pipe_type description application/pdf Detects table candidates from the document elements PIL.Image.Image, image, image/jpeg, image/png, image/tiff Extracts the table candidates from the document. As this is an image, we need to use a different method than for pdfs. Right now this relies on neural networks. TODO: add adtitional pure text-based method. <p>name : <code>&lt;Document&gt;.x('table_box_levels') or &lt;Document&gt;.table_box_levels</code></p> <p>return type : typing.Any</p> <p>supports pipeline flows : PIL.Image.Image, application/pdf, image, image/jpeg, image/png, image/tiff</p>"},{"location":"document/#table_candidates","title":"table_candidates","text":"pipe_type description application/pdf Detects table candidates from the document elements PIL.Image.Image, image, image/jpeg, image/png, image/tiff Extracts the table candidates from the document. As this is an image, we need to use a different method than for pdfs. Right now this relies on neural networks. TODO: add adtitional pure text-based method. <p>name : <code>&lt;Document&gt;.x('table_candidates') or &lt;Document&gt;.table_candidates</code></p> <p>return type : typing.Any</p> <p>supports pipeline flows : PIL.Image.Image, application/pdf, image, image/jpeg, image/png, image/tiff</p>"},{"location":"document/#table_context","title":"table_context","text":"<p>Outputs a dictionary with the context of each table in the document</p> <p>name : <code>&lt;Document&gt;.x('table_context') or &lt;Document&gt;.table_context</code></p> <p>return type : dict[int, str]</p> <p>supports pipeline flows : PIL.Image.Image, application/pdf, image, image/jpeg, image/png, image/tiff</p>"},{"location":"document/#table_df0","title":"table_df0","text":"<p>Filter valid tables from table candidates by looking if meaningful values can be extracted</p> <p>name : <code>&lt;Document&gt;.x('table_df0') or &lt;Document&gt;.table_df0</code></p> <p>return type : list[pandas.core.frame.DataFrame]</p> <p>supports pipeline flows : PIL.Image.Image, application/pdf, image, image/jpeg, image/png, image/tiff</p>"},{"location":"document/#tables","title":"tables","text":"<p>Extracts the tables from the document as a document element</p> <p>name : <code>&lt;Document&gt;.x('tables') or &lt;Document&gt;.tables</code></p> <p>return type : dict[int, pydoxtools.document_base.DocumentElement]</p> <p>supports pipeline flows : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#tables_df","title":"tables_df","text":"pipe_type description PIL.Image.Image, application/pdf, image, image/jpeg, image/png, image/tiff Dataframes of all tables text/html Extracts the main content from the html document, removing boilerplate and other noise *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, application/epub+zip, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, mediawiki, pandoc, text/markdown, text/rtf No documentation <p>name : <code>&lt;Document&gt;.x('tables_df') or &lt;Document&gt;.tables_df</code></p> <p>return type : list[pandas.core.frame.DataFrame] | typing.Any</p> <p>supports pipeline flows : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#tables_dict","title":"tables_dict","text":"<p>List of Table</p> <p>name : <code>&lt;Document&gt;.x('tables_dict') or &lt;Document&gt;.tables_dict</code></p> <p>return type : list[dict[int, dict[int, str]]]</p> <p>supports pipeline flows : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#text_box_elements","title":"text_box_elements","text":"pipe_type description &lt;class 'dict'&gt;, application/x-yaml Create a dataframe from a dictionary. TODO: this is not working correctly, it should create a list of  PIL.Image.Image, application/pdf, image, image/jpeg, image/png, image/tiff Extracts a dataframe of text boxes from the document by grouping text elements text/html Extracts the text boxes from the html document &lt;class 'list'&gt; No documentation *, application/epub+zip, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, mediawiki, pandoc, text/markdown, text/rtf Text boxes extracted as a pandas Dataframe with some additional metadata <p>name : <code>&lt;Document&gt;.x('text_box_elements') or &lt;Document&gt;.text_box_elements</code></p> <p>return type : list[pydoxtools.document_base.DocumentElement] | list[str]</p> <p>supports pipeline flows : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#text_segment_ids","title":"text_segment_ids","text":"<p>Get the a list of ids for individual text segments</p> <p>name : <code>&lt;Document&gt;.x('text_segment_ids') or &lt;Document&gt;.text_segment_ids</code></p> <p>return type : typing.Any</p> <p>supports pipeline flows : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#text_segment_index","title":"text_segment_index","text":"<p>Create an index for the text segments</p> <p>name : <code>&lt;Document&gt;.x('text_segment_index') or &lt;Document&gt;.text_segment_index</code></p> <p>return type : typing.Any</p> <p>supports pipeline flows : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#text_segment_vec_res","title":"text_segment_vec_res","text":"<p>Calculate the embeddings for each text segment</p> <p>name : <code>&lt;Document&gt;.x('text_segment_vec_res') or &lt;Document&gt;.text_segment_vec_res</code></p> <p>return type : typing.Any</p> <p>supports pipeline flows : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#text_segment_vecs","title":"text_segment_vecs","text":"<p>Get the embeddings for individual text segments</p> <p>name : <code>&lt;Document&gt;.x('text_segment_vecs') or &lt;Document&gt;.text_segment_vecs</code></p> <p>return type : typing.Any</p> <p>supports pipeline flows : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#text_segments","title":"text_segments","text":"<p>Split the text into segments</p> <p>name : <code>&lt;Document&gt;.x('text_segments') or &lt;Document&gt;.text_segments</code></p> <p>return type : list[str]</p> <p>supports pipeline flows : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#textrank_keywords","title":"textrank_keywords","text":"<p>Extract keywords from the graph of similar nouns</p> <p>name : <code>&lt;Document&gt;.x('textrank_keywords') or &lt;Document&gt;.textrank_keywords</code></p> <p>return type : set[str]</p> <p>supports pipeline flows : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#textrank_sents","title":"textrank_sents","text":"<p>Extract the most important sentences from the graph of similar sentences</p> <p>name : <code>&lt;Document&gt;.x('textrank_sents') or &lt;Document&gt;.textrank_sents</code></p> <p>return type : set[str]</p> <p>supports pipeline flows : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#title","title":"title","text":"<p>Extracts the main content from the html document, removing boilerplate and other noise</p> <p>name : <code>&lt;Document&gt;.x('title') or &lt;Document&gt;.title</code></p> <p>return type : typing.Any</p> <p>supports pipeline flows : text/html</p>"},{"location":"document/#titles","title":"titles","text":"pipe_type description PIL.Image.Image, application/pdf, image, image/jpeg, image/png, image/tiff Extracts the titles from the document by detecting unusual font styles text/html Extracts the titles from the html document <p>name : <code>&lt;Document&gt;.x('titles') or &lt;Document&gt;.titles</code></p> <p>return type : tuple[str, str] | typing.Any</p> <p>supports pipeline flows : PIL.Image.Image, application/pdf, image, image/jpeg, image/png, image/tiff, text/html</p>"},{"location":"document/#tok_embeddings","title":"tok_embeddings","text":"<p>Get the tokenized text</p> <p>name : <code>&lt;Document&gt;.x('tok_embeddings') or &lt;Document&gt;.tok_embeddings</code></p> <p>return type : typing.Any</p> <p>supports pipeline flows : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#tokens","title":"tokens","text":"<p>Get the tokenized text</p> <p>name : <code>&lt;Document&gt;.x('tokens') or &lt;Document&gt;.tokens</code></p> <p>return type : typing.Any</p> <p>supports pipeline flows : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#url","title":"url","text":"pipe_type description text/html Extracts the main content from the html document, removing boilerplate and other noise *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/markdown, text/rtf Url of this document <p>name : <code>&lt;Document&gt;.x('url') or &lt;Document&gt;.url</code></p> <p>return type : typing.Any</p> <p>supports pipeline flows : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#urls","title":"urls","text":"<p>Extracts the urls from the html document</p> <p>name : <code>&lt;Document&gt;.x('urls') or &lt;Document&gt;.urls</code></p> <p>return type : typing.Any</p> <p>supports pipeline flows : text/html</p>"},{"location":"document/#valid_tables","title":"valid_tables","text":"<p>Filter valid tables from table candidates by looking if meaningful values can be extracted</p> <p>name : <code>&lt;Document&gt;.x('valid_tables') or &lt;Document&gt;.valid_tables</code></p> <p>return type : typing.Any</p> <p>supports pipeline flows : PIL.Image.Image, application/pdf, image, image/jpeg, image/png, image/tiff</p>"},{"location":"document/#values","title":"values","text":"<p>Get the values of the dictionary</p> <p>name : <code>&lt;Document&gt;.x('values') or &lt;Document&gt;.values</code></p> <p>return type : typing.Any</p> <p>supports pipeline flows : &lt;class 'dict'&gt;, application/x-yaml</p>"},{"location":"document/#vec_res","title":"vec_res","text":"<p>Calculate context-based vectors (embeddings) for the entire text</p> <p>name : <code>&lt;Document&gt;.x('vec_res') or &lt;Document&gt;.vec_res</code></p> <p>return type : typing.Any</p> <p>supports pipeline flows : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#vector","title":"vector","text":"<p>Embeddings from spacy</p> <p>name : <code>&lt;Document&gt;.x('vector') or &lt;Document&gt;.vector</code></p> <p>return type : typing.Any</p> <p>supports pipeline flows : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#vectorizer","title":"vectorizer","text":"<p>Get the vectorizer function used for this document for an arbitrary text</p> <p>name : <code>&lt;Document&gt;.x('vectorizer') or &lt;Document&gt;.vectorizer</code></p> <p>return type : typing.Any</p> <p>supports pipeline flows : *, &lt;class 'dict'&gt;, &lt;class 'list'&gt;, PIL.Image.Image, application/epub+zip, application/pdf, application/vnd.oasis.opendocument.text, application/vnd.openxmlformats-officedocument.wordprocessingml.document, application/x-yaml, image, image/jpeg, image/png, image/tiff, mediawiki, pandoc, text/html, text/markdown, text/rtf</p>"},{"location":"document/#configuration-parameters","title":"Configuration parameters","text":"name description default_values chat_model_id In order to use openai-chatgpt, you can use 'gpt-3.5-turbo' or 'gpt-4'.Additionally, we support models used by gpt4all library whichcan be run locally and most are available for commercial purposes. Currently available models are: ['wizardlm-13b-v1.1-superhot-8k.ggmlv3.q4_0', 'ggml-model-gpt4all-falcon-q4_0', 'ous-hermes-13b.ggmlv3.q4_0', 'GPT4All-13B-snoozy.ggmlv3.q4_0', 'orca-mini-7b.ggmlv3.q4_0', 'orca-mini-3b.ggmlv3.q4_0', 'orca-mini-13b.ggmlv3.q4_0', 'wizardLM-13B-Uncensored.ggmlv3.q4_0', 'ggml-replit-code-v1-3', 'ggml-all-MiniLM-L6-v2-f16', 'starcoderbase-3b-ggml', 'starcoderbase-7b-ggml', 'llama-2-7b-chat.ggmlv3.q4_0'] gpt-3.5-turbo coreference_method can be 'fast' or 'accurate' fast full_text_format The format used to convert the document to a string markdown graph_debug_context_size can be 'fast' or 'accurate' 0 image_dpi The dpi when rendering the document. The standard image generation resolution is set to 216 dpi for pdfs as we want to have sufficient DPI for downstram OCR tasks (e.g. table extraction) 216 max_size_text_segment controls the text segmentation for knowledge basesoverlap is only relevant for large text segmenets that need tobe split up into smaller pieces. 512 max_text_segment_num controls the text segmentation for knowledge basesoverlap is only relevant for large text segmenets that need tobe split up into smaller pieces. 100 min_size_text_segment controls the text segmentation for knowledge basesoverlap is only relevant for large text segmenets that need tobe split up into smaller pieces. 256 ocr_lang Configuration for the ocr extractor. We can turn it on/off and specify the language used for OCR. auto ocr_on Configuration for the ocr extractor. We can turn it on/off and specify the language used for OCR. True qam_model_id Configuration for values: deepset/minilm-uncased-squad2 * qam_model_id = deepset/minilm-uncased-squad2 (default) spacy_model we can also explicitly specify the spacy model we want to use. auto spacy_model_size the model size which is used for spacy text analysis. Can be:  sm,md,lg,trf. md summarizer_max_text_len Configuration for values: 200 * summarizer_model = sshleifer/distilbart-cnn-12-6 (default) * summarizer_token_overlap = 50 (default) * summarizer_max_text_len = 200 (default) summarizer_model Configuration for values: sshleifer/distilbart-cnn-12-6 * summarizer_model = sshleifer/distilbart-cnn-12-6 (default) * summarizer_token_overlap = 50 (default) * summarizer_max_text_len = 200 (default) summarizer_token_overlap Configuration for values: 50 * summarizer_model = sshleifer/distilbart-cnn-12-6 (default) * summarizer_token_overlap = 50 (default) * summarizer_max_text_len = 200 (default) text_segment_overlap controls the text segmentation for knowledge basesoverlap is only relevant for large text segmenets that need tobe split up into smaller pieces. 0.3 top_k_text_rank_keywords Configuration for values: 5 * top_k_text_rank_keywords = 5 (default) top_k_text_rank_sentences controls the number of most important sentences that are extracted from the text. 5 use_clean_text_for_spacy Whether pydoxtools cleans up the text before using spacy on it. True vectorizer_model Choose the embeddings model (huggingface-style) and if we wantto do the vectorization using only the tokenizer. Using only thetokenizer is MUCH faster and uses lower CPU than creating actualcontextual embeddings using the model. BUt is also lower qualitybecause it lacks the context. sentence-transformers/all-MiniLM-L6-v2 vectorizer_only_tokenizer Choose the embeddings model (huggingface-style) and if we wantto do the vectorization using only the tokenizer. Using only thetokenizer is MUCH faster and uses lower CPU than creating actualcontextual embeddings using the model. BUt is also lower qualitybecause it lacks the context. False vectorizer_overlap_ratio Choose the embeddings model (huggingface-style) and if we wantto do the vectorization using only the tokenizer. Using only thetokenizer is MUCH faster and uses lower CPU than creating actualcontextual embeddings using the model. BUt is also lower qualitybecause it lacks the context. 0.1"},{"location":"documentbag/","title":"pydoxtools.DocumentBag","text":"<p>             Bases: <code>Pipeline</code></p> <p>This class is a work-in-progress (WIP), use with caution.</p> <p>The DocumentBag class loads and processes a set of documents using a pipeline. It leverages Dask bags for efficient memory usage and large-scale computations on documents.</p> Notes <ul> <li>Dask bags documentation can be found here.</li> <li>Dask dataframes can be used for downstream calculations.</li> <li>This class helps scale LLM &amp; AI inference to larger workloads.</li> <li>It uses iterative Dask bags &amp; dataframes to avoid out-of-memory issues.</li> </ul> Rationale <p>This function is needed to create and process new document bags, instead of using Dask bags directly with arbitrary data. It reduces boilerplate code for creating new documents and traceable datasources.</p>"},{"location":"documentbag/#pydoxtools.DocumentBag.__repr__","title":"<code>__repr__()</code>","text":"<p>Returns:</p> Name Type Description <code>str</code> <p>A string representation of the instance.</p>"},{"location":"documentbag/#text-extraction-attributes-and-functions","title":"Text extraction attributes and functions","text":"<p>The pydoxtools.DocumentBag is built on the pydoxtools.Pipeline class and most of the text extraction functionality makes extensive use of the pipeline features. All attributes and functions that are created by the pipeline are documented here.</p> <p>Pipeline visualizations for the structure of the Document pipelines for different document types can be found here.</p>"},{"location":"documentbag/#document","title":"Document","text":"<p>Get a factory for pre-configured documents. Can be called just like pydoxtools.Document class, but automatically gets assigned the same configuration as all Documents in this bag</p> <p>name : <code>&lt;DocumentBag&gt;.x('Document') or &lt;DocumentBag&gt;.Document</code></p> <p>return type : typing.Any</p> <p>supports pipeline flows : *, &lt;class 'dask.bag.core.Bag'&gt;, &lt;class 'list'&gt;, &lt;class 'pathlib.Path'&gt;, &lt;class 'pydoxtools.document.DatabaseSource'&gt;</p>"},{"location":"documentbag/#_stats","title":"_stats","text":"<p>A constant value</p> <p>name : <code>&lt;DocumentBag&gt;.x('_stats') or &lt;DocumentBag&gt;._stats</code></p> <p>return type : typing.Any</p> <p>supports pipeline flows : *, &lt;class 'dask.bag.core.Bag'&gt;, &lt;class 'list'&gt;, &lt;class 'pathlib.Path'&gt;, &lt;class 'pydoxtools.document.DatabaseSource'&gt;</p>"},{"location":"documentbag/#apply","title":"apply","text":"<p>Basically it creates a Documentbag from two sets of     on documents in a dask bag and then creates a new DocumentBag from that. This     works similar to pandas dataframes and series. But with documents     as a basic datatype. And apply functions are also required to     produce data which can be used as a document again (which is a lot).</p> <p>name : <code>&lt;DocumentBag&gt;.x('apply') or &lt;DocumentBag&gt;.apply</code></p> <p>return type : typing.Callable[..., pydoxtools.document.DocumentBag]</p> <p>supports pipeline flows : *, &lt;class 'dask.bag.core.Bag'&gt;, &lt;class 'list'&gt;, &lt;class 'pathlib.Path'&gt;, &lt;class 'pydoxtools.document.DatabaseSource'&gt;</p>"},{"location":"documentbag/#bag","title":"bag","text":"pipe_type description &lt;class 'list'&gt;, &lt;class 'pydoxtools.document.DatabaseSource'&gt; No documentation &lt;class 'pathlib.Path'&gt; create a dask bag with all the filepaths in it <p>name : <code>&lt;DocumentBag&gt;.x('bag') or &lt;DocumentBag&gt;.bag</code></p> <p>return type : typing.Any</p> <p>supports pipeline flows : &lt;class 'list'&gt;, &lt;class 'pathlib.Path'&gt;, &lt;class 'pydoxtools.document.DatabaseSource'&gt;</p>"},{"location":"documentbag/#bag_apply","title":"bag_apply","text":"<p>Basically it applies a function element-wise     on documents in a dask bag and then creates a new DocumentBag from that. This     works similar to pandas dataframes and series. But with documents     as a basic datatype. And apply functions are also required to     produce data which can be used as a document again (which is a lot).</p> <p>name : <code>&lt;DocumentBag&gt;.x('bag_apply') or &lt;DocumentBag&gt;.bag_apply</code></p> <p>return type : typing.Callable[..., dask.bag.core.Bag]</p> <p>supports pipeline flows : *, &lt;class 'dask.bag.core.Bag'&gt;, &lt;class 'list'&gt;, &lt;class 'pathlib.Path'&gt;, &lt;class 'pydoxtools.document.DatabaseSource'&gt;</p>"},{"location":"documentbag/#compute","title":"compute","text":"<p>No documentation</p> <p>name : <code>&lt;DocumentBag&gt;.x('compute') or &lt;DocumentBag&gt;.compute</code></p> <p>return type : typing.Any</p> <p>supports pipeline flows : &lt;class 'dask.bag.core.Bag'&gt;, &lt;class 'list'&gt;, &lt;class 'pathlib.Path'&gt;, &lt;class 'pydoxtools.document.DatabaseSource'&gt;</p>"},{"location":"documentbag/#connection_string","title":"connection_string","text":"<p>No documentation</p> <p>name : <code>&lt;DocumentBag&gt;.x('connection_string') or &lt;DocumentBag&gt;.connection_string</code></p> <p>return type : typing.Any</p> <p>supports pipeline flows : &lt;class 'pydoxtools.document.DatabaseSource'&gt;</p>"},{"location":"documentbag/#d","title":"d","text":"<p>Alias for: </p> <p>* get_dicts-&gt;d (output)</p> <p>name : <code>&lt;DocumentBag&gt;.x('d') or &lt;DocumentBag&gt;.d</code></p> <p>return type : typing.Callable[[typing.Any], dask.bag.core.Bag]</p> <p>supports pipeline flows : *, &lt;class 'dask.bag.core.Bag'&gt;, &lt;class 'list'&gt;, &lt;class 'pathlib.Path'&gt;, &lt;class 'pydoxtools.document.DatabaseSource'&gt;</p>"},{"location":"documentbag/#dataframe","title":"dataframe","text":"<p>Load a table using dask/pandas read_sql</p> <pre><code>sql: can either be the entire table or an SQL expression\n</code></pre> <p>name : <code>&lt;DocumentBag&gt;.x('dataframe') or &lt;DocumentBag&gt;.dataframe</code></p> <p>return type : &lt;class 'dask.dataframe.core.DataFrame'&gt;</p> <p>supports pipeline flows : &lt;class 'pydoxtools.document.DatabaseSource'&gt;</p>"},{"location":"documentbag/#dir_list","title":"dir_list","text":"pipe_type description &lt;class 'list'&gt; Applies any function on items in a dask bag and filters them based on the result. if func returns False, the element will be dropped from the bag. &lt;class 'pathlib.Path'&gt; No documentation <p>name : <code>&lt;DocumentBag&gt;.x('dir_list') or &lt;DocumentBag&gt;.dir_list</code></p> <p>return type : &lt;class 'dask.bag.core.Bag'&gt; | typing.Any</p> <p>supports pipeline flows : &lt;class 'list'&gt;, &lt;class 'pathlib.Path'&gt;</p>"},{"location":"documentbag/#docs","title":"docs","text":"pipe_type description &lt;class 'dask.bag.core.Bag'&gt; Alias for: * source-&gt;docs (output) &lt;class 'pydoxtools.document.DatabaseSource'&gt; Create a dask bag of one data document for each row of the source table &lt;class 'list'&gt;, &lt;class 'pathlib.Path'&gt; create a bag with one document for each file that was foundFrom this point we can hand off the logic to str(Bag) pipeline. <p>name : <code>&lt;DocumentBag&gt;.x('docs') or &lt;DocumentBag&gt;.docs</code></p> <p>return type : &lt;class 'dask.bag.core.Bag'&gt; | typing.Any</p> <p>supports pipeline flows : &lt;class 'dask.bag.core.Bag'&gt;, &lt;class 'list'&gt;, &lt;class 'pathlib.Path'&gt;, &lt;class 'pydoxtools.document.DatabaseSource'&gt;</p>"},{"location":"documentbag/#e","title":"e","text":"<p>Alias for: </p> <p>* exploded-&gt;e (output)</p> <p>name : <code>&lt;DocumentBag&gt;.x('e') or &lt;DocumentBag&gt;.e</code></p> <p>return type : typing.Callable[..., ForwardRef('DocumentBag')]</p> <p>supports pipeline flows : *, &lt;class 'dask.bag.core.Bag'&gt;, &lt;class 'list'&gt;, &lt;class 'pathlib.Path'&gt;, &lt;class 'pydoxtools.document.DatabaseSource'&gt;</p>"},{"location":"documentbag/#exploded","title":"exploded","text":"<p>name : <code>&lt;DocumentBag&gt;.x('exploded') or &lt;DocumentBag&gt;.exploded</code></p> <p>return type : typing.Callable[..., ForwardRef('DocumentBag')]</p> <p>supports pipeline flows : *, &lt;class 'dask.bag.core.Bag'&gt;, &lt;class 'list'&gt;, &lt;class 'pathlib.Path'&gt;, &lt;class 'pydoxtools.document.DatabaseSource'&gt;</p>"},{"location":"documentbag/#file_path_list","title":"file_path_list","text":"pipe_type description &lt;class 'list'&gt; Applies any function on items in a dask bag and filters them based on the result. if func returns False, the element will be dropped from the bag. &lt;class 'pathlib.Path'&gt; No documentation <p>name : <code>&lt;DocumentBag&gt;.x('file_path_list') or &lt;DocumentBag&gt;.file_path_list</code></p> <p>return type : &lt;class 'dask.bag.core.Bag'&gt; | typing.Any</p> <p>supports pipeline flows : &lt;class 'list'&gt;, &lt;class 'pathlib.Path'&gt;</p>"},{"location":"documentbag/#get_dicts","title":"get_dicts","text":"<p>Returns a function closure which returns a bag of the specified     property of the enclosed documents.</p> <p>name : <code>&lt;DocumentBag&gt;.x('get_dicts') or &lt;DocumentBag&gt;.get_dicts</code></p> <p>return type : typing.Callable[[typing.Any], dask.bag.core.Bag]</p> <p>supports pipeline flows : *, &lt;class 'dask.bag.core.Bag'&gt;, &lt;class 'list'&gt;, &lt;class 'pathlib.Path'&gt;, &lt;class 'pydoxtools.document.DatabaseSource'&gt;</p>"},{"location":"documentbag/#index_column","title":"index_column","text":"<p>No documentation</p> <p>name : <code>&lt;DocumentBag&gt;.x('index_column') or &lt;DocumentBag&gt;.index_column</code></p> <p>return type : typing.Any</p> <p>supports pipeline flows : &lt;class 'pydoxtools.document.DatabaseSource'&gt;</p>"},{"location":"documentbag/#paths","title":"paths","text":"<p>name : <code>&lt;DocumentBag&gt;.x('paths') or &lt;DocumentBag&gt;.paths</code></p> <p>return type : typing.Callable</p> <p>supports pipeline flows : &lt;class 'list'&gt;, &lt;class 'pathlib.Path'&gt;</p>"},{"location":"documentbag/#root_path","title":"root_path","text":"<p>Alias for: </p> <p>* source-&gt;root_path (output)</p> <p>name : <code>&lt;DocumentBag&gt;.x('root_path') or &lt;DocumentBag&gt;.root_path</code></p> <p>return type : typing.Any</p> <p>supports pipeline flows : &lt;class 'list'&gt;, &lt;class 'pathlib.Path'&gt;</p>"},{"location":"documentbag/#sql","title":"sql","text":"<p>No documentation</p> <p>name : <code>&lt;DocumentBag&gt;.x('sql') or &lt;DocumentBag&gt;.sql</code></p> <p>return type : typing.Any</p> <p>supports pipeline flows : &lt;class 'pydoxtools.document.DatabaseSource'&gt;</p>"},{"location":"documentbag/#stats","title":"stats","text":"<p>gather a number of statistics from documents as a pandas dataframe</p> <p>name : <code>&lt;DocumentBag&gt;.x('stats') or &lt;DocumentBag&gt;.stats</code></p> <p>return type : typing.Any</p> <p>supports pipeline flows : *, &lt;class 'dask.bag.core.Bag'&gt;, &lt;class 'list'&gt;, &lt;class 'pathlib.Path'&gt;, &lt;class 'pydoxtools.document.DatabaseSource'&gt;</p>"},{"location":"documentbag/#take","title":"take","text":"<p>No documentation</p> <p>name : <code>&lt;DocumentBag&gt;.x('take') or &lt;DocumentBag&gt;.take</code></p> <p>return type : typing.Any</p> <p>supports pipeline flows : &lt;class 'dask.bag.core.Bag'&gt;, &lt;class 'list'&gt;, &lt;class 'pathlib.Path'&gt;, &lt;class 'pydoxtools.document.DatabaseSource'&gt;</p>"},{"location":"documentbag/#vectorizer","title":"vectorizer","text":"<p>vectorizes a query, using the document configuration of the Documentbag to determine which model to use.</p> <p>name : <code>&lt;DocumentBag&gt;.x('vectorizer') or &lt;DocumentBag&gt;.vectorizer</code></p> <p>return type : typing.Any</p> <p>supports pipeline flows : *, &lt;class 'dask.bag.core.Bag'&gt;, &lt;class 'list'&gt;, &lt;class 'pathlib.Path'&gt;, &lt;class 'pydoxtools.document.DatabaseSource'&gt;</p>"},{"location":"documentbag/#configuration-parameters","title":"Configuration parameters","text":"name description default_values bytes_per_chunk Configuration for values: 256 MiB * bytes_per_chunk = 256 MiB (default) doc_configuration We can pass through a configuration object to Documents that are created in our document bag. Any setting that is supported by Document can be specified here. {} forgiving_extracts When enabled, if we execute certain batch operations on our document bag, this will not stop the extraction, but rather put an error message in the document. False verbosity Configuration for values: None * verbosity = None (default)"},{"location":"examples/","title":"Examples","text":""},{"location":"examples/#todo-add-examples-in-more-detail","title":"TODO: add examples in more detail","text":"<ul> <li>SQL queries</li> <li>automated blog writing</li> <li>table extraction</li> <li>directory index</li> <li>Pandas-style-NLP</li> </ul>"},{"location":"examples/#checkout-more-examples-here","title":"Checkout more examples here:","text":"<p>https://github.com/Xyntopia/pydoxtools/tree/main/examples</p>"},{"location":"readme_cp/","title":"\ud83c\udfa9\u2728\ud83d\udcc4 pydoxtools (Python Library) \ud83c\udfa9\u2728\ud83d\udcc4","text":"<p>Documentation</p> <p>If you have any problems or questions, please create a github issue. So that other poeple who might want to use it can see the potential solution!</p>"},{"location":"readme_cp/#summary","title":"Summary","text":"<p>Pydoxtools is a library that provides a sophisticated interface for reading and writing documents, designed to work with AI models such as GPT, LLama2, and a variety of models on Huggingface. It offers functionalities such as:b</p> <ul> <li>Pipeline management</li> <li>Integration with AI (LLMs and more) models</li> <li>low-resource (PDF) table extraction without configuration and expensive   layout detection algorithms!</li> <li>Knowledge base extraction as a one-liner</li> <li>Document analysis and question-answering</li> <li>Support for most of todays document formats</li> <li>Vector index Creation</li> <li>Entity, address identification and more</li> <li>List and keyword extraction</li> <li>Data normalization, translation, and cleaning</li> </ul> <p>The library allows for the creation of complex extraction pipelines for batch-processing of documents by defining them as a lazily-executed graph.</p>"},{"location":"readme_cp/#installation","title":"Installation","text":""},{"location":"readme_cp/#installing-from-github","title":"Installing from GitHub","text":"<p>While pydoxtools can already be installed through pip, due to the many updates coming in right now, it is currently recommended to use the latest version from GitHub as follows:</p> <pre><code>pip install -U \"pydoxtools[etl,inference] @ git+https://github.com/xyntopia/pydoxtools.git\"\n</code></pre>"},{"location":"readme_cp/#installing-from-pypi","title":"Installing from PyPI","text":"<p>Pydoxtools can also be installed through pip, which will become the recommended method once it becomes more stable:</p> <pre><code>pip install -U pydoxtools[etl,inference]\n</code></pre> <p>For loading additional file formats (docx, odt, epub), OCR and other options, check out the additional &gt; Installation Options &lt;.</p>"},{"location":"readme_cp/#teaser","title":"\ud83d\ude80 Teaser \ud83d\ude80","text":"<p>Experience a new level of convenience and efficiency in handling documents with Pydoxtools, and reimagine your data extraction pipelines!</p> <p>In this teaser, we'll demonstrate how to create a document, extract tables, and ask questions using AI models:</p> <pre><code>import pydoxtools as pdx\n\n# Create a document from various sources: file, string, bytestring, file-like object, or URL\ndoc = pdx.Document(\"https://www.raspberrypi.org/app/uploads/2012/12/quick-start-guide-v1.1.pdf\")\n\n# List available extraction functions\nprint(doc.x_funcs)\n\n# get all tables from a single document:\nprint(doc.tables)\n\n# Extract the first 20 tables that we can find in a directory (this might take a while,\n# make sure, to only choose a small directory for testing purposes)\ndocs = pdx.DocumentBag(\"./my_directory_with_documents\", forgiving_extracts=True)\nprint(docs.bag_apply([\"tables_df\", \"filename\"]).take(20))\n\n# Ask a question about the documents using a local Q&amp;A model\nprint(doc.answers([\"how much ram does it have?\"]))\n# Or only ask about the documents tables (or any other extracted information):\nprint(doc.answers([\"how much ram does it have?\"], \"tables\"))\n\n# To use ChatGPT for question-answering, set the API key as an environment variable:\n# OPENAI_API_KEY=\"sk ....\"\n# Then, ask questions about the document using ChatGPT\nprint(doc.chat_answers([\"What is the target group of this document?\"])[0].content)\nprint(doc.chat_answers([\"Answer if a 5-year old would be able to follow these instructions?\"])[0].content)\n</code></pre> <p>With Pydoxtools, you can easily access and process your documents, perform various extractions, and utilize AI models for more advanced analysis.</p>"},{"location":"readme_cp/#supported-file-formats","title":"Supported File Formats","text":"<p>Pydoxtools already supports loading from a large variety of different sources:</p> <ul> <li>Documents from URLs,</li> <li>pdf, html, docx, doc, odt, markdwn, rtf, epub, mediawiki</li> <li>everything supported by pandoc,</li> <li>images (png, jpg, bmp, tiff etc...),</li> <li>And some \"native-python\" dataformats: PIL.Image.Image, ,  <li>data formats: yaml (json in progress)</li> <li>And more!</li>"},{"location":"readme_cp/#some-features-in-more-detail","title":"Some Features in More Detail","text":""},{"location":"readme_cp/#large-pipelines","title":"Large Pipelines","text":"<p>Pydoxtools' main feature is the ability to mix LLMs and other AI models in large, composable, and customizable pipelines. Using pipelines comes with the slight disadvantage that it can be more challenging to add type hints to the code. However, using pipelines decouples all parts of your code, allowing all operators to work independently. This makes it easy to run the pipeline in a distributed setting for big data and enables easy, lazy evaluation. Additionally, mixing different LLM logics together becomes much easier.</p> <p>Check out how Pydoxtools' <code>Document</code> class mixes pipelines for each individual file type:</p> <ul> <li>Every node in an ellipse can be called as an attribute of the document-analysis pipeline.</li> <li>Every execution path is lazily executed throughout the entire graph.</li> <li>Every node is cached by default (but can be turned off).</li> <li>Every piece of this pipeline can be replaced by a customized version.</li> </ul> <p>As an example, consider this pipeline for *.png images from the repository, which includes OCR, keyword extraction, vectorization, and more:</p> <p></p> <p>Pipelines can be mixed, partially overwritten, and extended, giving you a lot of possibilities to extend and adapt the functionality for your specific use case.</p> <p>To learn more about Pydoxtools' large pipelines feature, please refer to the documentation.</p>"},{"location":"readme_cp/#pipeline-configuration","title":"Pipeline Configuration","text":"<p>Pipelines can be configured. For example the local model used for question answering can be selected like this:</p> <pre><code>doc = Document(fobj=\"./data/PFR-PR23_BAT-110__V1.00_.pdf\")\n.config(qam_model_id='bert-large-uncased-whole-word-masking-finetuned-squad')\n</code></pre> <p>where \"qam_model_id\" can be any model from huggingface for question answering.</p> <p>You can get a list of configuration options like this:</p> <pre><code>doc.configuration\n\n# &gt;&gt; will give you something like this:\n# {'spacy_model_size': 'md',\n# 'spacy_model': 'auto',\n# 'use_clean_text_for_spacy': True,\n# 'coreference_method': 'fast',\n# 'graph_debug_context_size': 0,\n# 'vectorizer_model': 'sentence-transformers/all-MiniLM-L6-v2',\n# 'vectorizer_only_tokenizer': False,\n# 'vectorizer_overlap_ratio': 0.1,\n# 'min_size_text_segment': 256,\n# 'max_size_text_segment': 512,\n# 'text_segment_overlap': 0.3,\n# 'max_text_segment_num': 100,\n# 'top_k_text_rank_keywords': 5,\n# 'top_k_text_rank_sentences': 5,\n# 'summarizer_model': 'sshleifer/distilbart-cnn-12-6',\n# 'summarizer_token_overlap': 50,\n# 'summarizer_max_text_len': 200,\n# 'qam_model_id': 'deepset/minilm-uncased-squad2',\n# 'chat_model_id': 'gpt-3.5-turbo',\n# 'image_dpi': 216,\n# 'ocr_lang': 'auto',\n# 'ocr_on': True}\n</code></pre> <p>For more information check the -&gt; documentation:</p>"},{"location":"readme_cp/#pdf-table-extraction-algorithms","title":"PDF Table Extraction Algorithms","text":"<p>The library features its own sophisticated Table extraction algorithm which is benchmarked against a large pdf table dataset. In contrast to how most \"classical\" table extraction algorithms work, it doesn't require:</p> <ul> <li>extensive configuration</li> <li>no expensive deep neural networks for table area recognition which need a GPU and   a lot of memory/CPU requirements</li> </ul> <p>This makes it possible to run analysis on PDF files with pydoxtools on CPU with very limited resources!</p>"},{"location":"readme_cp/#todo-describe-more-of-the-features-here","title":"TODO: Describe more of the features here...","text":""},{"location":"readme_cp/#use-cases","title":"Use Cases","text":"<ul> <li>create new documents from unstructured information</li> <li>analyze documents using any model from huggingface</li> <li>analyze documents using a custom model</li> <li>download a pdf from URL</li> <li>generate document keywords</li> <li>extract tables</li> <li>download document from URL \"manually\" and then feed to document</li> <li>extract addresses</li> <li>extract addresses and use this information for the qam</li> <li>ingest documents into a vector db</li> </ul>"},{"location":"readme_cp/#installation-options","title":"Installation Options","text":"<p>If you simply want to get going, you can install the following libraries on your system which will do evrything for you:</p> <pre><code>sudo apt-get install tesseract-ocr tesseract-ocr-deu tesseract-ocr-fra tesseract-ocr-eng tesseract-ocr-spa \\\n                     poppler-utils graphviz graphviz-dev \\\nsudo apt-get install pandoc\n# OR (for getting the newest version with all features)\n# cd /tmp\n# wget https://github.com/jgm/pandoc/releases/download/2.19.2/pandoc-2.19.2-1-amd64.deb\n# dpkg -i pandoc-2.19.2-1-amd64.deb\n</code></pre> <p>Below are some explanation what the different</p>"},{"location":"readme_cp/#supporting-docx-odt-epub","title":"Supporting *.docx, *.odt, *.epub","text":"<p>In order to be able to load docx, odt and rtf files, you have to install pandoc. Right now, the python pandoc library does not work with pandoc version &gt; 3.0.0. It is therefore recommended to install a version from here for your OS:</p> <p>https://github.com/jgm/pandoc/releases/tag/2.19.2</p>"},{"location":"readme_cp/#image-ocr-support","title":"Image OCR Support","text":"<p>Pydoxtools can automatically analyze images as well, makin use of OCR. In order to be able to use this, install tesseract on your system:</p> <p>Under linux this looks like the following:</p> <pre><code>sudo apt-get update &amp;&amp; sudo apt-get tesseract-ocr\n# install tesseract languages \n# Display a list of all Tesseract language packs:\n#   apt-cache search tesseract-ocr\n# install all languages:\n# sudo apt install tesseract-ocr-*\n# install only german, french, english, spanish language packs\nsudo apt install tesseract-ocr-deu tesseract-ocr-fra tesseract-ocr-eng tesseract-ocr-spa\n</code></pre>"},{"location":"readme_cp/#pdf-image-rendering","title":"pdf image rendering","text":"<p>For pdf rendering, Pydoxtools makes use of a library \"poppler\" which needs to be installed on your system. Under linux, this looks like the following:</p> <pre><code>sudo apt-get install poppler-utils\n</code></pre>"},{"location":"readme_cp/#graphviz","title":"Graphviz","text":"<p>For visualizing the document logic, you need to install graphviz on your system. Under linux, this looks like the following:</p> <pre><code>sudo apt-get install graphviz graphviz-dev\n</code></pre>"},{"location":"readme_cp/#development","title":"Development","text":"<p>--&gt; see </p>"},{"location":"readme_cp/#license","title":"License","text":"<p>This project is licensed under the terms of MIT license.</p> <p>You can check the compatibility using the following tool in a venv environment in a production setting:</p> <pre><code>pip install pip-licenses\npip-licenses | grep -Ev 'MIT License|BSD License|Apache Software License|Python Software Foundation License|Apache 2.0|MIT|Apache License 2.0|hnswlib|Pillow|new BSD|BSD'\n</code></pre>"},{"location":"readme_cp/#dependencies","title":"Dependencies","text":"<p>Here is a list of Libraries, that this project is based on:</p> <p>list</p>"},{"location":"readme_cp/#changelog","title":"Changelog","text":"<p>changelog</p>"},{"location":"reference/","title":"Reference","text":""},{"location":"reference/#pydoxtools.document_base.Pipeline","title":"<code>Pipeline</code>","text":"<p>Base class for all document classes in pydoxtools, defining a common pipeline interface and establishing a basic pipeline schema that derived classes can override.</p> <p>The MetaPipelineClassConfiguration acts as a compiler to resolve the pipeline hierarchy, allowing pipelines to inherit, mix, extend, or partially overwrite each other. Each key in the _pipelines dictionary represents a different pipeline version.</p> <p>The pydoxtools.Document class leverages this functionality to build separate pipelines for different file types, as the information processing requirements differ significantly between file types.</p> <p>Attributes:</p> Name Type Description <code>_operators</code> <code>dict[str, list[Operator]]</code> <p>Stores the definition of the pipeline graph, a collection of connected operators/functions that process data from a document.</p> <code>_pipelines</code> <code>dict[str, dict[str, Operator]]</code> <p>Provides access to all operator functions by their \"out-key\" which was defined in _operators.</p> Todo <ul> <li>Use pandera (https://github.com/unionai-oss/pandera) to validate dataframes   exchanged between operators &amp; loaders   (https://pandera.readthedocs.io/en/stable/pydantic_integration.html)</li> </ul>"},{"location":"reference/#pydoxtools.document_base.Pipeline.configuration","title":"<code>configuration</code>  <code>property</code>","text":"<p>Returns a dictionary of all configuration objects for the current pipeline.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the names and values of all configuration objects   for the current pipeline.</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.pipeline_chooser","title":"<code>pipeline_chooser: str</code>  <code>property</code>","text":"<p>Must be implemented by derived classes to decide which pipeline they should use.</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.uuid","title":"<code>uuid</code>  <code>cached</code> <code>property</code>","text":"<p>Retrieves a universally unique identifier (UUID) for the instance.</p> <p>This method generates a new UUID for the instance using Python's <code>uuid.uuid4()</code> function. The UUID is then cached as a property, ensuring that the same UUID is returned for subsequent accesses.</p> <p>Returns:</p> Type Description <p>uuid.UUID: A unique identifier for the instance.</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.x_funcs","title":"<code>x_funcs: dict[str, Operator]</code>  <code>cached</code> <code>property</code>","text":"<p>get all operators/pipeline nodes and their property names for this specific file type/pipeline</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.__getattr__","title":"<code>__getattr__(extract_name)</code>","text":"<p>Retrieves an extractor result by directly accessing it as an attribute.</p> <p>This method is automatically called for attribute names that aren't defined on class level, allowing for a convenient way to access pipeline operator outputs without needing to call the 'x' method.</p> Example <p>document.addresses instead of document.x('addresses')</p> <p>Parameters:</p> Name Type Description Default <code>extract_name</code> <code>str</code> <p>The name of the extractor result to be accessed.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The result of the extractor after processing the document.</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.__getitem__","title":"<code>__getitem__(extract_name)</code>","text":"<p>Retrieves an extractor result by directly accessing it as an attribute.</p> <p>This method is automatically called for attribute names that aren't defined on class level, allowing for a convenient way to access pipeline operator outputs without needing to call the 'x' method.</p> Example <p>document[\"addresses\"] instead of document.x('addresses')</p> <p>Parameters:</p> Name Type Description Default <code>extract_name</code> <code>str</code> <p>The name of the extractor result to be accessed.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The result of the extractor after processing the document.</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.__getstate__","title":"<code>__getstate__()</code>","text":"<p>return necessary variables for pickling, ensuring that we leave out everything that can potentially have a lambda function in it...</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.__init__","title":"<code>__init__(**configuration)</code>","text":"<p>Initializes the Pipeline instance with cache-related attributes.</p> <p>**configuration: A dictionary of key-value pairs representing the configuration         settings for the pipeline. Each key is a string representing the name         of the configuration setting, and the value is the corresponding value         to be set.</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.__repr__","title":"<code>__repr__()</code>","text":"<p>Returns:</p> Name Type Description <code>str</code> <p>A string representation of the instance.</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.__setstate__","title":"<code>__setstate__(state)</code>","text":"<p>we need to restore _x_func_cache for pickling to work...</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.gather_inputs","title":"<code>gather_inputs(mapped_args, traceable)</code>","text":"<p>Gathers arguments from the pipeline and class, and maps them to the provided keys of kwargs.</p> <p>This method retrieves all required input parameters from _in_mapping, which was declared with \"pipe\". It first checks if the parameter is available as an extractor. If so, it calls the function to get the value. Otherwise, it gets the member-variables or functions of the derived pipeline class if an extractor with that name cannot be found.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>dict</code> <p>A dictionary containing the keys to be mapped to the corresponding values.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the mapped keys and their corresponding values.</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.get_configuration_names","title":"<code>get_configuration_names(pipeline)</code>  <code>cached</code> <code>classmethod</code>","text":"<p>Returns a list of names of all configuration objects for a given pipeline.</p> <p>This is a cached function which is important,</p> <p>Parameters:</p> Name Type Description Default <code>pipeline</code> <code>str</code> <p>The name of the pipeline to retrieve configuration objects from.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list[str]</code> <p>A list of strings containing the names of all configuration objects for the   given pipeline.</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.node_infos","title":"<code>node_infos(pipeline_type=None)</code>  <code>classmethod</code>","text":"<p>Aggregates the pipeline operations and their corresponding types and metadata.</p> <p>This method iterates through all the pipelines registered in the class, and gathers information about each node/operation, such as the pipeline types it appears in, the return type of the operation, and the operation's docstring.</p> <p>Returns:</p> Type Description <code>list[dict]</code> <p>TODO...</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.non_interactive_pipeline","title":"<code>non_interactive_pipeline()</code>","text":"<p>return all non-interactive operators/pipeline nodes</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.operator_types","title":"<code>operator_types()</code>  <code>classmethod</code>","text":"<p>This function returns a dictionary of operators with their types which is suitable for declaring a pydantic model.</p> if this is set to True, we make sure that only valid json <p>schema types are included in the model. The typical use case is to expose the pipeline via this model to an http API e.g. through fastapi. In this case we should only allow types that are valid json schema. Therefore, this is set to \"False\" by default.</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.pipeline_graph","title":"<code>pipeline_graph(image_path=None, document_logic_id='*')</code>  <code>classmethod</code>","text":"<p>Generates a visualization of the defined pipelines and optionally saves it as an image.</p> <p>Parameters:</p> Name Type Description Default <code>image_path</code> <code>str | Path</code> <p>File path for the generated image. If provided, the                                        generated graph will be saved as an image.</p> <code>None</code> <code>document_logic_id</code> <code>str</code> <p>The document logic ID for which the pipeline graph should                                be generated. Defaults to \"current\".</p> <code>'*'</code> <p>Returns:</p> Name Type Description <code>AGraph</code> <p>A PyGraphviz AGraph object representing the pipeline graph. This object can be     visualized or manipulated using PyGraphviz functions.</p> Notes <p>This method requires the NetworkX and PyGraphviz libraries to be installed.</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.pre_cache","title":"<code>pre_cache()</code>","text":"<p>Pre-caches the results of all operators that have caching enabled.</p> <p>This method iterates through the defined operators and calls each one with caching enabled, storing the results for faster access in future calls.</p> <p>Returns:</p> Name Type Description <code>self</code> <p>The instance of the class, allowing for method chaining.</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.run_pipeline","title":"<code>run_pipeline(exclude=None)</code>","text":"<p>Runs all operators defined in the pipeline for testing or pre-caching purposes.</p> <p>!!IMPORTANT!!!  This function should normally not be used as the pipeline is lazily executed anyway.</p> <p>This method iterates through the defined operators and calls each one, ensuring that the extractor logic is functioning correctly and caching the results if required.</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.run_pipeline_fast","title":"<code>run_pipeline_fast()</code>","text":"<p>run pipeline, but exclude long-running calculations</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.set_disk_cache_settings","title":"<code>set_disk_cache_settings(enable, ttl=3600 * 24 * 7)</code>","text":"<p>Sets disk cache settings</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.to_dict","title":"<code>to_dict(*args, **kwargs)</code>","text":"<p>Returns a dictionary that accumulates the properties given in args or with a mapping in *kwargs.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>str</code> <p>A variable number of strings, each representing a property name.</p> <code>()</code> <code>**kwargs</code> <code>dict</code> <p>A dictionary mapping property names (values) to custom keys (keys) for the              returned dictionary.</p> <code>{}</code> Note <p>This function currently only supports properties that do not require any arguments, such as \"full_text\". Properties like \"answers\" that return a function requiring arguments cannot be used with this function.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary with the accumulated properties and their values, using either the   property names or custom keys as specified in the input arguments.</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.to_json","title":"<code>to_json(*args, **kwargs)</code>","text":"<p>Returns a dictionary that accumulates the properties given in args or with a mapping in *kwargs, and dumps the output as JSON.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>str</code> <p>A variable number of strings, each representing a property name.</p> <code>()</code> <code>**kwargs</code> <code>dict</code> <p>A dictionary mapping property names (values) to custom keys (keys) for the              returned dictionary.</p> <code>{}</code> Note <p>This function currently only supports properties that do not require any arguments, such as \"full_text\". Properties like \"answers\" that return a function requiring arguments cannot be used with this function.</p> <p>Returns:</p> Name Type Description <code>str</code> <p>A JSON-formatted string representing the accumulated properties and their values, using  either the property names or custom keys as specified in the input arguments.</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.to_yaml","title":"<code>to_yaml(*args, **kwargs)</code>","text":"<p>Returns a dictionary that accumulates the properties given in args or with a mapping in *kwargs, and dumps the output as YAML.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>str</code> <p>A variable number of strings, each representing a property name.</p> <code>()</code> <code>**kwargs</code> <code>dict</code> <p>A dictionary mapping property names (values) to custom keys (keys) for the              returned dictionary.</p> <code>{}</code> Note <p>This function currently only supports properties that do not require any arguments, such as \"full_text\". Properties like \"answers\" that return a function requiring arguments cannot be used with this function.</p> <p>Returns:</p> Name Type Description <code>str</code> <p>A YAML-formatted string representing the accumulated properties and their values, using  either the property names or custom keys as specified in the input arguments.</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.x","title":"<code>x(operator_name, disk_cache=False, traceable=False)</code>","text":"<p>Calls an extractor from the defined pipeline and returns the result.</p> <p>Parameters:</p> Name Type Description Default <code>operator_name</code> <code>str</code> <p>The name of the extractor to be called.</p> required <code>cache</code> <p>if we want to cache the call. We can explicitly tell     the pipeline to cache a call. to make caching more efficient     by only caching the calls we want.</p> required <code>traceable</code> <code>bool</code> <p>Some operators will propagate the source of their information        through the pipeline. This adds traceability. By setting this to        traceable=True we can turn this feature on.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The result of the extractor after processing the document.</p> <p>Raises:</p> Type Description <code>OperatorException</code> <p>If an error occurs while executing the extractor.</p> Notes <p>The extractor's parameters can be overridden using args and *kwargs.</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.x_all","title":"<code>x_all()</code>","text":"<p>Retrieves the results of all operators defined in the pipeline.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the results of all operators, with keys as the extractor   names and values as the corresponding results.</p>"}]}