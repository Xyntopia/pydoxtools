{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Pydoxtools Documentation!","text":"<p>Readme</p>"},{"location":"#introduction","title":"Introduction","text":"<p>The main interface that pydoxtools uses are three classes:</p> <ul> <li>pydoxtools.Document</li> <li>pydoxtools.DocumentSet</li> <li>pydoxtools.Pipeline</li> </ul> <p>And a set of operators:</p> <ul> <li>pydoxtools.operators</li> </ul>"},{"location":"#analyzing-document","title":"Analyzing Document","text":"<p>Document &amp; DocumentSet are both using pydoxtools.Pipeline and predefine a complex pipeline to extract data from individual documents or a set of documents. A list of all the \"out-of-the-box\" featurs for each pipeline can be found in:</p> <p>-&gt; Pipelines</p>"},{"location":"#building-your-own-pipelines-with-llms-large-language-models-and-other-types-of-ai","title":"Building your own Pipelines with LLMs (Large Language Models) and other types of AI","text":"<p>the Pipeline class can be used to build complex, custom pipelines which have several out-of-the-box features which makes them easy to use in modern pipelines involving the use of a lot of AI tools:</p> <ul> <li>they can be mixed, extended, (partially) overwritten with other pipelines</li> <li>they can export/import their data (yaml, json, python-dict),</li> <li>they can be configured &amp; optimized</li> <li>they can convert their data into pydoxtools.Document and pydoxtools.DocumentSet</li> </ul> <p>Additionally, in order to develop a custom pipeline, pydoxtools has a large library of pydoxtools.operators which can be used to build your custom pipeline. It usually makes sense to use pydoxtools.Document or pydoxtools.DocumentSet as a base for a new pipeline and only replace small parts of them in order to get desired custom functionality.</p>"},{"location":"#visualization-of-the-pipelines","title":"Visualization of the Pipelines","text":"<p>The pipelines can be visualized which helps a lot when developing your own pipeline on top of a complex pipeline such as the document pipeline. The extraction logic for different file types can be visualized like this:</p> <pre><code>doc = Document(fobj=make_path_absolute(\"./data/demo.docx\"), document_type=\".docx\")\n# for the currently loaded file type:\ndoc.logic_graph(image_path=settings._PYDOXTOOLS_DIR / \"docs/images/document_logic_docx.svg\")\n# for the \ndoc.logic_graph(image_path=settings._PYDOXTOOLS_DIR / \"docs/images/document_logic_png.svg\", document_logic_id=\".png\")\n</code></pre> <p>This way we can generate the pipelines for different filetypes:</p> <ul> <li>docx</li> <li>png   (click on links to open the images!)</li> </ul> <p>Pipelines for every supported file type can be found here.</p> <p>This also works for custom pipelines!</p> <p>In order to learn more continue to: Reference</p>"},{"location":"DEVELOPMENT/","title":"Development &amp; Contribution","text":"<p>The graph model of the library makes it very easy to extend it with new functionality.</p> <ul> <li>the document can be used as a base-model and overwritten with changes</li> <li>the graph can be changed dynamically</li> <li>new functions can be very easily integrated</li> </ul>"},{"location":"DEVELOPMENT/#pydoxtools-architecture","title":"Pydoxtools Architecture","text":"<p>--&gt; refer to \"document\"</p>"},{"location":"DEVELOPMENT/#contribution-guidelines","title":"Contribution Guidelines","text":""},{"location":"pipelines/","title":"Pipelines","text":"<p>This documents the output values of the nodes of each pipeline that  can be accessed through the pipeline interface.</p> <p>Pipeline visualizations for every supported file type can be found here.</p>"},{"location":"pipelines/#pydoxtoolsdocument","title":"pydoxtools.Document","text":""},{"location":"pipelines/#raw_content","title":"raw_content","text":"<p>Can be called using:</p> <pre><code>doc.x('raw_content')\n# or\ndoc.raw_content\n</code></pre> <p>return type : bytes | str</p> <p>supports pipelines : *,.docx,.epub,.html,.jpeg,.jpg,.markdown,.md,.odt,.pdf,.png,.rtf,.tif,.tiff,.yaml,dict,image,pandoc</p>"},{"location":"pipelines/#full_text","title":"full_text","text":"<p>Can be called using:</p> <pre><code>doc.x('full_text')\n# or\ndoc.full_text\n</code></pre> <p>return type :  <p>supports pipelines : *,.docx,.epub,.html,.jpeg,.jpg,.markdown,.md,.odt,.pdf,.png,.rtf,.tif,.tiff,.yaml,dict,image,pandoc</p>"},{"location":"pipelines/#clean_text","title":"clean_text","text":"<p>Can be called using:</p> <pre><code>doc.x('clean_text')\n# or\ndoc.clean_text\n</code></pre> <p>return type :  <p>supports pipelines : *,.docx,.epub,.html,.jpeg,.jpg,.markdown,.md,.odt,.pdf,.png,.rtf,.tif,.tiff,.yaml,dict,image,pandoc</p>"},{"location":"pipelines/#text_box_elements","title":"text_box_elements","text":"<p>Can be called using:</p> <pre><code>doc.x('text_box_elements')\n# or\ndoc.text_box_elements\n</code></pre> <p>return type : </p> <p>supports pipelines : *,.docx,.epub,.html,.jpeg,.jpg,.markdown,.md,.odt,.pdf,.png,.rtf,.tif,.tiff,.yaml,dict,image,pandoc</p>"},{"location":"pipelines/#text_box_list","title":"text_box_list","text":"<p>Can be called using:</p> <pre><code>doc.x('text_box_list')\n# or\ndoc.text_box_list\n</code></pre> <p>return type : </p> <p>supports pipelines : *,.docx,.epub,.html,.jpeg,.jpg,.markdown,.md,.odt,.pdf,.png,.rtf,.tif,.tiff,.yaml,dict,image,pandoc</p>"},{"location":"pipelines/#tables_df","title":"tables_df","text":"<p>Can be called using:</p> <pre><code>doc.x('tables_df')\n# or\ndoc.tables_df\n</code></pre> <p>return type : str | list[str] | list[pandas.core.frame.DataFrame]</p> <p>supports pipelines : *,.docx,.epub,.html,.jpeg,.jpg,.markdown,.md,.odt,.pdf,.png,.rtf,.tif,.tiff,.yaml,dict,image,pandoc</p>"},{"location":"pipelines/#tables_dict","title":"tables_dict","text":"<p>Can be called using:</p> <pre><code>doc.x('tables_dict')\n# or\ndoc.tables_dict\n</code></pre> <p>return type : </p> <p>supports pipelines : *,.docx,.epub,.html,.jpeg,.jpg,.markdown,.md,.odt,.pdf,.png,.rtf,.tif,.tiff,.yaml,dict,image,pandoc</p>"},{"location":"pipelines/#tables","title":"tables","text":"<p>Can be called using:</p> <pre><code>doc.x('tables')\n# or\ndoc.tables\n</code></pre> <p>return type : </p> <p>supports pipelines : *,.docx,.epub,.html,.jpeg,.jpg,.markdown,.md,.odt,.pdf,.png,.rtf,.tif,.tiff,.yaml,dict,image,pandoc</p>"},{"location":"pipelines/#addresses","title":"addresses","text":"<p>Can be called using:</p> <pre><code>doc.x('addresses')\n# or\ndoc.addresses\n</code></pre> <p>return type : </p> <p>supports pipelines : *,.docx,.epub,.html,.jpeg,.jpg,.markdown,.md,.odt,.pdf,.png,.rtf,.tif,.tiff,.yaml,dict,image,pandoc</p>"},{"location":"pipelines/#num_pages","title":"num_pages","text":"<p>Can be called using:</p> <pre><code>doc.x('num_pages')\n# or\ndoc.num_pages\n</code></pre> <p>return type : </p> <p>supports pipelines : *,.docx,.epub,.html,.jpeg,.jpg,.markdown,.md,.odt,.pdf,.png,.rtf,.tif,.tiff,.yaml,dict,image,pandoc</p>"},{"location":"pipelines/#num_words","title":"num_words","text":"<p>Can be called using:</p> <pre><code>doc.x('num_words')\n# or\ndoc.num_words\n</code></pre> <p>return type : </p> <p>supports pipelines : *,.docx,.epub,.html,.jpeg,.jpg,.markdown,.md,.odt,.pdf,.png,.rtf,.tif,.tiff,.yaml,dict,image,pandoc</p>"},{"location":"pipelines/#num_sents","title":"num_sents","text":"<p>Can be called using:</p> <pre><code>doc.x('num_sents')\n# or\ndoc.num_sents\n</code></pre> <p>return type : </p> <p>supports pipelines : *,.docx,.epub,.html,.jpeg,.jpg,.markdown,.md,.odt,.pdf,.png,.rtf,.tif,.tiff,.yaml,dict,image,pandoc</p>"},{"location":"pipelines/#a_d_ratio","title":"a_d_ratio","text":"<p>Can be called using:</p> <pre><code>doc.x('a_d_ratio')\n# or\ndoc.a_d_ratio\n</code></pre> <p>return type : </p> <p>supports pipelines : *,.docx,.epub,.html,.jpeg,.jpg,.markdown,.md,.odt,.pdf,.png,.rtf,.tif,.tiff,.yaml,dict,image,pandoc</p>"},{"location":"pipelines/#language","title":"language","text":"<p>Can be called using:</p> <pre><code>doc.x('language')\n# or\ndoc.language\n</code></pre> <p>return type :  <p>supports pipelines : *,.docx,.epub,.html,.jpeg,.jpg,.markdown,.md,.odt,.pdf,.png,.rtf,.tif,.tiff,.yaml,dict,image,pandoc</p>"},{"location":"pipelines/#spacy_model_size","title":"spacy_model_size","text":"<p>Can be called using:</p> <pre><code>doc.x('spacy_model_size')\n# or\ndoc.spacy_model_size\n</code></pre> <p>return type : </p> <p>supports pipelines : *,.docx,.epub,.html,.jpeg,.jpg,.markdown,.md,.odt,.pdf,.png,.rtf,.tif,.tiff,.yaml,dict,image,pandoc</p>"},{"location":"pipelines/#spacy_model","title":"spacy_model","text":"<p>Can be called using:</p> <pre><code>doc.x('spacy_model')\n# or\ndoc.spacy_model\n</code></pre> <p>return type : </p> <p>supports pipelines : *,.docx,.epub,.html,.jpeg,.jpg,.markdown,.md,.odt,.pdf,.png,.rtf,.tif,.tiff,.yaml,dict,image,pandoc</p>"},{"location":"pipelines/#spacy_doc","title":"spacy_doc","text":"<p>Can be called using:</p> <pre><code>doc.x('spacy_doc')\n# or\ndoc.spacy_doc\n</code></pre> <p>return type : dict[str, typing.Any]</p> <p>supports pipelines : *,.docx,.epub,.html,.jpeg,.jpg,.markdown,.md,.odt,.pdf,.png,.rtf,.tif,.tiff,.yaml,dict,image,pandoc</p>"},{"location":"pipelines/#spacy_nlp","title":"spacy_nlp","text":"<p>Can be called using:</p> <pre><code>doc.x('spacy_nlp')\n# or\ndoc.spacy_nlp\n</code></pre> <p>return type : dict[str, typing.Any]</p> <p>supports pipelines : *,.docx,.epub,.html,.jpeg,.jpg,.markdown,.md,.odt,.pdf,.png,.rtf,.tif,.tiff,.yaml,dict,image,pandoc</p>"},{"location":"pipelines/#spacy_vectors","title":"spacy_vectors","text":"<p>Can be called using:</p> <pre><code>doc.x('spacy_vectors')\n# or\ndoc.spacy_vectors\n</code></pre> <p>return type : </p> <p>supports pipelines : *,.docx,.epub,.html,.jpeg,.jpg,.markdown,.md,.odt,.pdf,.png,.rtf,.tif,.tiff,.yaml,dict,image,pandoc</p>"},{"location":"pipelines/#spacy_embeddings","title":"spacy_embeddings","text":"<p>Can be called using:</p> <pre><code>doc.x('spacy_embeddings')\n# or\ndoc.spacy_embeddings\n</code></pre> <p>return type : </p> <p>supports pipelines : *,.docx,.epub,.html,.jpeg,.jpg,.markdown,.md,.odt,.pdf,.png,.rtf,.tif,.tiff,.yaml,dict,image,pandoc</p>"},{"location":"pipelines/#spacy_sents","title":"spacy_sents","text":"<p>Can be called using:</p> <pre><code>doc.x('spacy_sents')\n# or\ndoc.spacy_sents\n</code></pre> <p>return type : </p> <p>supports pipelines : *,.docx,.epub,.html,.jpeg,.jpg,.markdown,.md,.odt,.pdf,.png,.rtf,.tif,.tiff,.yaml,dict,image,pandoc</p>"},{"location":"pipelines/#spacy_noun_chunks","title":"spacy_noun_chunks","text":"<p>Can be called using:</p> <pre><code>doc.x('spacy_noun_chunks')\n# or\ndoc.spacy_noun_chunks\n</code></pre> <p>return type : </p> <p>supports pipelines : *,.docx,.epub,.html,.jpeg,.jpg,.markdown,.md,.odt,.pdf,.png,.rtf,.tif,.tiff,.yaml,dict,image,pandoc</p>"},{"location":"pipelines/#entities","title":"entities","text":"<p>Can be called using:</p> <pre><code>doc.x('entities')\n# or\ndoc.entities\n</code></pre> <p>return type : dict[str, list[str]]</p> <p>supports pipelines : *,.docx,.epub,.html,.jpeg,.jpg,.markdown,.md,.odt,.pdf,.png,.rtf,.tif,.tiff,.yaml,dict,image,pandoc</p>"},{"location":"pipelines/#url","title":"url","text":"<p>Can be called using:</p> <pre><code>doc.x('url')\n# or\ndoc.url\n</code></pre> <p>return type : </p> <p>supports pipelines : *,.docx,.epub,.html,.jpeg,.jpg,.markdown,.md,.odt,.pdf,.png,.rtf,.tif,.tiff,.yaml,dict,image,pandoc</p>"},{"location":"pipelines/#sents","title":"sents","text":"<p>Can be called using:</p> <pre><code>doc.x('sents')\n# or\ndoc.sents\n</code></pre> <p>return type : </p> <p>supports pipelines : *,.docx,.epub,.html,.jpeg,.jpg,.markdown,.md,.odt,.pdf,.png,.rtf,.tif,.tiff,.yaml,dict,image,pandoc</p>"},{"location":"pipelines/#noun_chunks","title":"noun_chunks","text":"<p>Can be called using:</p> <pre><code>doc.x('noun_chunks')\n# or\ndoc.noun_chunks\n</code></pre> <p>return type : </p> <p>supports pipelines : *,.docx,.epub,.html,.jpeg,.jpg,.markdown,.md,.odt,.pdf,.png,.rtf,.tif,.tiff,.yaml,dict,image,pandoc</p>"},{"location":"pipelines/#vector","title":"vector","text":"<p>Can be called using:</p> <pre><code>doc.x('vector')\n# or\ndoc.vector\n</code></pre> <p>return type : </p> <p>supports pipelines : *,.docx,.epub,.html,.jpeg,.jpg,.markdown,.md,.odt,.pdf,.png,.rtf,.tif,.tiff,.yaml,dict,image,pandoc</p>"},{"location":"pipelines/#sent_vecs","title":"sent_vecs","text":"<p>Can be called using:</p> <pre><code>doc.x('sent_vecs')\n# or\ndoc.sent_vecs\n</code></pre> <p>return type : </p> <p>supports pipelines : *,.docx,.epub,.html,.jpeg,.jpg,.markdown,.md,.odt,.pdf,.png,.rtf,.tif,.tiff,.yaml,dict,image,pandoc</p>"},{"location":"pipelines/#sent_ids","title":"sent_ids","text":"<p>Can be called using:</p> <pre><code>doc.x('sent_ids')\n# or\ndoc.sent_ids\n</code></pre> <p>return type : </p> <p>supports pipelines : *,.docx,.epub,.html,.jpeg,.jpg,.markdown,.md,.odt,.pdf,.png,.rtf,.tif,.tiff,.yaml,dict,image,pandoc</p>"},{"location":"pipelines/#noun_vecs","title":"noun_vecs","text":"<p>Can be called using:</p> <pre><code>doc.x('noun_vecs')\n# or\ndoc.noun_vecs\n</code></pre> <p>return type : </p> <p>supports pipelines : *,.docx,.epub,.html,.jpeg,.jpg,.markdown,.md,.odt,.pdf,.png,.rtf,.tif,.tiff,.yaml,dict,image,pandoc</p>"},{"location":"pipelines/#noun_ids","title":"noun_ids","text":"<p>Can be called using:</p> <pre><code>doc.x('noun_ids')\n# or\ndoc.noun_ids\n</code></pre> <p>return type : </p> <p>supports pipelines : *,.docx,.epub,.html,.jpeg,.jpg,.markdown,.md,.odt,.pdf,.png,.rtf,.tif,.tiff,.yaml,dict,image,pandoc</p>"},{"location":"pipelines/#text_segments","title":"text_segments","text":"<p>Can be called using:</p> <pre><code>doc.x('text_segments')\n# or\ndoc.text_segments\n</code></pre> <p>return type : </p> <p>supports pipelines : *,.docx,.epub,.html,.jpeg,.jpg,.markdown,.md,.odt,.pdf,.png,.rtf,.tif,.tiff,.yaml,dict,image,pandoc</p>"},{"location":"pipelines/#text_segment_model","title":"text_segment_model","text":"<p>Can be called using:</p> <pre><code>doc.x('text_segment_model')\n# or\ndoc.text_segment_model\n</code></pre> <p>return type : </p> <p>supports pipelines : *,.docx,.epub,.html,.jpeg,.jpg,.markdown,.md,.odt,.pdf,.png,.rtf,.tif,.tiff,.yaml,dict,image,pandoc</p>"},{"location":"pipelines/#text_segment_only_tokenizer","title":"text_segment_only_tokenizer","text":"<p>Can be called using:</p> <pre><code>doc.x('text_segment_only_tokenizer')\n# or\ndoc.text_segment_only_tokenizer\n</code></pre> <p>return type : </p> <p>supports pipelines : *,.docx,.epub,.html,.jpeg,.jpg,.markdown,.md,.odt,.pdf,.png,.rtf,.tif,.tiff,.yaml,dict,image,pandoc</p>"},{"location":"pipelines/#text_segment_vectors","title":"text_segment_vectors","text":"<p>Can be called using:</p> <pre><code>doc.x('text_segment_vectors')\n# or\ndoc.text_segment_vectors\n</code></pre> <p>return type : </p> <p>supports pipelines : *,.docx,.epub,.html,.jpeg,.jpg,.markdown,.md,.odt,.pdf,.png,.rtf,.tif,.tiff,.yaml,dict,image,pandoc</p>"},{"location":"pipelines/#noun_index","title":"noun_index","text":"<p>Can be called using:</p> <pre><code>doc.x('noun_index')\n# or\ndoc.noun_index\n</code></pre> <p>return type : </p> <p>supports pipelines : *,.docx,.epub,.html,.jpeg,.jpg,.markdown,.md,.odt,.pdf,.png,.rtf,.tif,.tiff,.yaml,dict,image,pandoc</p>"},{"location":"pipelines/#vectorizer","title":"vectorizer","text":"<p>Can be called using:</p> <pre><code>doc.x('vectorizer')\n# or\ndoc.vectorizer\n</code></pre> <p>return type : </p> <p>supports pipelines : *,.docx,.epub,.html,.jpeg,.jpg,.markdown,.md,.odt,.pdf,.png,.rtf,.tif,.tiff,.yaml,dict,image,pandoc</p>"},{"location":"pipelines/#noun_query","title":"noun_query","text":"<p>Can be called using:</p> <pre><code>doc.x('noun_query')\n# or\ndoc.noun_query\n</code></pre> <p>return type : typing.Callable</p> <p>supports pipelines : *,.docx,.epub,.html,.jpeg,.jpg,.markdown,.md,.odt,.pdf,.png,.rtf,.tif,.tiff,.yaml,dict,image,pandoc</p>"},{"location":"pipelines/#noun_graph","title":"noun_graph","text":"<p>Can be called using:</p> <pre><code>doc.x('noun_graph')\n# or\ndoc.noun_graph\n</code></pre> <p>return type : </p> <p>supports pipelines : *,.docx,.epub,.html,.jpeg,.jpg,.markdown,.md,.odt,.pdf,.png,.rtf,.tif,.tiff,.yaml,dict,image,pandoc</p>"},{"location":"pipelines/#top_k_text_rank_keywords","title":"top_k_text_rank_keywords","text":"<p>Can be called using:</p> <pre><code>doc.x('top_k_text_rank_keywords')\n# or\ndoc.top_k_text_rank_keywords\n</code></pre> <p>return type : </p> <p>supports pipelines : *,.docx,.epub,.html,.jpeg,.jpg,.markdown,.md,.odt,.pdf,.png,.rtf,.tif,.tiff,.yaml,dict,image,pandoc</p>"},{"location":"pipelines/#textrank_keywords","title":"textrank_keywords","text":"<p>Can be called using:</p> <pre><code>doc.x('textrank_keywords')\n# or\ndoc.textrank_keywords\n</code></pre> <p>return type : set[str]</p> <p>supports pipelines : *,.docx,.epub,.html,.jpeg,.jpg,.markdown,.md,.odt,.pdf,.png,.rtf,.tif,.tiff,.yaml,dict,image,pandoc</p>"},{"location":"pipelines/#keywords","title":"keywords","text":"<p>Can be called using:</p> <pre><code>doc.x('keywords')\n# or\ndoc.keywords\n</code></pre> <p>return type : </p> <p>supports pipelines : *,.docx,.epub,.html,.jpeg,.jpg,.markdown,.md,.odt,.pdf,.png,.rtf,.tif,.tiff,.yaml,dict,image,pandoc</p>"},{"location":"pipelines/#sent_index","title":"sent_index","text":"<p>Can be called using:</p> <pre><code>doc.x('sent_index')\n# or\ndoc.sent_index\n</code></pre> <p>return type : </p> <p>supports pipelines : *,.docx,.epub,.html,.jpeg,.jpg,.markdown,.md,.odt,.pdf,.png,.rtf,.tif,.tiff,.yaml,dict,image,pandoc</p>"},{"location":"pipelines/#sent_query","title":"sent_query","text":"<p>Can be called using:</p> <pre><code>doc.x('sent_query')\n# or\ndoc.sent_query\n</code></pre> <p>return type : typing.Callable</p> <p>supports pipelines : *,.docx,.epub,.html,.jpeg,.jpg,.markdown,.md,.odt,.pdf,.png,.rtf,.tif,.tiff,.yaml,dict,image,pandoc</p>"},{"location":"pipelines/#sent_graph","title":"sent_graph","text":"<p>Can be called using:</p> <pre><code>doc.x('sent_graph')\n# or\ndoc.sent_graph\n</code></pre> <p>return type : </p> <p>supports pipelines : *,.docx,.epub,.html,.jpeg,.jpg,.markdown,.md,.odt,.pdf,.png,.rtf,.tif,.tiff,.yaml,dict,image,pandoc</p>"},{"location":"pipelines/#top_k_text_rank_sentences","title":"top_k_text_rank_sentences","text":"<p>Can be called using:</p> <pre><code>doc.x('top_k_text_rank_sentences')\n# or\ndoc.top_k_text_rank_sentences\n</code></pre> <p>return type : </p> <p>supports pipelines : *,.docx,.epub,.html,.jpeg,.jpg,.markdown,.md,.odt,.pdf,.png,.rtf,.tif,.tiff,.yaml,dict,image,pandoc</p>"},{"location":"pipelines/#textrank_sents","title":"textrank_sents","text":"<p>Can be called using:</p> <pre><code>doc.x('textrank_sents')\n# or\ndoc.textrank_sents\n</code></pre> <p>return type : set[str]</p> <p>supports pipelines : *,.docx,.epub,.html,.jpeg,.jpg,.markdown,.md,.odt,.pdf,.png,.rtf,.tif,.tiff,.yaml,dict,image,pandoc</p>"},{"location":"pipelines/#summarizer_model","title":"summarizer_model","text":"<p>Can be called using:</p> <pre><code>doc.x('summarizer_model')\n# or\ndoc.summarizer_model\n</code></pre> <p>return type : </p> <p>supports pipelines : *,.docx,.epub,.html,.jpeg,.jpg,.markdown,.md,.odt,.pdf,.png,.rtf,.tif,.tiff,.yaml,dict,image,pandoc</p>"},{"location":"pipelines/#summarizer_token_overlap","title":"summarizer_token_overlap","text":"<p>Can be called using:</p> <pre><code>doc.x('summarizer_token_overlap')\n# or\ndoc.summarizer_token_overlap\n</code></pre> <p>return type : </p> <p>supports pipelines : *,.docx,.epub,.html,.jpeg,.jpg,.markdown,.md,.odt,.pdf,.png,.rtf,.tif,.tiff,.yaml,dict,image,pandoc</p>"},{"location":"pipelines/#summarizer_max_text_len","title":"summarizer_max_text_len","text":"<p>Can be called using:</p> <pre><code>doc.x('summarizer_max_text_len')\n# or\ndoc.summarizer_max_text_len\n</code></pre> <p>return type : </p> <p>supports pipelines : *,.docx,.epub,.html,.jpeg,.jpg,.markdown,.md,.odt,.pdf,.png,.rtf,.tif,.tiff,.yaml,dict,image,pandoc</p>"},{"location":"pipelines/#summary","title":"summary","text":"<p>Can be called using:</p> <pre><code>doc.x('summary')\n# or\ndoc.summary\n</code></pre> <p>return type : </p> <p>supports pipelines : *,.docx,.epub,.html,.jpeg,.jpg,.markdown,.md,.odt,.pdf,.png,.rtf,.tif,.tiff,.yaml,dict,image,pandoc</p>"},{"location":"pipelines/#qam_model_id","title":"qam_model_id","text":"<p>Can be called using:</p> <pre><code>doc.x('qam_model_id')\n# or\ndoc.qam_model_id\n</code></pre> <p>return type : </p> <p>supports pipelines : *,.docx,.epub,.html,.jpeg,.jpg,.markdown,.md,.odt,.pdf,.png,.rtf,.tif,.tiff,.yaml,dict,image,pandoc</p>"},{"location":"pipelines/#answers","title":"answers","text":"<p>Can be called using:</p> <pre><code>doc.x('answers')\n# or\ndoc.answers\n</code></pre> <p>return type : </p> <p>supports pipelines : *,.docx,.epub,.html,.jpeg,.jpg,.markdown,.md,.odt,.pdf,.png,.rtf,.tif,.tiff,.yaml,dict,image,pandoc</p>"},{"location":"pipelines/#openai_chat_model_id","title":"openai_chat_model_id","text":"<p>Can be called using:</p> <pre><code>doc.x('openai_chat_model_id')\n# or\ndoc.openai_chat_model_id\n</code></pre> <p>return type : </p> <p>supports pipelines : *,.docx,.epub,.html,.jpeg,.jpg,.markdown,.md,.odt,.pdf,.png,.rtf,.tif,.tiff,.yaml,dict,image,pandoc</p>"},{"location":"pipelines/#chat_answers","title":"chat_answers","text":"<p>Can be called using:</p> <pre><code>doc.x('chat_answers')\n# or\ndoc.chat_answers\n</code></pre> <p>return type : typing.Callable[[list[str], list[str] | str], list[str]]</p> <p>supports pipelines : *,.docx,.epub,.html,.jpeg,.jpg,.markdown,.md,.odt,.pdf,.png,.rtf,.tif,.tiff,.yaml,dict,image,pandoc</p>"},{"location":"pipelines/#page_set","title":"page_set","text":"<p>Can be called using:</p> <pre><code>doc.x('page_set')\n# or\ndoc.page_set\n</code></pre> <p>return type : </p> <p>supports pipelines : .jpeg,.jpg,.pdf,.png,.tif,.tiff,image</p>"},{"location":"pipelines/#pages_bbox","title":"pages_bbox","text":"<p>Can be called using:</p> <pre><code>doc.x('pages_bbox')\n# or\ndoc.pages_bbox\n</code></pre> <p>return type : </p> <p>supports pipelines : .jpeg,.jpg,.pdf,.png,.tif,.tiff,image</p>"},{"location":"pipelines/#elements","title":"elements","text":"<p>Can be called using:</p> <pre><code>doc.x('elements')\n# or\ndoc.elements\n</code></pre> <p>return type : </p> <p>supports pipelines : .jpeg,.jpg,.pdf,.png,.tif,.tiff,image</p>"},{"location":"pipelines/#meta","title":"meta","text":"<p>Can be called using:</p> <pre><code>doc.x('meta')\n# or\ndoc.meta\n</code></pre> <p>return type : </p> <p>supports pipelines : .jpeg,.jpg,.pdf,.png,.tif,.tiff,image</p>"},{"location":"pipelines/#line_elements","title":"line_elements","text":"<p>Can be called using:</p> <pre><code>doc.x('line_elements')\n# or\ndoc.line_elements\n</code></pre> <p>return type : </p> <p>supports pipelines : .jpeg,.jpg,.pdf,.png,.tif,.tiff,image</p>"},{"location":"pipelines/#graphic_elements","title":"graphic_elements","text":"<p>Can be called using:</p> <pre><code>doc.x('graphic_elements')\n# or\ndoc.graphic_elements\n</code></pre> <p>return type : </p> <p>supports pipelines : .jpeg,.jpg,.pdf,.png,.tif,.tiff,image</p>"},{"location":"pipelines/#lists","title":"lists","text":"<p>Can be called using:</p> <pre><code>doc.x('lists')\n# or\ndoc.lists\n</code></pre> <p>return type : str | list[str] | list[pandas.core.frame.DataFrame]</p> <p>supports pipelines : .docx,.epub,.jpeg,.jpg,.markdown,.md,.odt,.pdf,.png,.rtf,.tif,.tiff,image,pandoc</p>"},{"location":"pipelines/#table_box_levels","title":"table_box_levels","text":"<p>Can be called using:</p> <pre><code>doc.x('table_box_levels')\n# or\ndoc.table_box_levels\n</code></pre> <p>return type : </p> <p>supports pipelines : .jpeg,.jpg,.pdf,.png,.tif,.tiff,image</p>"},{"location":"pipelines/#table_candidates","title":"table_candidates","text":"<p>Can be called using:</p> <pre><code>doc.x('table_candidates')\n# or\ndoc.table_candidates\n</code></pre> <p>return type : </p> <p>supports pipelines : .jpeg,.jpg,.pdf,.png,.tif,.tiff,image</p>"},{"location":"pipelines/#table_df0","title":"table_df0","text":"<p>Can be called using:</p> <pre><code>doc.x('table_df0')\n# or\ndoc.table_df0\n</code></pre> <p>return type : </p> <p>supports pipelines : .jpeg,.jpg,.pdf,.png,.tif,.tiff,image</p>"},{"location":"pipelines/#titles","title":"titles","text":"<p>Can be called using:</p> <pre><code>doc.x('titles')\n# or\ndoc.titles\n</code></pre> <p>return type : </p> <p>supports pipelines : .html,.jpeg,.jpg,.pdf,.png,.tif,.tiff,image</p>"},{"location":"pipelines/#side_titles","title":"side_titles","text":"<p>Can be called using:</p> <pre><code>doc.x('side_titles')\n# or\ndoc.side_titles\n</code></pre> <p>return type : </p> <p>supports pipelines : .jpeg,.jpg,.pdf,.png,.tif,.tiff,image</p>"},{"location":"pipelines/#html_keywords_str","title":"html_keywords_str","text":"<p>Can be called using:</p> <pre><code>doc.x('html_keywords_str')\n# or\ndoc.html_keywords_str\n</code></pre> <p>return type : </p> <p>supports pipelines : .html</p>"},{"location":"pipelines/#main_content_clean_html","title":"main_content_clean_html","text":"<p>Can be called using:</p> <pre><code>doc.x('main_content_clean_html')\n# or\ndoc.main_content_clean_html\n</code></pre> <p>return type : </p> <p>supports pipelines : .html</p>"},{"location":"pipelines/#goose_article","title":"goose_article","text":"<p>Can be called using:</p> <pre><code>doc.x('goose_article')\n# or\ndoc.goose_article\n</code></pre> <p>return type : </p> <p>supports pipelines : .html</p>"},{"location":"pipelines/#main_content","title":"main_content","text":"<p>Can be called using:</p> <pre><code>doc.x('main_content')\n# or\ndoc.main_content\n</code></pre> <p>return type : </p> <p>supports pipelines : .html</p>"},{"location":"pipelines/#schemadata","title":"schemadata","text":"<p>Can be called using:</p> <pre><code>doc.x('schemadata')\n# or\ndoc.schemadata\n</code></pre> <p>return type : </p> <p>supports pipelines : .html</p>"},{"location":"pipelines/#final_urls","title":"final_urls","text":"<p>Can be called using:</p> <pre><code>doc.x('final_urls')\n# or\ndoc.final_urls\n</code></pre> <p>return type : </p> <p>supports pipelines : .html</p>"},{"location":"pipelines/#pdf_links","title":"pdf_links","text":"<p>Can be called using:</p> <pre><code>doc.x('pdf_links')\n# or\ndoc.pdf_links\n</code></pre> <p>return type : </p> <p>supports pipelines : .html</p>"},{"location":"pipelines/#title","title":"title","text":"<p>Can be called using:</p> <pre><code>doc.x('title')\n# or\ndoc.title\n</code></pre> <p>return type : </p> <p>supports pipelines : .html</p>"},{"location":"pipelines/#short_title","title":"short_title","text":"<p>Can be called using:</p> <pre><code>doc.x('short_title')\n# or\ndoc.short_title\n</code></pre> <p>return type : </p> <p>supports pipelines : .html</p>"},{"location":"pipelines/#urls","title":"urls","text":"<p>Can be called using:</p> <pre><code>doc.x('urls')\n# or\ndoc.urls\n</code></pre> <p>return type : </p> <p>supports pipelines : .html</p>"},{"location":"pipelines/#main_image","title":"main_image","text":"<p>Can be called using:</p> <pre><code>doc.x('main_image')\n# or\ndoc.main_image\n</code></pre> <p>return type : </p> <p>supports pipelines : .html</p>"},{"location":"pipelines/#html_keywords","title":"html_keywords","text":"<p>Can be called using:</p> <pre><code>doc.x('html_keywords')\n# or\ndoc.html_keywords\n</code></pre> <p>return type : </p> <p>supports pipelines : .html</p>"},{"location":"pipelines/#pandoc_document","title":"pandoc_document","text":"<p>Can be called using:</p> <pre><code>doc.x('pandoc_document')\n# or\ndoc.pandoc_document\n</code></pre> <p>return type : Pandoc(Meta, [Block])</p> <p>supports pipelines : .docx,.epub,.markdown,.md,.odt,.rtf,pandoc</p>"},{"location":"pipelines/#output_format","title":"output_format","text":"<p>Can be called using:</p> <pre><code>doc.x('output_format')\n# or\ndoc.output_format\n</code></pre> <p>return type : </p> <p>supports pipelines : .docx,.epub,.markdown,.md,.odt,.rtf,pandoc</p>"},{"location":"pipelines/#clean_format","title":"clean_format","text":"<p>Can be called using:</p> <pre><code>doc.x('clean_format')\n# or\ndoc.clean_format\n</code></pre> <p>return type : </p> <p>supports pipelines : .docx,.epub,.markdown,.md,.odt,.rtf,pandoc</p>"},{"location":"pipelines/#pandoc_blocks","title":"pandoc_blocks","text":"<p>Can be called using:</p> <pre><code>doc.x('pandoc_blocks')\n# or\ndoc.pandoc_blocks\n</code></pre> <p>return type : list['pandoc.types.Block']</p> <p>supports pipelines : .docx,.epub,.markdown,.md,.odt,.rtf,pandoc</p>"},{"location":"pipelines/#headers","title":"headers","text":"<p>Can be called using:</p> <pre><code>doc.x('headers')\n# or\ndoc.headers\n</code></pre> <p>return type : str | list[str] | list[pandas.core.frame.DataFrame]</p> <p>supports pipelines : .docx,.epub,.markdown,.md,.odt,.rtf,pandoc</p>"},{"location":"pipelines/#ocr_lang","title":"ocr_lang","text":"<p>Can be called using:</p> <pre><code>doc.x('ocr_lang')\n# or\ndoc.ocr_lang\n</code></pre> <p>return type : </p> <p>supports pipelines : .jpeg,.jpg,.png,.tif,.tiff,image</p>"},{"location":"pipelines/#ocr_on","title":"ocr_on","text":"<p>Can be called using:</p> <pre><code>doc.x('ocr_on')\n# or\ndoc.ocr_on\n</code></pre> <p>return type : </p> <p>supports pipelines : .jpeg,.jpg,.png,.tif,.tiff,image</p>"},{"location":"pipelines/#ocr_pdf_file","title":"ocr_pdf_file","text":"<p>Can be called using:</p> <pre><code>doc.x('ocr_pdf_file')\n# or\ndoc.ocr_pdf_file\n</code></pre> <p>return type : </p> <p>supports pipelines : .jpeg,.jpg,.png,.tif,.tiff,image</p>"},{"location":"pipelines/#data","title":"data","text":"<p>Can be called using:</p> <pre><code>doc.x('data')\n# or\ndoc.data\n</code></pre> <p>return type : </p> <p>supports pipelines : .yaml,dict</p>"},{"location":"readme_cp/","title":"\ud83d\ude80 pydoxtools (Python Library) \ud83d\ude80","text":"<p>(WIP) Documentation</p> <p>Pydoxtools is a library that provides a sophisticated interface for reading and writing documents, designed to work with AI models such as GPT, Alpaca, and Huggingface. It offers functionalities such as:</p> <ul> <li>Pipeline management on a new level</li> <li>Integration with AI (LLMs and more) models</li> <li>(PDF) table extraction without configuration!</li> <li>Document analysis and question-answering</li> <li>Support for most of todays document formats</li> <li>Vector index Creation</li> <li>Entity, address identification and more</li> <li>List and keyword extraction</li> <li>Data normalization, translation, and cleaning</li> </ul> <p>The library allows for the creation of complex extraction pipelines for batch-processing of documents by defining them as a lazily-executed graph.</p>"},{"location":"readme_cp/#installation","title":"Installation","text":""},{"location":"readme_cp/#installing-from-github","title":"Installing from GitHub","text":"<p>While pydoxtools can already be installed through pip, due to the many updates coming in right now, it is currently recommended to use the latest version from GitHub as follows:</p> <pre><code>pip install -U \"pydoxtools[etl,inference] @ git+https://github.com/xyntopia/pydoxtools.git\"\n</code></pre>"},{"location":"readme_cp/#installing-from-pypi","title":"Installing from PyPI","text":"<p>Pydoxtools can also be installed through pip, which will become the recommended method once it becomes more stable:</p> <pre><code>pip install -U pydoxtools[etl,inference]\n</code></pre> <p>For loading additional file formats (docx, odt, epub) and images, check out the additional &gt; Installation Options &lt;.</p>"},{"location":"readme_cp/#teaser","title":"Teaser","text":"<p>Experience a new level of convenience and efficiency in handling documents with Pydoxtools, and reimagine your data extraction pipelines! \ud83c\udfa9\u2728\ud83d\udcc4.</p> <p>In this teaser, we'll demonstrate how to create a document, extract tables, and ask questions using AI models:</p> <pre><code>import pydoxtools as pdx\n\n# Create a document from various sources: file, string, bytestring, file-like object, or URL\ndoc = pdx.Document(\n    \"https://www.raspberrypi.org/app/uploads/2012/12/quick-start-guide-v1.1.pdf\",\n    document_type=\".pdf\"\n)\n\n# List available extraction functions\nprint(doc.x_funcs)\n\n# Extract tables from the PDF as a pandas DataFrame\nprint(doc.tables_df)\n\n# Ask a question about the documents using a local Q&amp;A model\nprint(doc.answers([\"how much ram does it have?\"]))\n# Or only ask about the documents tables (or any other extracted information):\nprint(doc.answers([\"how much ram does it have?\"], \"tables\"))\n\n# To use ChatGPT for question-answering, set the API key as an environment variable:\n# OPENAI_API_KEY=\"sk ....\"\n# Then, ask questions about the document using ChatGPT\nprint(doc.chat_answers([\"What is the target group of this document?\"])[0].content)\nprint(doc.chat_answers([\"Answer if a 5-year old would be able to follow these instructions?\"])[0].content)\n</code></pre> <p>With Pydoxtools, you can easily access and process your documents, perform various extractions, and utilize AI models for more advanced analysis.</p>"},{"location":"readme_cp/#some-features-in-more-detail","title":"Some Features in more Detail","text":""},{"location":"readme_cp/#large-pipelines","title":"Large pipelines","text":"<p>Pydoxtools main feature is the ability to mix LLMs and other AI models in large, composable and customizable pipelines. As a teaser, check out this pipeline for *.png images from the repository including OCR, keyword extraction, vectorization and more. In this pipeline:</p> <ul> <li>Every node in an ellipse can be called as an attribute of the document-analysis pipeline.</li> <li>Every execution-path is lazily executed throughout the entire graph.</li> <li>Every node is cached by default (can be turned off).</li> <li>Every piece of this pipeline can be replaced by a customized version.</li> </ul> <p></p> <p>Pipelines can be mixed, partially overwritten and extended which gives you a lot of possibilities to extend and adapt the functionality for your specific use-case.</p> <p>Find out more about it in the documentation</p>"},{"location":"readme_cp/#pipeline-configuration","title":"Pipeline Configuration","text":"<p>Pipelines can be configured. For example the local model used for question answering can be selected like this:</p> <pre><code>doc = Document(fobj=\"./data/PFR-PR23_BAT-110__V1.00_.pdf\"))\n        .config(qam_model_id='bert-large-uncased-whole-word-masking-finetuned-squad')\n</code></pre> <p>where \"qam_model_id\" can be any model from huggingface for question answering.</p> <pre><code>TODO: document how to configure a pipeline\n</code></pre>"},{"location":"readme_cp/#pdf-table-extraction-algorithms","title":"PDF Table Extraction Algorithms","text":"<p>The library features its own sophisticated Table extraction algorithm which is benchmarked against a large pdf table dataset. In contrast to most other table extraction frameworks out there it does not require:</p> <ul> <li>extensive configuration</li> <li>no expensive deep neural networks which need a GPU</li> </ul> <p>This makes it possible to run analysis on PDF files with pydoxtools on CPU with very limited resources!</p>"},{"location":"readme_cp/#todo-describe-more-of-the-features-here","title":"TODO: Describe more of the features here...","text":""},{"location":"readme_cp/#use-cases","title":"Use Cases","text":"<ul> <li>analyze documents using any model from huggingface...</li> <li>analyze documents using a custom model</li> <li>download a pdf from URL</li> <li>generate document keywords</li> <li>extract tables</li> <li>download document from URL \"manually\" and then feed to document</li> <li>extract addresses</li> <li>extract addresses and use this information for the qam</li> <li>ingest documents into a vector db</li> </ul>"},{"location":"readme_cp/#installation-options","title":"Installation Options","text":""},{"location":"readme_cp/#supporting-docx-odt-epub","title":"Supporting .docx, .odt, *.epub","text":"<p>In order to be able to load docx, odt and rtf files, you have to install pandoc. Right now, the python pandoc library does not work with pandoc version &gt; 3.0.0. It is therefore recommended to install a version from here for your OS:</p> <p>https://github.com/jgm/pandoc/releases/tag/2.19.2</p>"},{"location":"readme_cp/#image-ocr-support","title":"Image OCR Support","text":"<p>Pydoxtools can automatically analyze images as well, makin use of OCR. In order to be able to use this, install tesseract on your system:</p> <p>Under linux this looks like the following:</p> <pre><code>apt-get update &amp;&amp; tesseract-ocr\n# install tesseract languages \n# Display a list of all Tesseract language packs:\n#   apt-cache search tesseract-ocr\n# install all languages:\n# sudo apt install tesseract-ocr-*\n# install only german, french, english, spanish language packs\n# sudo apt install tesseract-ocr-deu tesseract-ocr-fra tesseract-ocr-eng tesseract-ocr-spa\n</code></pre>"},{"location":"readme_cp/#development","title":"Development","text":"<p>--&gt; see </p>"},{"location":"readme_cp/#license","title":"License","text":"<p>This project is licensed under the terms of MIT license.</p> <p>You can check the compatibility using the following tool in a venv environment in a production setting:</p> <pre><code>pip install pip-licenses\npip-licenses | grep -Ev 'MIT License|BSD License|Apache Software License|Python Software Foundation License|Apache 2.0|MIT|Apache License 2.0|hnswlib|Pillow|new BSD|BSD'\n</code></pre>"},{"location":"readme_cp/#dependencies","title":"Dependencies","text":"<p>Here is a list of Libraries, that this project is based on:</p> <p>list</p>"},{"location":"reference/","title":"Reference","text":""},{"location":"reference/#pydoxtools.document.Document","title":"<code>Document</code>","text":"<p>         Bases: <code>Pipeline</code></p> <p>Basic document pipeline class to analyze documents from all kinds of formats.</p> <p>A list and documentation of all document analysis related functions can be found -&gt;here&lt;-.</p> <p>The Document class is designed for information extraction from documents. It inherits from the pydoxtools.document_base.Pipeline class and uses a predefined extraction pipeline focused on document processing tasks. To load a document, create an instance of the Document class with a file path, a file object, a string, a URL or give it some data directly as a dict:</p> <pre><code>from pydoxtools import Document\ndoc = Document(fobj=Path('./data/demo.docx'))\n</code></pre> <p>Extracted data can be accessed by calling the <code>x</code> method with the specified output in the pipeline:</p> <pre><code>doc.x(\"addresses\")\ndoc.x(\"entities\")\ndoc.x(\"full_text\")\n# etc...\n</code></pre> <p>Most members can also be called as normal class attributes for easier readability:</p> <pre><code>doc.addresses\n</code></pre> <p>Additionally, it is possible to get the data directly in dict, yaml or json form:</p> <pre><code>doc.property_dict(\"addresses\",\"filename\",\"keywords\")\ndoc.yaml(\"addresses\",\"filename\",\"keywords\")\ndoc.json(\"addresses\",\"filename\",\"keywords\")\n</code></pre> <p>To retrieve a list of all available extraction data methods, call the <code>x_funcs()</code> method:</p> <pre><code>doc.x_funcs()\n</code></pre>"},{"location":"reference/#pydoxtools.document.Document--customizing-the-pipeline","title":"Customizing the Pipeline:","text":"<p>The extraction pipeline can be partially overwritten or completely replaced to customize the document processing. To customize the pipeline, it's recommended to use the basic document pipeline defined in <code>pydoxtools.Document</code> as a starting point and only overwrite parts as needed.</p> <p>Inherited classes can override any part of the graph. To exchange, override, extend or introduce extraction pipelines for specific file types (including the generic one: \"\"), such as .html, .pdf, .txt, etc., follow the example below.</p> <p>TODO: provide more information on how to customize the pipeline and override the graph.</p>"},{"location":"reference/#pydoxtools.document.Document--examples","title":"Examples","text":"<p>The following is an example extension pipeline for an OCR extractor that converts images into text and supports file types: \".png\", \".jpeg\", \".jpg\", \".tif\", \".tiff\":</p> <pre><code>\"image\": [\n        OCRExtractor()\n        .pipe(file=\"raw_content\")\n        .out(\"ocr_pdf_file\")\n        .cache(),\n    ],\n\".png\": [\"image\", \".pdf\"],\n\".jpeg\": [\"image\", \".pdf\"],\n\".jpg\": [\"image\", \".pdf\"],\n\".tif\": [\"image\", \".pdf\"],\n\".tiff\": [\"image\", \".pdf\"],\n\"*\": [...]\n</code></pre> <p>Each function (or node) in the extraction pipeline connects to other nodes in the pipeline through the \"pipe\" command. Arguments can be overwritten by a new pipeline in inherited documents or document types higher up in the hierarchy. The argument precedence is as follows:</p> <pre><code>python-class-member &lt; extractor-graph-function &lt; configuration\n</code></pre> <p>When creating a new pipeline for documentation purposes, use a function or class for complex operations and include the documentation there. Lambda functions should not be used in this case.</p>"},{"location":"reference/#pydoxtools.document.Document.document_type","title":"<code>document_type</code>  <code>cached</code> <code>property</code>","text":"<p>detect doc type based on file-ending TODO add a doc-type extractor using for example python-magic</p>"},{"location":"reference/#pydoxtools.document.Document.filename","title":"<code>filename: str | None</code>  <code>cached</code> <code>property</code>","text":"<p>TODO: move this into document pipeline</p>"},{"location":"reference/#pydoxtools.document.Document.__init__","title":"<code>__init__(fobj=None, source=None, page_numbers=None, max_pages=None, mime_type=None, filename=None, document_type=None)</code>","text":"<p>Initialize a Document instance.</p> <p>Parameters:</p> Name Type Description Default <code>fobj</code> <code>str | bytes | Path | IO</code> <p>The file object to load. Depending on the type of object passed: - If a string or bytes object: the object itself is the document. - If a string representing a URL: the document will be loaded from the URL. - If a pathlib.Path object: load the document from the path. - If a file object: load the document from the file object (e.g., bytestream).</p> <code>None</code> <code>source</code> <code>str | Path</code> <p>The source of the extracted data (e.g., URL, 'pdfupload', parent-URL, or a path).</p> <code>None</code> <code>page_numbers</code> <code>list[int]</code> <p>A list of specific pages to extract from the document (e.g., in a PDF).</p> <code>None</code> <code>max_pages</code> <code>int</code> <p>The maximum number of pages to extract to protect resources.</p> <code>None</code> <code>mime_type</code> <code>str</code> <p>The MIME type of the document, if available.</p> <code>None</code> <code>filename</code> <code>str</code> <p>The filename of the document, which can sometimes help in determining its purpose.</p> <code>None</code> <code>document_type</code> <code>str</code> <p>The document type to directly specify the extraction logic to be used.</p> <code>None</code>"},{"location":"reference/#pydoxtools.document.DocumentSet","title":"<code>DocumentSet</code>","text":"<p>         Bases: <code>Pipeline</code></p> <p>This class is WIP use with caution</p> <p>This class loads an entire set of documents and processes it using a pipeline.</p> <p>This class is still experimental. Expect more documentation in the near future.</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline","title":"<code>Pipeline</code>","text":"<p>Base class for all document classes in pydoxtools, defining a common pipeline interface and establishing a basic pipeline schema that derived classes can override.</p> <p>The MetaPipelineClassConfiguration acts as a compiler to resolve the pipeline hierarchy, allowing pipelines to inherit, mix, extend, or partially overwrite each other. Each key in the _pipelines dictionary represents a different pipeline version.</p> <p>The pydoxtools.Document class leverages this functionality to build separate pipelines for different file types, as the information processing requirements differ significantly between file types.</p> <p>Attributes:</p> Name Type Description <code>_operators</code> <code>dict[str, list[pydoxtools.document_base.Operator]]</code> <p>Stores the definition of the pipeline graph, a collection of connected operators/functions that process data from a document.</p> <code>_pipelines</code> <code>dict[str, dict[str, pydoxtools.document_base.Operator]]</code> <p>Provides access to all operator functions by their \"out-key\" which was defined in _operators.</p> Todo <ul> <li>Use pandera (https://github.com/unionai-oss/pandera) to validate dataframes   exchanged between extractors &amp; loaders   (https://pandera.readthedocs.io/en/stable/pydantic_integration.html)</li> </ul>"},{"location":"reference/#pydoxtools.document_base.Pipeline.configuration","title":"<code>configuration: dict[str, Any]</code>  <code>property</code>","text":"<p>Gets all configuration objects of the pipeline and merges them into a single dictionary.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict[str, Any]</code> <p>A dictionary containing the merged configuration objects of the pipeline, with keys as   the configuration names and values as the configuration objects.</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.pipeline_chooser","title":"<code>pipeline_chooser: str</code>  <code>cached</code> <code>property</code>","text":"<p>Must be implemented by derived classes to decide which pipeline they should use.</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.uuid","title":"<code>uuid</code>  <code>cached</code> <code>property</code>","text":"<p>Retrieves a universally unique identifier (UUID) for the instance.</p> <p>This method generates a new UUID for the instance using Python's <code>uuid.uuid4()</code> function. The UUID is then cached as a property, ensuring that the same UUID is returned for subsequent accesses.</p> <p>Returns:</p> Type Description <p>uuid.UUID: A unique identifier for the instance.</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.x_funcs","title":"<code>x_funcs: dict[str, Operator]</code>  <code>cached</code> <code>property</code>","text":"<p>get all operators/pipeline nodes and their property names for this specific file type/pipeline</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.__getattr__","title":"<code>__getattr__(extract_name)</code>","text":"<p>Retrieves an extractor result by directly accessing it as an attribute.</p> <p>This method is automatically called for attribute names that aren't defined on class level, allowing for a convenient way to access pipeline operator outputs without needing to call the 'x' method.</p> Example <p>document.addresses instead of document.x('addresses')</p> <p>Parameters:</p> Name Type Description Default <code>extract_name</code> <code>str</code> <p>The name of the extractor result to be accessed.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The result of the extractor after processing the document.</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.__init__","title":"<code>__init__()</code>","text":"<p>Initializes the Pipeline instance with cache-related attributes.</p> <p>Attributes:</p> Name Type Description <code>_cache_hits</code> <code>int</code> <p>Number of cache hits during pipeline execution.</p> <code>_x_func_cache</code> <code>dict[pydoxtools.document_base.Operator, dict[str, Any]]</code> <p>Cache for operator functions to store intermediate results.</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.__repr__","title":"<code>__repr__()</code>","text":"<p>Returns a string representation of the instance.</p> <p>This method provides a string representation of the instance, including the module and class names, as well as the file object or string and the source.</p> <p>Returns:</p> Name Type Description <code>str</code> <p>A string representation of the instance.</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.config","title":"<code>config(**settings)</code>","text":"<p>Set configuration parameters for a pipeline.</p> <p>This method loops through all \"operators.Configure\" instances in the pipeline and assigns the provided configuration settings to them.</p> <p>Parameters:</p> Name Type Description Default <code>**settings</code> <code>dict[str, Any]</code> <p>A dictionary of key-value pairs representing the configuration settings for the pipeline. Each key is a string representing the name of the configuration setting, and the value is the corresponding value to be set.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>self</code> <code>Pipeline</code> <p>A reference to the current pipeline instance, allowing for method chaining.</p> Example <p>pipeline = Pipeline() pipeline.config(param1=value1, param2=value2)</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.json","title":"<code>json(*args, **kwargs)</code>","text":"<p>Returns a dictionary that accumulates the properties given in args or with a mapping in *kwargs, and dumps the output as JSON.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>str</code> <p>A variable number of strings, each representing a property name.</p> <code>()</code> <code>**kwargs</code> <code>dict</code> <p>A dictionary mapping property names (values) to custom keys (keys) for the              returned dictionary.</p> <code>{}</code> Note <p>This function currently only supports properties that do not require any arguments, such as \"full_text\". Properties like \"answers\" that return a function requiring arguments cannot be used with this function.</p> <p>Returns:</p> Name Type Description <code>str</code> <p>A JSON-formatted string representing the accumulated properties and their values, using  either the property names or custom keys as specified in the input arguments.</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.non_interactive_pipeline","title":"<code>non_interactive_pipeline()</code>","text":"<p>return all non-interactive extractors/pipeline nodes</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.pipeline_docs","title":"<code>pipeline_docs()</code>  <code>classmethod</code>","text":"<p>Returns a formatted string containing the documentation for each pipeline operation in the class.</p> <p>This class method iterates through the pipeline operations, collects information about their output types and supported pipelines, and formats the documentation accordingly.</p> <p>Returns:</p> Name Type Description <code>str</code> <p>A formatted string containing the documentation for each pipeline operation, including  operation name, usage, return type, and supported pipelines.</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.pipeline_graph","title":"<code>pipeline_graph(image_path=None, document_logic_id='current')</code>","text":"<p>Generates a visualization of the defined pipelines and optionally saves it as an image.</p> <p>Parameters:</p> Name Type Description Default <code>image_path</code> <code>str | pathlib.Path</code> <p>File path for the generated image. If provided, the                                        generated graph will be saved as an image.</p> <code>None</code> <code>document_logic_id</code> <code>str</code> <p>The document logic ID for which the pipeline graph should                                be generated. Defaults to \"current\".</p> <code>'current'</code> <p>Returns:</p> Name Type Description <code>AGraph</code> <p>A PyGraphviz AGraph object representing the pipeline graph. This object can be     visualized or manipulated using PyGraphviz functions.</p> Notes <p>This method requires the NetworkX and PyGraphviz libraries to be installed.</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.pre_cache","title":"<code>pre_cache()</code>","text":"<p>Pre-caches the results of all extractors that have caching enabled.</p> <p>This method iterates through the defined extractors and calls each one with caching enabled, storing the results for faster access in future calls.</p> <p>Returns:</p> Name Type Description <code>self</code> <p>The instance of the class, allowing for method chaining.</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.property_dict","title":"<code>property_dict(*args, **kwargs)</code>","text":"<p>Returns a dictionary that accumulates the properties given in args or with a mapping in *kwargs.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>str</code> <p>A variable number of strings, each representing a property name.</p> <code>()</code> <code>**kwargs</code> <code>dict</code> <p>A dictionary mapping property names (values) to custom keys (keys) for the              returned dictionary.</p> <code>{}</code> Note <p>This function currently only supports properties that do not require any arguments, such as \"full_text\". Properties like \"answers\" that return a function requiring arguments cannot be used with this function.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary with the accumulated properties and their values, using either the   property names or custom keys as specified in the input arguments.</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.run_pipeline","title":"<code>run_pipeline()</code>","text":"<p>Runs all extractors defined in the pipeline for testing or pre-caching purposes.</p> <p>!!IMPORTANT!!!  This function should normally not be used as the pipeline is lazily executed anyway.</p> <p>This method iterates through the defined extractors and calls each one, ensuring that the extractor logic is functioning correctly and caching the results if required.</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.x","title":"<code>x(extract_name, *args, **kwargs)</code>","text":"<p>Calls an extractor from the defined pipeline and returns the result.</p> <p>Parameters:</p> Name Type Description Default <code>extract_name</code> <code>str</code> <p>The name of the extractor to be called.</p> required <code>*args</code> <p>Variable-length argument list to be passed to the extractor.</p> <code>()</code> <code>**kwargs</code> <p>Arbitrary keyword arguments to be passed to the extractor.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The result of the extractor after processing the document.</p> <p>Raises:</p> Type Description <code>operators.OperatorException</code> <p>If an error occurs while executing the extractor.</p> Notes <p>The extractor's parameters can be overridden using args and *kwargs.</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.x_all","title":"<code>x_all()</code>","text":"<p>Retrieves the results of all extractors defined in the pipeline.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the results of all extractors, with keys as the extractor   names and values as the corresponding results.</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.yaml","title":"<code>yaml(*args, **kwargs)</code>","text":"<p>Returns a dictionary that accumulates the properties given in args or with a mapping in *kwargs, and dumps the output as YAML.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>str</code> <p>A variable number of strings, each representing a property name.</p> <code>()</code> <code>**kwargs</code> <code>dict</code> <p>A dictionary mapping property names (values) to custom keys (keys) for the              returned dictionary.</p> <code>{}</code> Note <p>This function currently only supports properties that do not require any arguments, such as \"full_text\". Properties like \"answers\" that return a function requiring arguments cannot be used with this function.</p> <p>Returns:</p> Name Type Description <code>str</code> <p>A YAML-formatted string representing the accumulated properties and their values, using  either the property names or custom keys as specified in the input arguments.</p>"},{"location":"reference/#pydoxtools.operators","title":"<code>pydoxtools.operators</code>","text":"<p>This module simply gathers all operators from across the board to make them easier to access.</p>"}]}