{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Pydoxtools Documentation!","text":"<p>Readme</p>"},{"location":"#introduction","title":"Introduction","text":"<p>The main interface that pydoxtools uses are three classes:</p> <ul> <li>pydoxtools.Document</li> <li>pydoxtools.DocumentBag</li> <li>pydoxtools.Pipeline</li> </ul> <p>And a set of operators:</p> <ul> <li>pydoxtools.operators</li> </ul>"},{"location":"#analyzing-documents","title":"Analyzing Documents","text":"<p>Document &amp; DocumentSet are both using pydoxtools.Pipeline and predefine a complex pipeline to extract data from individual documents or a set of documents. A list of all the \"out-of-the-box\" featurs for each pipeline can be found in:</p> <p>-&gt; Pipelines</p> <p>In order for this to work, this library has the philosophy that Document &amp; DocumentSet should always automatically \"know\" how to organize information internally in a sensible way. Pydoxtools will try to keep memory &amp; CPU footprint as low as possible. This makes the library much easier to use in automated settings together with AI &amp; LLMs. This is why it is not possible to configure how documents are loaded with configuration parameters. If document data is required to be organized in a specific format, this can easily be achieved by chaining them together.</p>"},{"location":"#building-your-own-pipelines-with-llms-large-language-models-and-other-types-of-ai","title":"Building your own Pipelines with LLMs (Large Language Models) and other types of AI","text":"<p>the Pipeline class can be used to build complex, custom pipelines which have several out-of-the-box features which makes them easy to use in modern pipelines involving the use of a lot of AI tools:</p> <ul> <li>they can be mixed, extended, (partially) overwritten with other pipelines</li> <li>they can export/import their data (yaml, json, python-dict),</li> <li>they can be configured &amp; optimized</li> <li>they can convert their data into pydoxtools.Document and pydoxtools.DocumentSet</li> </ul> <p>Additionally, in order to develop a custom pipeline, pydoxtools has a large library of pydoxtools.operators which can be used to build your custom pipeline. It usually makes sense to use pydoxtools.Document or pydoxtools.DocumentSet as a base for a new pipeline and only replace small parts of them in order to get desired custom functionality.</p>"},{"location":"#visualization-of-the-pipelines","title":"Visualization of the Pipelines","text":"<p>The pipelines can be visualized which helps a lot when developing your own pipeline on top of a complex pipeline such as the document pipeline. The extraction logic for different file types can be visualized like this:</p> <pre><code>doc = Document(fobj=make_path_absolute(\"./data/demo.docx\"))\n# for the currently loaded file type:\ndoc.logic_graph(image_path=settings._PYDOXTOOLS_DIR / \"docs/images/document_logic_docx.svg\")\n# for the \ndoc.logic_graph(image_path=settings._PYDOXTOOLS_DIR / \"docs/images/document_logic_png.svg\", document_logic_id=\".png\")\n</code></pre> <p>This way we can generate the pipelines for different filetypes:</p> <ul> <li>docx</li> <li>png   (click on links to open the images!)</li> </ul> <p>Pipelines for every supported file type can be found here.</p> <p>This also works for custom pipelines!</p> <p>In order to learn more continue to: Reference</p>"},{"location":"DEVELOPMENT/","title":"Development &amp; Contribution","text":"<p>The graph model of the library makes it very easy to extend it with new functionality.</p> <ul> <li>the document can be used as a base-model and overwritten with changes</li> <li>the graph can be changed dynamically</li> <li>new functions can be very easily integrated</li> </ul>"},{"location":"DEVELOPMENT/#installation-from-other-branches","title":"Installation from other branches","text":"<p>In order to install pydoxtools from a development branch \"development_branch\" you can do this:</p> <p>pip install -U \"pydoxtools[etl,inference] @ git+https://github.com/xyntopia/pydoxtools.git@development_branch\"</p>"},{"location":"DEVELOPMENT/#pydoxtools-architecture","title":"Pydoxtools Architecture","text":"<p>--&gt; refer to \"document\"</p>"},{"location":"DEVELOPMENT/#contribution-guidelines","title":"Contribution Guidelines","text":""},{"location":"pipelines/","title":"Pipelines","text":"<p>This documents the output values of the nodes of each pipeline that  can be accessed through the pipeline interface.</p> <p>Pipeline visualizations for every supported file type can be found here.</p>"},{"location":"pipelines/#pydoxtoolsdocument","title":"pydoxtools.Document","text":""},{"location":"pipelines/#raw_content","title":"raw_content","text":"<p>Can be called using:</p> <pre><code>doc.x('raw_content')\n# or\ndoc.raw_content\n</code></pre> <p>return type : bytes | str</p> <p>supports pipelines : *,,application/epub+zip,application/pdf,application/vnd.oasis.opendocument.text,application/vnd.openxmlformats-officedocument.wordprocessingml.document,application/x-yaml,image,image/jpeg,image/png,image/tiff,pandoc,text/html,text/markdown,text/rtf"},{"location":"pipelines/#full_text","title":"full_text","text":"<p>Can be called using:</p> <pre><code>doc.x('full_text')\n# or\ndoc.full_text\n</code></pre> <p>return type :  <p>supports pipelines : *,,application/epub+zip,application/pdf,application/vnd.oasis.opendocument.text,application/vnd.openxmlformats-officedocument.wordprocessingml.document,application/x-yaml,image,image/jpeg,image/png,image/tiff,pandoc,text/html,text/markdown,text/rtf"},{"location":"pipelines/#clean_text","title":"clean_text","text":"<p>Can be called using:</p> <pre><code>doc.x('clean_text')\n# or\ndoc.clean_text\n</code></pre> <p>return type :  <p>supports pipelines : *,,application/epub+zip,application/pdf,application/vnd.oasis.opendocument.text,application/vnd.openxmlformats-officedocument.wordprocessingml.document,application/x-yaml,image,image/jpeg,image/png,image/tiff,pandoc,text/html,text/markdown,text/rtf"},{"location":"pipelines/#data","title":"data","text":"<p>Can be called using:</p> <pre><code>doc.x('data')\n# or\ndoc.data\n</code></pre> <p>return type : </p> <p>supports pipelines : *,,application/epub+zip,application/pdf,application/vnd.oasis.opendocument.text,application/vnd.openxmlformats-officedocument.wordprocessingml.document,application/x-yaml,image,image/jpeg,image/png,image/tiff,pandoc,text/html,text/markdown,text/rtf"},{"location":"pipelines/#text_box_elements","title":"text_box_elements","text":"<p>Can be called using:</p> <pre><code>doc.x('text_box_elements')\n# or\ndoc.text_box_elements\n</code></pre> <p>return type :  <p>supports pipelines : *,,application/epub+zip,application/pdf,application/vnd.oasis.opendocument.text,application/vnd.openxmlformats-officedocument.wordprocessingml.document,application/x-yaml,image,image/jpeg,image/png,image/tiff,pandoc,text/html,text/markdown,text/rtf"},{"location":"pipelines/#text_box_list","title":"text_box_list","text":"<p>Can be called using:</p> <pre><code>doc.x('text_box_list')\n# or\ndoc.text_box_list\n</code></pre> <p>return type : </p> <p>supports pipelines : *,,application/epub+zip,application/pdf,application/vnd.oasis.opendocument.text,application/vnd.openxmlformats-officedocument.wordprocessingml.document,application/x-yaml,image,image/jpeg,image/png,image/tiff,pandoc,text/html,text/markdown,text/rtf"},{"location":"pipelines/#tables_df","title":"tables_df","text":"<p>Can be called using:</p> <pre><code>doc.x('tables_df')\n# or\ndoc.tables_df\n</code></pre> <p>return type : str | list[str] | list[pandas.core.frame.DataFrame]</p> <p>supports pipelines : *,,application/epub+zip,application/pdf,application/vnd.oasis.opendocument.text,application/vnd.openxmlformats-officedocument.wordprocessingml.document,application/x-yaml,image,image/jpeg,image/png,image/tiff,pandoc,text/html,text/markdown,text/rtf"},{"location":"pipelines/#tables_dict","title":"tables_dict","text":"<p>Can be called using:</p> <pre><code>doc.x('tables_dict')\n# or\ndoc.tables_dict\n</code></pre> <p>return type : </p> <p>supports pipelines : *,,application/epub+zip,application/pdf,application/vnd.oasis.opendocument.text,application/vnd.openxmlformats-officedocument.wordprocessingml.document,application/x-yaml,image,image/jpeg,image/png,image/tiff,pandoc,text/html,text/markdown,text/rtf"},{"location":"pipelines/#tables","title":"tables","text":"<p>Can be called using:</p> <pre><code>doc.x('tables')\n# or\ndoc.tables\n</code></pre> <p>return type : </p> <p>supports pipelines : *,,application/epub+zip,application/pdf,application/vnd.oasis.opendocument.text,application/vnd.openxmlformats-officedocument.wordprocessingml.document,application/x-yaml,image,image/jpeg,image/png,image/tiff,pandoc,text/html,text/markdown,text/rtf"},{"location":"pipelines/#addresses","title":"addresses","text":"<p>Can be called using:</p> <pre><code>doc.x('addresses')\n# or\ndoc.addresses\n</code></pre> <p>return type : </p> <p>supports pipelines : *,,application/epub+zip,application/pdf,application/vnd.oasis.opendocument.text,application/vnd.openxmlformats-officedocument.wordprocessingml.document,application/x-yaml,image,image/jpeg,image/png,image/tiff,pandoc,text/html,text/markdown,text/rtf"},{"location":"pipelines/#num_pages","title":"num_pages","text":"<p>Can be called using:</p> <pre><code>doc.x('num_pages')\n# or\ndoc.num_pages\n</code></pre> <p>return type : </p> <p>supports pipelines : *,,application/epub+zip,application/pdf,application/vnd.oasis.opendocument.text,application/vnd.openxmlformats-officedocument.wordprocessingml.document,application/x-yaml,image,image/jpeg,image/png,image/tiff,pandoc,text/html,text/markdown,text/rtf"},{"location":"pipelines/#num_words","title":"num_words","text":"<p>Can be called using:</p> <pre><code>doc.x('num_words')\n# or\ndoc.num_words\n</code></pre> <p>return type : </p> <p>supports pipelines : *,,application/epub+zip,application/pdf,application/vnd.oasis.opendocument.text,application/vnd.openxmlformats-officedocument.wordprocessingml.document,application/x-yaml,image,image/jpeg,image/png,image/tiff,pandoc,text/html,text/markdown,text/rtf"},{"location":"pipelines/#num_sents","title":"num_sents","text":"<p>Can be called using:</p> <pre><code>doc.x('num_sents')\n# or\ndoc.num_sents\n</code></pre> <p>return type : </p> <p>supports pipelines : *,,application/epub+zip,application/pdf,application/vnd.oasis.opendocument.text,application/vnd.openxmlformats-officedocument.wordprocessingml.document,application/x-yaml,image,image/jpeg,image/png,image/tiff,pandoc,text/html,text/markdown,text/rtf"},{"location":"pipelines/#a_d_ratio","title":"a_d_ratio","text":"<p>Can be called using:</p> <pre><code>doc.x('a_d_ratio')\n# or\ndoc.a_d_ratio\n</code></pre> <p>return type : </p> <p>supports pipelines : *,,application/epub+zip,application/pdf,application/vnd.oasis.opendocument.text,application/vnd.openxmlformats-officedocument.wordprocessingml.document,application/x-yaml,image,image/jpeg,image/png,image/tiff,pandoc,text/html,text/markdown,text/rtf"},{"location":"pipelines/#language","title":"language","text":"<p>Can be called using:</p> <pre><code>doc.x('language')\n# or\ndoc.language\n</code></pre> <p>return type :  <p>supports pipelines : *,,application/epub+zip,application/pdf,application/vnd.oasis.opendocument.text,application/vnd.openxmlformats-officedocument.wordprocessingml.document,application/x-yaml,image,image/jpeg,image/png,image/tiff,pandoc,text/html,text/markdown,text/rtf"},{"location":"pipelines/#spacy_model_size","title":"spacy_model_size","text":"<p>Can be called using:</p> <pre><code>doc.x('spacy_model_size')\n# or\ndoc.spacy_model_size\n</code></pre> <p>return type : </p> <p>supports pipelines : *,,application/epub+zip,application/pdf,application/vnd.oasis.opendocument.text,application/vnd.openxmlformats-officedocument.wordprocessingml.document,application/x-yaml,image,image/jpeg,image/png,image/tiff,pandoc,text/html,text/markdown,text/rtf"},{"location":"pipelines/#spacy_model","title":"spacy_model","text":"<p>Can be called using:</p> <pre><code>doc.x('spacy_model')\n# or\ndoc.spacy_model\n</code></pre> <p>return type : </p> <p>supports pipelines : *,,application/epub+zip,application/pdf,application/vnd.oasis.opendocument.text,application/vnd.openxmlformats-officedocument.wordprocessingml.document,application/x-yaml,image,image/jpeg,image/png,image/tiff,pandoc,text/html,text/markdown,text/rtf"},{"location":"pipelines/#spacy_doc","title":"spacy_doc","text":"<p>Can be called using:</p> <pre><code>doc.x('spacy_doc')\n# or\ndoc.spacy_doc\n</code></pre> <p>return type : dict[str, typing.Any]</p> <p>supports pipelines : *,,application/epub+zip,application/pdf,application/vnd.oasis.opendocument.text,application/vnd.openxmlformats-officedocument.wordprocessingml.document,application/x-yaml,image,image/jpeg,image/png,image/tiff,pandoc,text/html,text/markdown,text/rtf"},{"location":"pipelines/#spacy_nlp","title":"spacy_nlp","text":"<p>Can be called using:</p> <pre><code>doc.x('spacy_nlp')\n# or\ndoc.spacy_nlp\n</code></pre> <p>return type : dict[str, typing.Any]</p> <p>supports pipelines : *,,application/epub+zip,application/pdf,application/vnd.oasis.opendocument.text,application/vnd.openxmlformats-officedocument.wordprocessingml.document,application/x-yaml,image,image/jpeg,image/png,image/tiff,pandoc,text/html,text/markdown,text/rtf"},{"location":"pipelines/#spacy_vectors","title":"spacy_vectors","text":"<p>Can be called using:</p> <pre><code>doc.x('spacy_vectors')\n# or\ndoc.spacy_vectors\n</code></pre> <p>return type : </p> <p>supports pipelines : *,,application/epub+zip,application/pdf,application/vnd.oasis.opendocument.text,application/vnd.openxmlformats-officedocument.wordprocessingml.document,application/x-yaml,image,image/jpeg,image/png,image/tiff,pandoc,text/html,text/markdown,text/rtf"},{"location":"pipelines/#spacy_embeddings","title":"spacy_embeddings","text":"<p>Can be called using:</p> <pre><code>doc.x('spacy_embeddings')\n# or\ndoc.spacy_embeddings\n</code></pre> <p>return type : </p> <p>supports pipelines : *,,application/epub+zip,application/pdf,application/vnd.oasis.opendocument.text,application/vnd.openxmlformats-officedocument.wordprocessingml.document,application/x-yaml,image,image/jpeg,image/png,image/tiff,pandoc,text/html,text/markdown,text/rtf"},{"location":"pipelines/#spacy_sents","title":"spacy_sents","text":"<p>Can be called using:</p> <pre><code>doc.x('spacy_sents')\n# or\ndoc.spacy_sents\n</code></pre> <p>return type : </p> <p>supports pipelines : *,,application/epub+zip,application/pdf,application/vnd.oasis.opendocument.text,application/vnd.openxmlformats-officedocument.wordprocessingml.document,application/x-yaml,image,image/jpeg,image/png,image/tiff,pandoc,text/html,text/markdown,text/rtf"},{"location":"pipelines/#spacy_noun_chunks","title":"spacy_noun_chunks","text":"<p>Can be called using:</p> <pre><code>doc.x('spacy_noun_chunks')\n# or\ndoc.spacy_noun_chunks\n</code></pre> <p>return type : </p> <p>supports pipelines : *,,application/epub+zip,application/pdf,application/vnd.oasis.opendocument.text,application/vnd.openxmlformats-officedocument.wordprocessingml.document,application/x-yaml,image,image/jpeg,image/png,image/tiff,pandoc,text/html,text/markdown,text/rtf"},{"location":"pipelines/#entities","title":"entities","text":"<p>Can be called using:</p> <pre><code>doc.x('entities')\n# or\ndoc.entities\n</code></pre> <p>return type : dict[str, list[str]]</p> <p>supports pipelines : *,,application/epub+zip,application/pdf,application/vnd.oasis.opendocument.text,application/vnd.openxmlformats-officedocument.wordprocessingml.document,application/x-yaml,image,image/jpeg,image/png,image/tiff,pandoc,text/html,text/markdown,text/rtf"},{"location":"pipelines/#url","title":"url","text":"<p>Can be called using:</p> <pre><code>doc.x('url')\n# or\ndoc.url\n</code></pre> <p>return type : </p> <p>supports pipelines : *,,application/epub+zip,application/pdf,application/vnd.oasis.opendocument.text,application/vnd.openxmlformats-officedocument.wordprocessingml.document,application/x-yaml,image,image/jpeg,image/png,image/tiff,pandoc,text/html,text/markdown,text/rtf"},{"location":"pipelines/#sents","title":"sents","text":"<p>Can be called using:</p> <pre><code>doc.x('sents')\n# or\ndoc.sents\n</code></pre> <p>return type : </p> <p>supports pipelines : *,,application/epub+zip,application/pdf,application/vnd.oasis.opendocument.text,application/vnd.openxmlformats-officedocument.wordprocessingml.document,application/x-yaml,image,image/jpeg,image/png,image/tiff,pandoc,text/html,text/markdown,text/rtf"},{"location":"pipelines/#noun_chunks","title":"noun_chunks","text":"<p>Can be called using:</p> <pre><code>doc.x('noun_chunks')\n# or\ndoc.noun_chunks\n</code></pre> <p>return type : </p> <p>supports pipelines : *,,application/epub+zip,application/pdf,application/vnd.oasis.opendocument.text,application/vnd.openxmlformats-officedocument.wordprocessingml.document,application/x-yaml,image,image/jpeg,image/png,image/tiff,pandoc,text/html,text/markdown,text/rtf"},{"location":"pipelines/#vector","title":"vector","text":"<p>Can be called using:</p> <pre><code>doc.x('vector')\n# or\ndoc.vector\n</code></pre> <p>return type : </p> <p>supports pipelines : *,,application/epub+zip,application/pdf,application/vnd.oasis.opendocument.text,application/vnd.openxmlformats-officedocument.wordprocessingml.document,application/x-yaml,image,image/jpeg,image/png,image/tiff,pandoc,text/html,text/markdown,text/rtf"},{"location":"pipelines/#sent_vecs","title":"sent_vecs","text":"<p>Can be called using:</p> <pre><code>doc.x('sent_vecs')\n# or\ndoc.sent_vecs\n</code></pre> <p>return type : </p> <p>supports pipelines : *,,application/epub+zip,application/pdf,application/vnd.oasis.opendocument.text,application/vnd.openxmlformats-officedocument.wordprocessingml.document,application/x-yaml,image,image/jpeg,image/png,image/tiff,pandoc,text/html,text/markdown,text/rtf"},{"location":"pipelines/#sent_ids","title":"sent_ids","text":"<p>Can be called using:</p> <pre><code>doc.x('sent_ids')\n# or\ndoc.sent_ids\n</code></pre> <p>return type : </p> <p>supports pipelines : *,,application/epub+zip,application/pdf,application/vnd.oasis.opendocument.text,application/vnd.openxmlformats-officedocument.wordprocessingml.document,application/x-yaml,image,image/jpeg,image/png,image/tiff,pandoc,text/html,text/markdown,text/rtf"},{"location":"pipelines/#noun_vecs","title":"noun_vecs","text":"<p>Can be called using:</p> <pre><code>doc.x('noun_vecs')\n# or\ndoc.noun_vecs\n</code></pre> <p>return type : </p> <p>supports pipelines : *,,application/epub+zip,application/pdf,application/vnd.oasis.opendocument.text,application/vnd.openxmlformats-officedocument.wordprocessingml.document,application/x-yaml,image,image/jpeg,image/png,image/tiff,pandoc,text/html,text/markdown,text/rtf"},{"location":"pipelines/#noun_ids","title":"noun_ids","text":"<p>Can be called using:</p> <pre><code>doc.x('noun_ids')\n# or\ndoc.noun_ids\n</code></pre> <p>return type : </p> <p>supports pipelines : *,,application/epub+zip,application/pdf,application/vnd.oasis.opendocument.text,application/vnd.openxmlformats-officedocument.wordprocessingml.document,application/x-yaml,image,image/jpeg,image/png,image/tiff,pandoc,text/html,text/markdown,text/rtf"},{"location":"pipelines/#vectorizer_model","title":"vectorizer_model","text":"<p>Choose the embeddings model (huggingface-style) and if we wantto do the vectorization using only the tokenizer. Using only thetokenizer is MUCH faster and uses lower CPU than creating actualcontextual embeddings using the model. BUt is also lower qualitybecause it lacks the context.</p> <p>Can be called using:</p> <pre><code>doc.x('vectorizer_model')\n# or\ndoc.vectorizer_model\n</code></pre> <p>return type : </p> <p>supports pipelines : *,,application/epub+zip,application/pdf,application/vnd.oasis.opendocument.text,application/vnd.openxmlformats-officedocument.wordprocessingml.document,application/x-yaml,image,image/jpeg,image/png,image/tiff,pandoc,text/html,text/markdown,text/rtf"},{"location":"pipelines/#vectorizer_only_tokenizer","title":"vectorizer_only_tokenizer","text":"<p>Choose the embeddings model (huggingface-style) and if we wantto do the vectorization using only the tokenizer. Using only thetokenizer is MUCH faster and uses lower CPU than creating actualcontextual embeddings using the model. BUt is also lower qualitybecause it lacks the context.</p> <p>Can be called using:</p> <pre><code>doc.x('vectorizer_only_tokenizer')\n# or\ndoc.vectorizer_only_tokenizer\n</code></pre> <p>return type : </p> <p>supports pipelines : *,,application/epub+zip,application/pdf,application/vnd.oasis.opendocument.text,application/vnd.openxmlformats-officedocument.wordprocessingml.document,application/x-yaml,image,image/jpeg,image/png,image/tiff,pandoc,text/html,text/markdown,text/rtf"},{"location":"pipelines/#vectorizer_overlap_ratio","title":"vectorizer_overlap_ratio","text":"<p>Choose the embeddings model (huggingface-style) and if we wantto do the vectorization using only the tokenizer. Using only thetokenizer is MUCH faster and uses lower CPU than creating actualcontextual embeddings using the model. BUt is also lower qualitybecause it lacks the context.</p> <p>Can be called using:</p> <pre><code>doc.x('vectorizer_overlap_ratio')\n# or\ndoc.vectorizer_overlap_ratio\n</code></pre> <p>return type : </p> <p>supports pipelines : *,,application/epub+zip,application/pdf,application/vnd.oasis.opendocument.text,application/vnd.openxmlformats-officedocument.wordprocessingml.document,application/x-yaml,image,image/jpeg,image/png,image/tiff,pandoc,text/html,text/markdown,text/rtf"},{"location":"pipelines/#vec_res","title":"vec_res","text":"<p>Can be called using:</p> <pre><code>doc.x('vec_res')\n# or\ndoc.vec_res\n</code></pre> <p>return type : </p> <p>supports pipelines : *,,application/epub+zip,application/pdf,application/vnd.oasis.opendocument.text,application/vnd.openxmlformats-officedocument.wordprocessingml.document,application/x-yaml,image,image/jpeg,image/png,image/tiff,pandoc,text/html,text/markdown,text/rtf"},{"location":"pipelines/#tok_embeddings","title":"tok_embeddings","text":"<p>Can be called using:</p> <pre><code>doc.x('tok_embeddings')\n# or\ndoc.tok_embeddings\n</code></pre> <p>return type : </p> <p>supports pipelines : *,,application/epub+zip,application/pdf,application/vnd.oasis.opendocument.text,application/vnd.openxmlformats-officedocument.wordprocessingml.document,application/x-yaml,image,image/jpeg,image/png,image/tiff,pandoc,text/html,text/markdown,text/rtf"},{"location":"pipelines/#tokens","title":"tokens","text":"<p>Can be called using:</p> <pre><code>doc.x('tokens')\n# or\ndoc.tokens\n</code></pre> <p>return type : </p> <p>supports pipelines : *,,application/epub+zip,application/pdf,application/vnd.oasis.opendocument.text,application/vnd.openxmlformats-officedocument.wordprocessingml.document,application/x-yaml,image,image/jpeg,image/png,image/tiff,pandoc,text/html,text/markdown,text/rtf"},{"location":"pipelines/#embedding","title":"embedding","text":"<p>Can be called using:</p> <pre><code>doc.x('embedding')\n# or\ndoc.embedding\n</code></pre> <p>return type : </p> <p>supports pipelines : *,,application/epub+zip,application/pdf,application/vnd.oasis.opendocument.text,application/vnd.openxmlformats-officedocument.wordprocessingml.document,application/x-yaml,image,image/jpeg,image/png,image/tiff,pandoc,text/html,text/markdown,text/rtf"},{"location":"pipelines/#min_size_text_segment","title":"min_size_text_segment","text":"<p>controls the text segmentation for knowledge basesoverlap is only relevant for large text segmenets that need tobe split up into smaller pieces.</p> <p>Can be called using:</p> <pre><code>doc.x('min_size_text_segment')\n# or\ndoc.min_size_text_segment\n</code></pre> <p>return type : </p> <p>supports pipelines : *,,application/epub+zip,application/pdf,application/vnd.oasis.opendocument.text,application/vnd.openxmlformats-officedocument.wordprocessingml.document,application/x-yaml,image,image/jpeg,image/png,image/tiff,pandoc,text/html,text/markdown,text/rtf"},{"location":"pipelines/#max_size_text_segment","title":"max_size_text_segment","text":"<p>controls the text segmentation for knowledge basesoverlap is only relevant for large text segmenets that need tobe split up into smaller pieces.</p> <p>Can be called using:</p> <pre><code>doc.x('max_size_text_segment')\n# or\ndoc.max_size_text_segment\n</code></pre> <p>return type : </p> <p>supports pipelines : *,,application/epub+zip,application/pdf,application/vnd.oasis.opendocument.text,application/vnd.openxmlformats-officedocument.wordprocessingml.document,application/x-yaml,image,image/jpeg,image/png,image/tiff,pandoc,text/html,text/markdown,text/rtf"},{"location":"pipelines/#text_segment_overlap","title":"text_segment_overlap","text":"<p>controls the text segmentation for knowledge basesoverlap is only relevant for large text segmenets that need tobe split up into smaller pieces.</p> <p>Can be called using:</p> <pre><code>doc.x('text_segment_overlap')\n# or\ndoc.text_segment_overlap\n</code></pre> <p>return type : </p> <p>supports pipelines : *,,application/epub+zip,application/pdf,application/vnd.oasis.opendocument.text,application/vnd.openxmlformats-officedocument.wordprocessingml.document,application/x-yaml,image,image/jpeg,image/png,image/tiff,pandoc,text/html,text/markdown,text/rtf"},{"location":"pipelines/#text_segments","title":"text_segments","text":"<p>Can be called using:</p> <pre><code>doc.x('text_segments')\n# or\ndoc.text_segments\n</code></pre> <p>return type : </p> <p>supports pipelines : *,,application/epub+zip,application/pdf,application/vnd.oasis.opendocument.text,application/vnd.openxmlformats-officedocument.wordprocessingml.document,application/x-yaml,image,image/jpeg,image/png,image/tiff,pandoc,text/html,text/markdown,text/rtf"},{"location":"pipelines/#text_segment_vectors","title":"text_segment_vectors","text":"<p>Can be called using:</p> <pre><code>doc.x('text_segment_vectors')\n# or\ndoc.text_segment_vectors\n</code></pre> <p>return type : </p> <p>supports pipelines : *,,application/epub+zip,application/pdf,application/vnd.oasis.opendocument.text,application/vnd.openxmlformats-officedocument.wordprocessingml.document,application/x-yaml,image,image/jpeg,image/png,image/tiff,pandoc,text/html,text/markdown,text/rtf"},{"location":"pipelines/#noun_index","title":"noun_index","text":"<p>Can be called using:</p> <pre><code>doc.x('noun_index')\n# or\ndoc.noun_index\n</code></pre> <p>return type : </p> <p>supports pipelines : *,,application/epub+zip,application/pdf,application/vnd.oasis.opendocument.text,application/vnd.openxmlformats-officedocument.wordprocessingml.document,application/x-yaml,image,image/jpeg,image/png,image/tiff,pandoc,text/html,text/markdown,text/rtf"},{"location":"pipelines/#vectorizer","title":"vectorizer","text":"<p>Can be called using:</p> <pre><code>doc.x('vectorizer')\n# or\ndoc.vectorizer\n</code></pre> <p>return type : </p> <p>supports pipelines : *,,application/epub+zip,application/pdf,application/vnd.oasis.opendocument.text,application/vnd.openxmlformats-officedocument.wordprocessingml.document,application/x-yaml,image,image/jpeg,image/png,image/tiff,pandoc,text/html,text/markdown,text/rtf"},{"location":"pipelines/#noun_query","title":"noun_query","text":"<p>Can be called using:</p> <pre><code>doc.x('noun_query')\n# or\ndoc.noun_query\n</code></pre> <p>return type : typing.Callable</p> <p>supports pipelines : *,,application/epub+zip,application/pdf,application/vnd.oasis.opendocument.text,application/vnd.openxmlformats-officedocument.wordprocessingml.document,application/x-yaml,image,image/jpeg,image/png,image/tiff,pandoc,text/html,text/markdown,text/rtf"},{"location":"pipelines/#noun_graph","title":"noun_graph","text":"<p>Can be called using:</p> <pre><code>doc.x('noun_graph')\n# or\ndoc.noun_graph\n</code></pre> <p>return type : </p> <p>supports pipelines : *,,application/epub+zip,application/pdf,application/vnd.oasis.opendocument.text,application/vnd.openxmlformats-officedocument.wordprocessingml.document,application/x-yaml,image,image/jpeg,image/png,image/tiff,pandoc,text/html,text/markdown,text/rtf"},{"location":"pipelines/#top_k_text_rank_keywords","title":"top_k_text_rank_keywords","text":"<p>Can be called using:</p> <pre><code>doc.x('top_k_text_rank_keywords')\n# or\ndoc.top_k_text_rank_keywords\n</code></pre> <p>return type : </p> <p>supports pipelines : *,,application/epub+zip,application/pdf,application/vnd.oasis.opendocument.text,application/vnd.openxmlformats-officedocument.wordprocessingml.document,application/x-yaml,image,image/jpeg,image/png,image/tiff,pandoc,text/html,text/markdown,text/rtf"},{"location":"pipelines/#textrank_keywords","title":"textrank_keywords","text":"<p>Can be called using:</p> <pre><code>doc.x('textrank_keywords')\n# or\ndoc.textrank_keywords\n</code></pre> <p>return type : set[str]</p> <p>supports pipelines : *,,application/epub+zip,application/pdf,application/vnd.oasis.opendocument.text,application/vnd.openxmlformats-officedocument.wordprocessingml.document,application/x-yaml,image,image/jpeg,image/png,image/tiff,pandoc,text/html,text/markdown,text/rtf"},{"location":"pipelines/#keywords","title":"keywords","text":"<p>Can be called using:</p> <pre><code>doc.x('keywords')\n# or\ndoc.keywords\n</code></pre> <p>return type : </p> <p>supports pipelines : *,,application/epub+zip,application/pdf,application/vnd.oasis.opendocument.text,application/vnd.openxmlformats-officedocument.wordprocessingml.document,application/x-yaml,image,image/jpeg,image/png,image/tiff,pandoc,text/html,text/markdown,text/rtf"},{"location":"pipelines/#sent_index","title":"sent_index","text":"<p>Can be called using:</p> <pre><code>doc.x('sent_index')\n# or\ndoc.sent_index\n</code></pre> <p>return type : </p> <p>supports pipelines : *,,application/epub+zip,application/pdf,application/vnd.oasis.opendocument.text,application/vnd.openxmlformats-officedocument.wordprocessingml.document,application/x-yaml,image,image/jpeg,image/png,image/tiff,pandoc,text/html,text/markdown,text/rtf"},{"location":"pipelines/#sent_query","title":"sent_query","text":"<p>Can be called using:</p> <pre><code>doc.x('sent_query')\n# or\ndoc.sent_query\n</code></pre> <p>return type : typing.Callable</p> <p>supports pipelines : *,,application/epub+zip,application/pdf,application/vnd.oasis.opendocument.text,application/vnd.openxmlformats-officedocument.wordprocessingml.document,application/x-yaml,image,image/jpeg,image/png,image/tiff,pandoc,text/html,text/markdown,text/rtf"},{"location":"pipelines/#sent_graph","title":"sent_graph","text":"<p>Can be called using:</p> <pre><code>doc.x('sent_graph')\n# or\ndoc.sent_graph\n</code></pre> <p>return type : </p> <p>supports pipelines : *,,application/epub+zip,application/pdf,application/vnd.oasis.opendocument.text,application/vnd.openxmlformats-officedocument.wordprocessingml.document,application/x-yaml,image,image/jpeg,image/png,image/tiff,pandoc,text/html,text/markdown,text/rtf"},{"location":"pipelines/#top_k_text_rank_sentences","title":"top_k_text_rank_sentences","text":"<p>Can be called using:</p> <pre><code>doc.x('top_k_text_rank_sentences')\n# or\ndoc.top_k_text_rank_sentences\n</code></pre> <p>return type : </p> <p>supports pipelines : *,,application/epub+zip,application/pdf,application/vnd.oasis.opendocument.text,application/vnd.openxmlformats-officedocument.wordprocessingml.document,application/x-yaml,image,image/jpeg,image/png,image/tiff,pandoc,text/html,text/markdown,text/rtf"},{"location":"pipelines/#textrank_sents","title":"textrank_sents","text":"<p>Can be called using:</p> <pre><code>doc.x('textrank_sents')\n# or\ndoc.textrank_sents\n</code></pre> <p>return type : set[str]</p> <p>supports pipelines : *,,application/epub+zip,application/pdf,application/vnd.oasis.opendocument.text,application/vnd.openxmlformats-officedocument.wordprocessingml.document,application/x-yaml,image,image/jpeg,image/png,image/tiff,pandoc,text/html,text/markdown,text/rtf"},{"location":"pipelines/#summarizer_model","title":"summarizer_model","text":"<p>Can be called using:</p> <pre><code>doc.x('summarizer_model')\n# or\ndoc.summarizer_model\n</code></pre> <p>return type : </p> <p>supports pipelines : *,,application/epub+zip,application/pdf,application/vnd.oasis.opendocument.text,application/vnd.openxmlformats-officedocument.wordprocessingml.document,application/x-yaml,image,image/jpeg,image/png,image/tiff,pandoc,text/html,text/markdown,text/rtf"},{"location":"pipelines/#summarizer_token_overlap","title":"summarizer_token_overlap","text":"<p>Can be called using:</p> <pre><code>doc.x('summarizer_token_overlap')\n# or\ndoc.summarizer_token_overlap\n</code></pre> <p>return type : </p> <p>supports pipelines : *,,application/epub+zip,application/pdf,application/vnd.oasis.opendocument.text,application/vnd.openxmlformats-officedocument.wordprocessingml.document,application/x-yaml,image,image/jpeg,image/png,image/tiff,pandoc,text/html,text/markdown,text/rtf"},{"location":"pipelines/#summarizer_max_text_len","title":"summarizer_max_text_len","text":"<p>Can be called using:</p> <pre><code>doc.x('summarizer_max_text_len')\n# or\ndoc.summarizer_max_text_len\n</code></pre> <p>return type : </p> <p>supports pipelines : *,,application/epub+zip,application/pdf,application/vnd.oasis.opendocument.text,application/vnd.openxmlformats-officedocument.wordprocessingml.document,application/x-yaml,image,image/jpeg,image/png,image/tiff,pandoc,text/html,text/markdown,text/rtf"},{"location":"pipelines/#slow_summary","title":"slow_summary","text":"<p>Can be called using:</p> <pre><code>doc.x('slow_summary')\n# or\ndoc.slow_summary\n</code></pre> <p>return type : </p> <p>supports pipelines : *,,application/epub+zip,application/pdf,application/vnd.oasis.opendocument.text,application/vnd.openxmlformats-officedocument.wordprocessingml.document,application/x-yaml,image,image/jpeg,image/png,image/tiff,pandoc,text/html,text/markdown,text/rtf"},{"location":"pipelines/#qam_model_id","title":"qam_model_id","text":"<p>Can be called using:</p> <pre><code>doc.x('qam_model_id')\n# or\ndoc.qam_model_id\n</code></pre> <p>return type : </p> <p>supports pipelines : *,,application/epub+zip,application/pdf,application/vnd.oasis.opendocument.text,application/vnd.openxmlformats-officedocument.wordprocessingml.document,application/x-yaml,image,image/jpeg,image/png,image/tiff,pandoc,text/html,text/markdown,text/rtf"},{"location":"pipelines/#answers","title":"answers","text":"<p>Can be called using:</p> <pre><code>doc.x('answers')\n# or\ndoc.answers\n</code></pre> <p>return type : </p> <p>supports pipelines : *,,application/epub+zip,application/pdf,application/vnd.oasis.opendocument.text,application/vnd.openxmlformats-officedocument.wordprocessingml.document,application/x-yaml,image,image/jpeg,image/png,image/tiff,pandoc,text/html,text/markdown,text/rtf"},{"location":"pipelines/#openai_chat_model_id","title":"openai_chat_model_id","text":"<p>Can be called using:</p> <pre><code>doc.x('openai_chat_model_id')\n# or\ndoc.openai_chat_model_id\n</code></pre> <p>return type : </p> <p>supports pipelines : *,,application/epub+zip,application/pdf,application/vnd.oasis.opendocument.text,application/vnd.openxmlformats-officedocument.wordprocessingml.document,application/x-yaml,image,image/jpeg,image/png,image/tiff,pandoc,text/html,text/markdown,text/rtf"},{"location":"pipelines/#chat_answers","title":"chat_answers","text":"<p>Can be called using:</p> <pre><code>doc.x('chat_answers')\n# or\ndoc.chat_answers\n</code></pre> <p>return type : typing.Callable[[list[str], list[str] | str], list[str]]</p> <p>supports pipelines : *,,application/epub+zip,application/pdf,application/vnd.oasis.opendocument.text,application/vnd.openxmlformats-officedocument.wordprocessingml.document,application/x-yaml,image,image/jpeg,image/png,image/tiff,pandoc,text/html,text/markdown,text/rtf"},{"location":"pipelines/#page_set","title":"page_set","text":"<p>Can be called using:</p> <pre><code>doc.x('page_set')\n# or\ndoc.page_set\n</code></pre> <p>return type : </p> <p>supports pipelines : application/pdf,image,image/jpeg,image/png,image/tiff</p>"},{"location":"pipelines/#pages_bbox","title":"pages_bbox","text":"<p>Can be called using:</p> <pre><code>doc.x('pages_bbox')\n# or\ndoc.pages_bbox\n</code></pre> <p>return type : </p> <p>supports pipelines : application/pdf,image,image/jpeg,image/png,image/tiff</p>"},{"location":"pipelines/#elements","title":"elements","text":"<p>Can be called using:</p> <pre><code>doc.x('elements')\n# or\ndoc.elements\n</code></pre> <p>return type : </p> <p>supports pipelines : application/pdf,image,image/jpeg,image/png,image/tiff</p>"},{"location":"pipelines/#meta","title":"meta","text":"<p>Can be called using:</p> <pre><code>doc.x('meta')\n# or\ndoc.meta\n</code></pre> <p>return type : </p> <p>supports pipelines : application/pdf,image,image/jpeg,image/png,image/tiff</p>"},{"location":"pipelines/#line_elements","title":"line_elements","text":"<p>Can be called using:</p> <pre><code>doc.x('line_elements')\n# or\ndoc.line_elements\n</code></pre> <p>return type : </p> <p>supports pipelines : application/pdf,image,image/jpeg,image/png,image/tiff</p>"},{"location":"pipelines/#graphic_elements","title":"graphic_elements","text":"<p>Can be called using:</p> <pre><code>doc.x('graphic_elements')\n# or\ndoc.graphic_elements\n</code></pre> <p>return type : </p> <p>supports pipelines : application/pdf,image,image/jpeg,image/png,image/tiff</p>"},{"location":"pipelines/#lists","title":"lists","text":"<p>Can be called using:</p> <pre><code>doc.x('lists')\n# or\ndoc.lists\n</code></pre> <p>return type : str | list[str] | list[pandas.core.frame.DataFrame]</p> <p>supports pipelines : application/epub+zip,application/pdf,application/vnd.oasis.opendocument.text,application/vnd.openxmlformats-officedocument.wordprocessingml.document,image,image/jpeg,image/png,image/tiff,pandoc,text/markdown,text/rtf</p>"},{"location":"pipelines/#table_box_levels","title":"table_box_levels","text":"<p>Can be called using:</p> <pre><code>doc.x('table_box_levels')\n# or\ndoc.table_box_levels\n</code></pre> <p>return type : </p> <p>supports pipelines : application/pdf,image,image/jpeg,image/png,image/tiff</p>"},{"location":"pipelines/#table_candidates","title":"table_candidates","text":"<p>Can be called using:</p> <pre><code>doc.x('table_candidates')\n# or\ndoc.table_candidates\n</code></pre> <p>return type : </p> <p>supports pipelines : application/pdf,image,image/jpeg,image/png,image/tiff</p>"},{"location":"pipelines/#table_df0","title":"table_df0","text":"<p>Can be called using:</p> <pre><code>doc.x('table_df0')\n# or\ndoc.table_df0\n</code></pre> <p>return type : </p> <p>supports pipelines : application/pdf,image,image/jpeg,image/png,image/tiff</p>"},{"location":"pipelines/#titles","title":"titles","text":"<p>Can be called using:</p> <pre><code>doc.x('titles')\n# or\ndoc.titles\n</code></pre> <p>return type : </p> <p>supports pipelines : application/pdf,image,image/jpeg,image/png,image/tiff,text/html</p>"},{"location":"pipelines/#side_titles","title":"side_titles","text":"<p>Can be called using:</p> <pre><code>doc.x('side_titles')\n# or\ndoc.side_titles\n</code></pre> <p>return type : </p> <p>supports pipelines : application/pdf,image,image/jpeg,image/png,image/tiff</p>"},{"location":"pipelines/#html_keywords_str","title":"html_keywords_str","text":"<p>Can be called using:</p> <pre><code>doc.x('html_keywords_str')\n# or\ndoc.html_keywords_str\n</code></pre> <p>return type : </p> <p>supports pipelines : text/html</p>"},{"location":"pipelines/#main_content_clean_html","title":"main_content_clean_html","text":"<p>Can be called using:</p> <pre><code>doc.x('main_content_clean_html')\n# or\ndoc.main_content_clean_html\n</code></pre> <p>return type : </p> <p>supports pipelines : text/html</p>"},{"location":"pipelines/#summary","title":"summary","text":"<p>Can be called using:</p> <pre><code>doc.x('summary')\n# or\ndoc.summary\n</code></pre> <p>return type : </p> <p>supports pipelines : text/html</p>"},{"location":"pipelines/#goose_article","title":"goose_article","text":"<p>Can be called using:</p> <pre><code>doc.x('goose_article')\n# or\ndoc.goose_article\n</code></pre> <p>return type : </p> <p>supports pipelines : text/html</p>"},{"location":"pipelines/#main_content","title":"main_content","text":"<p>Can be called using:</p> <pre><code>doc.x('main_content')\n# or\ndoc.main_content\n</code></pre> <p>return type : </p> <p>supports pipelines : text/html</p>"},{"location":"pipelines/#schemadata","title":"schemadata","text":"<p>Can be called using:</p> <pre><code>doc.x('schemadata')\n# or\ndoc.schemadata\n</code></pre> <p>return type : </p> <p>supports pipelines : text/html</p>"},{"location":"pipelines/#final_urls","title":"final_urls","text":"<p>Can be called using:</p> <pre><code>doc.x('final_urls')\n# or\ndoc.final_urls\n</code></pre> <p>return type : </p> <p>supports pipelines : text/html</p>"},{"location":"pipelines/#pdf_links","title":"pdf_links","text":"<p>Can be called using:</p> <pre><code>doc.x('pdf_links')\n# or\ndoc.pdf_links\n</code></pre> <p>return type : </p> <p>supports pipelines : text/html</p>"},{"location":"pipelines/#title","title":"title","text":"<p>Can be called using:</p> <pre><code>doc.x('title')\n# or\ndoc.title\n</code></pre> <p>return type : </p> <p>supports pipelines : text/html</p>"},{"location":"pipelines/#short_title","title":"short_title","text":"<p>Can be called using:</p> <pre><code>doc.x('short_title')\n# or\ndoc.short_title\n</code></pre> <p>return type : </p> <p>supports pipelines : text/html</p>"},{"location":"pipelines/#urls","title":"urls","text":"<p>Can be called using:</p> <pre><code>doc.x('urls')\n# or\ndoc.urls\n</code></pre> <p>return type : </p> <p>supports pipelines : text/html</p>"},{"location":"pipelines/#main_image","title":"main_image","text":"<p>Can be called using:</p> <pre><code>doc.x('main_image')\n# or\ndoc.main_image\n</code></pre> <p>return type : </p> <p>supports pipelines : text/html</p>"},{"location":"pipelines/#html_keywords","title":"html_keywords","text":"<p>Can be called using:</p> <pre><code>doc.x('html_keywords')\n# or\ndoc.html_keywords\n</code></pre> <p>return type : </p> <p>supports pipelines : text/html</p>"},{"location":"pipelines/#pandoc_document","title":"pandoc_document","text":"<p>Can be called using:</p> <pre><code>doc.x('pandoc_document')\n# or\ndoc.pandoc_document\n</code></pre> <p>return type : Pandoc(Meta, [Block])</p> <p>supports pipelines : application/epub+zip,application/vnd.oasis.opendocument.text,application/vnd.openxmlformats-officedocument.wordprocessingml.document,pandoc,text/markdown,text/rtf</p>"},{"location":"pipelines/#full_text_format","title":"full_text_format","text":"<p>Can be called using:</p> <pre><code>doc.x('full_text_format')\n# or\ndoc.full_text_format\n</code></pre> <p>return type : </p> <p>supports pipelines : application/epub+zip,application/vnd.oasis.opendocument.text,application/vnd.openxmlformats-officedocument.wordprocessingml.document,pandoc,text/markdown,text/rtf</p>"},{"location":"pipelines/#convert_to","title":"convert_to","text":"<p>Can be called using:</p> <pre><code>doc.x('convert_to')\n# or\ndoc.convert_to\n</code></pre> <p>return type : </p> <p>supports pipelines : application/epub+zip,application/vnd.oasis.opendocument.text,application/vnd.openxmlformats-officedocument.wordprocessingml.document,pandoc,text/markdown,text/rtf</p>"},{"location":"pipelines/#clean_format","title":"clean_format","text":"<p>Can be called using:</p> <pre><code>doc.x('clean_format')\n# or\ndoc.clean_format\n</code></pre> <p>return type : </p> <p>supports pipelines : application/epub+zip,application/vnd.oasis.opendocument.text,application/vnd.openxmlformats-officedocument.wordprocessingml.document,pandoc,text/markdown,text/rtf</p>"},{"location":"pipelines/#sections","title":"sections","text":"<p>Can be called using:</p> <pre><code>doc.x('sections')\n# or\ndoc.sections\n</code></pre> <p>return type : </p> <p>supports pipelines : application/epub+zip,application/vnd.oasis.opendocument.text,application/vnd.openxmlformats-officedocument.wordprocessingml.document,pandoc,text/markdown,text/rtf</p>"},{"location":"pipelines/#pandoc_blocks","title":"pandoc_blocks","text":"<p>Can be called using:</p> <pre><code>doc.x('pandoc_blocks')\n# or\ndoc.pandoc_blocks\n</code></pre> <p>return type : list['pandoc.types.Block']</p> <p>supports pipelines : application/epub+zip,application/vnd.oasis.opendocument.text,application/vnd.openxmlformats-officedocument.wordprocessingml.document,pandoc,text/markdown,text/rtf</p>"},{"location":"pipelines/#headers","title":"headers","text":"<p>Can be called using:</p> <pre><code>doc.x('headers')\n# or\ndoc.headers\n</code></pre> <p>return type : str | list[str] | list[pandas.core.frame.DataFrame]</p> <p>supports pipelines : application/epub+zip,application/vnd.oasis.opendocument.text,application/vnd.openxmlformats-officedocument.wordprocessingml.document,pandoc,text/markdown,text/rtf</p>"},{"location":"pipelines/#ocr_lang","title":"ocr_lang","text":"<p>Can be called using:</p> <pre><code>doc.x('ocr_lang')\n# or\ndoc.ocr_lang\n</code></pre> <p>return type : </p> <p>supports pipelines : image,image/jpeg,image/png,image/tiff</p>"},{"location":"pipelines/#ocr_on","title":"ocr_on","text":"<p>Can be called using:</p> <pre><code>doc.x('ocr_on')\n# or\ndoc.ocr_on\n</code></pre> <p>return type : </p> <p>supports pipelines : image,image/jpeg,image/png,image/tiff</p>"},{"location":"pipelines/#ocr_pdf_file","title":"ocr_pdf_file","text":"<p>Can be called using:</p> <pre><code>doc.x('ocr_pdf_file')\n# or\ndoc.ocr_pdf_file\n</code></pre> <p>return type : </p> <p>supports pipelines : image,image/jpeg,image/png,image/tiff</p>"},{"location":"pipelines/#data_sel","title":"data_sel","text":"<p>select values by key from source data in Document</p> <p>Can be called using:</p> <pre><code>doc.x('data_sel')\n# or\ndoc.data_sel\n</code></pre> <p>return type : typing.Callable[..., dict]</p> <p>supports pipelines : ,application/x-yaml"},{"location":"pipelines/#keys","title":"keys","text":"<p>Can be called using:</p> <pre><code>doc.x('keys')\n# or\ndoc.keys\n</code></pre> <p>return type : </p> <p>supports pipelines : ,application/x-yaml"},{"location":"pipelines/#values","title":"values","text":"<p>Can be called using:</p> <pre><code>doc.x('values')\n# or\ndoc.values\n</code></pre> <p>return type : </p> <p>supports pipelines : ,application/x-yaml"},{"location":"pipelines/#items","title":"items","text":"<p>Can be called using:</p> <pre><code>doc.x('items')\n# or\ndoc.items\n</code></pre> <p>return type : </p> <p>supports pipelines : ,application/x-yaml"},{"location":"pipelines/#pydoxtoolsdocumentbag","title":"pydoxtools.DocumentBag","text":""},{"location":"pipelines/#doc_configuration","title":"doc_configuration","text":"<p>We can pass through a configuration object to Documents that are created in our document bag. Any setting that is supported by Document can be specified here.</p> <p>Can be called using:</p> <pre><code>doc.x('doc_configuration')\n# or\ndoc.doc_configuration\n</code></pre> <p>return type : </p> <p>supports pipelines : ,,,"},{"location":"pipelines/#docs","title":"docs","text":"<p>create a bag with one document for each file that was foundFrom this point we can hand off the logic to str(Bag) pipeline.</p> <p>Can be called using:</p> <pre><code>doc.x('docs')\n# or\ndoc.docs\n</code></pre> <p>return type :  <p>supports pipelines : ,,,"},{"location":"pipelines/#take","title":"take","text":"<p>Can be called using:</p> <pre><code>doc.x('take')\n# or\ndoc.take\n</code></pre> <p>return type : </p> <p>supports pipelines : ,,,"},{"location":"pipelines/#compute","title":"compute","text":"<p>Can be called using:</p> <pre><code>doc.x('compute')\n# or\ndoc.compute\n</code></pre> <p>return type : </p> <p>supports pipelines : ,,,"},{"location":"pipelines/#get_dicts","title":"get_dicts","text":"<p>Can be called using:</p> <pre><code>doc.x('get_dicts')\n# or\ndoc.get_dicts\n</code></pre> <p>return type : typing.Callable[[typing.Any], dask.bag.core.Bag]</p> <p>supports pipelines : ,,,"},{"location":"pipelines/#e","title":"e","text":"<p>Can be called using:</p> <pre><code>doc.x('e')\n# or\ndoc.e\n</code></pre> <p>return type : typing.Callable[..., pydoxtools.document.DocumentBag]</p> <p>supports pipelines : ,,,"},{"location":"pipelines/#idx_dict","title":"idx_dict","text":"<p>Can be called using:</p> <pre><code>doc.x('idx_dict')\n# or\ndoc.idx_dict\n</code></pre> <p>return type : </p> <p>supports pipelines : ,,,"},{"location":"pipelines/#vectorizer_1","title":"vectorizer","text":"<p>Can be called using:</p> <pre><code>doc.x('vectorizer')\n# or\ndoc.vectorizer\n</code></pre> <p>return type : </p> <p>supports pipelines : ,,,"},{"location":"pipelines/#chroma_index","title":"chroma_index","text":"<p>in order to build an index in chrome db we need a key, text, embeddings and a key. Those come from a daskbag with dictionaries with those keys. Caching is important here in order to retain the index</p> <p>Can be called using:</p> <pre><code>doc.x('chroma_index')\n# or\ndoc.chroma_index\n</code></pre> <p>return type : </p> <p>supports pipelines : ,,,"},{"location":"pipelines/#compute_index","title":"compute_index","text":"<p>in order to build an index in chrome db we need a key, text, embeddings and a key. Those come from a daskbag with dictionaries with those keys. Caching is important here in order to retain the index</p> <p>Can be called using:</p> <pre><code>doc.x('compute_index')\n# or\ndoc.compute_index\n</code></pre> <p>return type : </p> <p>supports pipelines : ,,,"},{"location":"pipelines/#query_chroma","title":"query_chroma","text":"<p>in order to build an index in chrome db we need a key, text, embeddings and a key. Those come from a daskbag with dictionaries with those keys. Caching is important here in order to retain the index</p> <p>Can be called using:</p> <pre><code>doc.x('query_chroma')\n# or\ndoc.query_chroma\n</code></pre> <p>return type : </p> <p>supports pipelines : ,,,"},{"location":"pipelines/#sql","title":"sql","text":"<p>Can be called using:</p> <pre><code>doc.x('sql')\n# or\ndoc.sql\n</code></pre> <p>return type : </p> <p>supports pipelines :"},{"location":"pipelines/#connection_string","title":"connection_string","text":"<p>Can be called using:</p> <pre><code>doc.x('connection_string')\n# or\ndoc.connection_string\n</code></pre> <p>return type : </p> <p>supports pipelines :"},{"location":"pipelines/#index_column","title":"index_column","text":"<p>Can be called using:</p> <pre><code>doc.x('index_column')\n# or\ndoc.index_column\n</code></pre> <p>return type : </p> <p>supports pipelines :"},{"location":"pipelines/#bytes_per_chunk","title":"bytes_per_chunk","text":"<p>Can be called using:</p> <pre><code>doc.x('bytes_per_chunk')\n# or\ndoc.bytes_per_chunk\n</code></pre> <p>return type : </p> <p>supports pipelines :"},{"location":"pipelines/#dataframe","title":"dataframe","text":"<p>Can be called using:</p> <pre><code>doc.x('dataframe')\n# or\ndoc.dataframe\n</code></pre> <p>return type :  <p>supports pipelines :"},{"location":"pipelines/#bag","title":"bag","text":"<p>create a dask bag with all the filepaths in it</p> <p>Can be called using:</p> <pre><code>doc.x('bag')\n# or\ndoc.bag\n</code></pre> <p>return type : </p> <p>supports pipelines : ,,"},{"location":"pipelines/#root_path","title":"root_path","text":"<p>Can be called using:</p> <pre><code>doc.x('root_path')\n# or\ndoc.root_path\n</code></pre> <p>return type : </p> <p>supports pipelines : ,"},{"location":"pipelines/#file_path_list","title":"file_path_list","text":"<p>Can be called using:</p> <pre><code>doc.x('file_path_list')\n# or\ndoc.file_path_list\n</code></pre> <p>return type : list[pathlib.Path] <p>supports pipelines : ,"},{"location":"pipelines/#dir_list","title":"dir_list","text":"<p>Can be called using:</p> <pre><code>doc.x('dir_list')\n# or\ndoc.dir_list\n</code></pre> <p>return type : list[pathlib.Path] <p>supports pipelines : ,"},{"location":"readme_cp/","title":"\ud83d\ude80 pydoxtools (Python Library) \ud83d\ude80","text":"<p>(WIP) Documentation</p> <p>Pydoxtools is a library that provides a sophisticated interface for reading and writing documents, designed to work with AI models such as GPT, Alpaca, and Huggingface. It offers functionalities such as:</p> <ul> <li>Pipeline management</li> <li>Integration with AI (LLMs and more) models</li> <li>low-resource (PDF) table extraction without configuration and expensive   layout detection algorithms!</li> <li>Document analysis and question-answering</li> <li>Support for most of todays document formats</li> <li>Vector index Creation</li> <li>Entity, address identification and more</li> <li>List and keyword extraction</li> <li>Data normalization, translation, and cleaning</li> </ul> <p>The library allows for the creation of complex extraction pipelines for batch-processing of documents by defining them as a lazily-executed graph.</p>"},{"location":"readme_cp/#installation","title":"Installation","text":""},{"location":"readme_cp/#installing-from-github","title":"Installing from GitHub","text":"<p>While pydoxtools can already be installed through pip, due to the many updates coming in right now, it is currently recommended to use the latest version from GitHub as follows:</p> <pre><code>pip install -U \"pydoxtools[etl,inference] @ git+https://github.com/xyntopia/pydoxtools.git\"\n</code></pre>"},{"location":"readme_cp/#installing-from-pypi","title":"Installing from PyPI","text":"<p>Pydoxtools can also be installed through pip, which will become the recommended method once it becomes more stable:</p> <pre><code>pip install -U pydoxtools[etl,inference]\n</code></pre> <p>For loading additional file formats (docx, odt, epub) and images, check out the additional &gt; Installation Options &lt;.</p>"},{"location":"readme_cp/#teaser","title":"Teaser","text":"<p>Experience a new level of convenience and efficiency in handling documents with Pydoxtools, and reimagine your data extraction pipelines! \ud83c\udfa9\u2728\ud83d\udcc4.</p> <p>In this teaser, we'll demonstrate how to create a document, extract tables, and ask questions using AI models:</p> <pre><code>import pydoxtools as pdx\n\n# Create a document from various sources: file, string, bytestring, file-like object, or URL\ndoc = pdx.Document(\"https://www.raspberrypi.org/app/uploads/2012/12/quick-start-guide-v1.1.pdf\")\n\n# List available extraction functions\nprint(doc.x_funcs)\n\n# Extract tables from the PDF as a pandas DataFrame\nprint(doc.tables_df)\n\n# Ask a question about the documents using a local Q&amp;A model\nprint(doc.answers([\"how much ram does it have?\"]))\n# Or only ask about the documents tables (or any other extracted information):\nprint(doc.answers([\"how much ram does it have?\"], \"tables\"))\n\n# To use ChatGPT for question-answering, set the API key as an environment variable:\n# OPENAI_API_KEY=\"sk ....\"\n# Then, ask questions about the document using ChatGPT\nprint(doc.chat_answers([\"What is the target group of this document?\"])[0].content)\nprint(doc.chat_answers([\"Answer if a 5-year old would be able to follow these instructions?\"])[0].content)\n</code></pre> <p>With Pydoxtools, you can easily access and process your documents, perform various extractions, and utilize AI models for more advanced analysis.</p>"},{"location":"readme_cp/#some-features-in-more-detail","title":"Some Features in more Detail","text":""},{"location":"readme_cp/#large-pipelines","title":"Large pipelines","text":""},{"location":"readme_cp/#some-features-in-more-detail_1","title":"Some Features in More Detail","text":""},{"location":"readme_cp/#large-pipelines_1","title":"Large Pipelines","text":"<p>Pydoxtools' main feature is the ability to mix LLMs and other AI models in large, composable, and customizable pipelines. Using pipelines comes with the slight disadvantage that it can be more challenging to add type hints to the code. However, using pipelines decouples all parts of your code, allowing all operators to work independently. This makes it easy to run the pipeline in a distributed setting for big data and enables easy, lazy evaluation. Additionally, mixing different LLM logics together becomes much easier.</p> <p>Check out how Pydoxtools' <code>Document</code> class mixes pipelines for each individual file type:</p> <ul> <li>Every node in an ellipse can be called as an attribute of the document-analysis pipeline.</li> <li>Every execution path is lazily executed throughout the entire graph.</li> <li>Every node is cached by default (but can be turned off).</li> <li>Every piece of this pipeline can be replaced by a customized version.</li> </ul> <p>As an example, consider this pipeline for *.png images from the repository, which includes OCR, keyword extraction, vectorization, and more:</p> <p></p> <p>Pipelines can be mixed, partially overwritten, and extended, giving you a lot of possibilities to extend and adapt the functionality for your specific use case.</p> <p>To learn more about Pydoxtools' large pipelines feature, please refer to the documentation.</p>"},{"location":"readme_cp/#pipeline-configuration","title":"Pipeline Configuration","text":"<p>Pipelines can be configured. For example the local model used for question answering can be selected like this:</p> <pre><code>doc = Document(fobj=\"./data/PFR-PR23_BAT-110__V1.00_.pdf\"))\n        .config(qam_model_id='bert-large-uncased-whole-word-masking-finetuned-squad')\n</code></pre> <p>where \"qam_model_id\" can be any model from huggingface for question answering.</p> <pre><code>TODO: document how to configure a pipeline\n</code></pre>"},{"location":"readme_cp/#pdf-table-extraction-algorithms","title":"PDF Table Extraction Algorithms","text":"<p>The library features its own sophisticated Table extraction algorithm which is benchmarked against a large pdf table dataset. In contrast to how most \"classical\" table extraction algorithms work, it doesn't require:</p> <ul> <li>extensive configuration</li> <li>no expensive deep neural networks for table area recognition which need a GPU and   a lot of memory/CPU requirements</li> </ul> <p>This makes it possible to run analysis on PDF files with pydoxtools on CPU with very limited resources!</p>"},{"location":"readme_cp/#todo-describe-more-of-the-features-here","title":"TODO: Describe more of the features here...","text":""},{"location":"readme_cp/#use-cases","title":"Use Cases","text":"<ul> <li>analyze documents using any model from huggingface...</li> <li>analyze documents using a custom model</li> <li>download a pdf from URL</li> <li>generate document keywords</li> <li>extract tables</li> <li>download document from URL \"manually\" and then feed to document</li> <li>extract addresses</li> <li>extract addresses and use this information for the qam</li> <li>ingest documents into a vector db</li> </ul>"},{"location":"readme_cp/#installation-options","title":"Installation Options","text":""},{"location":"readme_cp/#supporting-docx-odt-epub","title":"Supporting .docx, .odt, *.epub","text":"<p>In order to be able to load docx, odt and rtf files, you have to install pandoc. Right now, the python pandoc library does not work with pandoc version &gt; 3.0.0. It is therefore recommended to install a version from here for your OS:</p> <p>https://github.com/jgm/pandoc/releases/tag/2.19.2</p>"},{"location":"readme_cp/#image-ocr-support","title":"Image OCR Support","text":"<p>Pydoxtools can automatically analyze images as well, makin use of OCR. In order to be able to use this, install tesseract on your system:</p> <p>Under linux this looks like the following:</p> <pre><code>apt-get update &amp;&amp; tesseract-ocr\n# install tesseract languages \n# Display a list of all Tesseract language packs:\n#   apt-cache search tesseract-ocr\n# install all languages:\n# sudo apt install tesseract-ocr-*\n# install only german, french, english, spanish language packs\n# sudo apt install tesseract-ocr-deu tesseract-ocr-fra tesseract-ocr-eng tesseract-ocr-spa\n</code></pre>"},{"location":"readme_cp/#development","title":"Development","text":"<p>--&gt; see </p>"},{"location":"readme_cp/#license","title":"License","text":"<p>This project is licensed under the terms of MIT license.</p> <p>You can check the compatibility using the following tool in a venv environment in a production setting:</p> <pre><code>pip install pip-licenses\npip-licenses | grep -Ev 'MIT License|BSD License|Apache Software License|Python Software Foundation License|Apache 2.0|MIT|Apache License 2.0|hnswlib|Pillow|new BSD|BSD'\n</code></pre>"},{"location":"readme_cp/#dependencies","title":"Dependencies","text":"<p>Here is a list of Libraries, that this project is based on:</p> <p>list</p>"},{"location":"reference/","title":"Reference","text":""},{"location":"reference/#pydoxtools.document.Document","title":"<code>Document</code>","text":"<p>         Bases: <code>Pipeline</code></p> <p>Basic document pipeline class to analyze documents from all kinds of formats.</p> <p>A list and documentation of all document analysis related functions can be found -&gt;here&lt;-.</p> <p>The Document class is designed for information extraction from documents. It inherits from the pydoxtools.document_base.Pipeline class and uses a predefined extraction pipeline focused on document processing tasks. To load a document, create an instance of the Document class with a file path, a file object, a string, a URL or give it some data directly as a dict:</p> <pre><code>from pydoxtools import Document\ndoc = Document(fobj=Path('./data/demo.docx'))\n</code></pre> <p>Extracted data can be accessed by calling the <code>x</code> method with the specified output in the pipeline:</p> <pre><code>doc.x(\"addresses\")\ndoc.x(\"entities\")\ndoc.x(\"full_text\")\n# etc...\n</code></pre> <p>Most members can also be called as normal class attributes for easier readability:</p> <pre><code>doc.addresses\n</code></pre> <p>Additionally, it is possible to get the data directly in dict, yaml or json form:</p> <pre><code>doc.property_dict(\"addresses\",\"filename\",\"keywords\")\ndoc.yaml(\"addresses\",\"filename\",\"keywords\")\ndoc.json(\"addresses\",\"filename\",\"keywords\")\n</code></pre> <p>To retrieve a list of all available extraction data methods, call the <code>x_funcs()</code> method:</p> <pre><code>doc.x_funcs()\n</code></pre>"},{"location":"reference/#pydoxtools.document.Document--customizing-the-pipeline","title":"Customizing the Pipeline:","text":"<p>The extraction pipeline can be partially overwritten or completely replaced to customize the document processing. To customize the pipeline, it's recommended to use the basic document pipeline defined in <code>pydoxtools.Document</code> as a starting point and only overwrite parts as needed.</p> <p>Inherited classes can override any part of the graph. To exchange, override, extend or introduce extraction pipelines for specific file types (including the generic one: \"\"), such as .html, .pdf, .txt, etc., follow the example below.</p> <p>TODO: provide more information on how to customize the pipeline and override the graph.</p>"},{"location":"reference/#pydoxtools.document.Document--examples","title":"Examples","text":"<p>The following is an example extension pipeline for an OCR extractor that converts images into text and supports file types: \".png\", \".jpeg\", \".jpg\", \".tif\", \".tiff\":</p> <pre><code>\"image\": [\n        OCRExtractor()\n        .pipe(file=\"raw_content\")\n        .out(\"ocr_pdf_file\")\n        .cache(),\n    ],\n\".png\": [\"image\", \".pdf\"],\n\".jpeg\": [\"image\", \".pdf\"],\n\".jpg\": [\"image\", \".pdf\"],\n\".tif\": [\"image\", \".pdf\"],\n\".tiff\": [\"image\", \".pdf\"],\n\"*\": [...]\n</code></pre> <p>Each function (or node) in the extraction pipeline connects to other nodes in the pipeline through the \"pipe\" command. Arguments can be overwritten by a new pipeline in inherited documents or document types higher up in the hierarchy. The argument precedence is as follows:</p> <pre><code>python-class-member &lt; extractor-graph-function &lt; configuration\n</code></pre> <p>When creating a new pipeline for documentation purposes, use a function or class for complex operations and include the documentation there. Lambda functions should not be used in this case.</p>"},{"location":"reference/#pydoxtools.document.Document.document_type","title":"<code>document_type</code>  <code>property</code>","text":"<p>This has to be done in a member function and not in the pipeline, because the selection of the pipeline depends on this...</p>"},{"location":"reference/#pydoxtools.document.Document.filename","title":"<code>filename: str | None</code>  <code>property</code>","text":"<p>return filename or some other identifier of a file</p>"},{"location":"reference/#pydoxtools.document.Document.__init__","title":"<code>__init__(fobj=None, source=None, document_type='auto', page_numbers=None, max_pages=None)</code>","text":"<p>Initialize a Document instance.</p> <p>Either fobj or source are required. They can both be given. If either of them isn't specified the other one is inferred automatically.</p> <p>document_type, page_number and max_pages are also not required, but can be used to override the default behaviour. specifically document_type can be used manually specify the pipeline that should be used.</p> <p>Parameters:</p> Name Type Description Default <code>fobj</code> <code>str | bytes | Path | IO | dict | list | set</code> <p>The file object or data to load. Depending on the type of object passed: - If a string or bytes object: the object itself is the document. IN case of a bytes      object, the source helps in determining the filetype through file endings. - If a string representing a URL: the document will be loaded from the URL. - If a pathlib.Path object: load the document from the path. - If a file object: load the document from the file object (e.g., bytestream). - If a python dict object: interprete a \"dict\" as a document - If a python list object: interprete a \"list\" as a document</p> <code>None</code> <code>source</code> <code>str | Path</code> <p>The source of the extracted data (e.g., URL, 'pdfupload', parent-URL, or a path). source is given in addition to fobj it overrides the automatically inferred source. A special case applies if our document is a dataobject from a database. In that case the index key from the database should be used as source. This facilitates downstream tasks immensely where we have to refer back to where the data came from.</p> <p>This also applies for \"explode\" operations on documents where the newly created documents will all try to trace their origin using the \"source\" attribute</p> <code>None</code> <code>document_type</code> <code>str</code> <p>The document type to directly specify the pipeline to be used. If \"auto\" is given it will try to be inferred automatically. For example in some cases we would like to have a string given in fobj not to be loaded as a file but actually be used as raw \"string\" data. In this case we can explicitly specify document_type=\"string\"</p> <code>'auto'</code> <code>page_numbers</code> <code>list[int]</code> <p>A list of specific pages to extract from the document (e.g., in a PDF).</p> <code>None</code> <code>max_pages</code> <code>int</code> <p>The maximum number of pages to extract to protect resources.</p> <code>None</code>"},{"location":"reference/#pydoxtools.document.Document.__repr__","title":"<code>__repr__()</code>","text":"<p>Returns:</p> Name Type Description <code>str</code> <p>A string representation of the instance.</p>"},{"location":"reference/#pydoxtools.document.Document.document_type_detection","title":"<code>document_type_detection()</code>  <code>cached</code>","text":"<p>This one here is actually important as it detects the type of data that we are going to use for out pipeline. That is also why this is implemented as a member function and can not be pushed in the pipeline itself, because in needs to be run in order to select which pipline we are going to use.</p> <p>detect doc type based on various criteria TODO add a doc-type extractor using for example python-magic</p>"},{"location":"reference/#pydoxtools.document.DocumentBag","title":"<code>DocumentBag</code>","text":"<p>         Bases: <code>Pipeline</code></p> <p>This class is WIP use with caution</p> <p>This class loads an entire set of documents and processes it using a pipeline.</p> <p>In order to fit into memory we are making extensive use of dask bags to store and make calculations on documents.</p> <p>docs: is the property which gives access to all the dask bag functions</p> <p>For more information check the documentation for dask bags -&gt;here&lt;-.</p> <p>This has the added benefit that one can use dask dataframes out of the box for downstream calculations!!</p> <p>This class is still experimental. Expect more documentation in the near future.</p> <p>This class is developed to do work on larger-than-memory datasets and scale LLM &amp; AI inference to very large workloads.</p> <p>This class makes mostly use of iterative dask bags &amp; dataframes to avoid out-of-memory problems.</p> <p>Why do we need this function?</p> <p>--&gt; we could also simply add documents to a dask bag and then use dask bag directly...     ... BUT that would mean \"a lot\" more boilerplate code creating new documents,     creating source chains for traceable datasources etc  etc...     So the main reason is that we want new bags of documents instead of just bags     of arbitrary data.</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline","title":"<code>Pipeline</code>","text":"<p>Base class for all document classes in pydoxtools, defining a common pipeline interface and establishing a basic pipeline schema that derived classes can override.</p> <p>The MetaPipelineClassConfiguration acts as a compiler to resolve the pipeline hierarchy, allowing pipelines to inherit, mix, extend, or partially overwrite each other. Each key in the _pipelines dictionary represents a different pipeline version.</p> <p>The pydoxtools.Document class leverages this functionality to build separate pipelines for different file types, as the information processing requirements differ significantly between file types.</p> <p>Attributes:</p> Name Type Description <code>_operators</code> <code>dict[str, list[pydoxtools.document_base.Operator]]</code> <p>Stores the definition of the pipeline graph, a collection of connected operators/functions that process data from a document.</p> <code>_pipelines</code> <code>dict[str, dict[str, pydoxtools.document_base.Operator]]</code> <p>Provides access to all operator functions by their \"out-key\" which was defined in _operators.</p> Todo <ul> <li>Use pandera (https://github.com/unionai-oss/pandera) to validate dataframes   exchanged between extractors &amp; loaders   (https://pandera.readthedocs.io/en/stable/pydantic_integration.html)</li> </ul>"},{"location":"reference/#pydoxtools.document_base.Pipeline.configuration","title":"<code>configuration</code>  <code>property</code>","text":"<p>Returns a dictionary of all configuration objects for the current pipeline.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the names and values of all configuration objects   for the current pipeline.</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.pipeline_chooser","title":"<code>pipeline_chooser: str</code>  <code>property</code>","text":"<p>Must be implemented by derived classes to decide which pipeline they should use.</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.uuid","title":"<code>uuid</code>  <code>cached</code> <code>property</code>","text":"<p>Retrieves a universally unique identifier (UUID) for the instance.</p> <p>This method generates a new UUID for the instance using Python's <code>uuid.uuid4()</code> function. The UUID is then cached as a property, ensuring that the same UUID is returned for subsequent accesses.</p> <p>Returns:</p> Type Description <p>uuid.UUID: A unique identifier for the instance.</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.x_funcs","title":"<code>x_funcs: dict[str, Operator]</code>  <code>cached</code> <code>property</code>","text":"<p>get all operators/pipeline nodes and their property names for this specific file type/pipeline</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.__getattr__","title":"<code>__getattr__(extract_name)</code>","text":"<p>Retrieves an extractor result by directly accessing it as an attribute.</p> <p>This method is automatically called for attribute names that aren't defined on class level, allowing for a convenient way to access pipeline operator outputs without needing to call the 'x' method.</p> Example <p>document.addresses instead of document.x('addresses')</p> <p>Parameters:</p> Name Type Description Default <code>extract_name</code> <code>str</code> <p>The name of the extractor result to be accessed.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The result of the extractor after processing the document.</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.__getitem__","title":"<code>__getitem__(extract_name)</code>","text":"<p>Retrieves an extractor result by directly accessing it as an attribute.</p> <p>This method is automatically called for attribute names that aren't defined on class level, allowing for a convenient way to access pipeline operator outputs without needing to call the 'x' method.</p> Example <p>document[\"addresses\"] instead of document.x('addresses')</p> <p>Parameters:</p> Name Type Description Default <code>extract_name</code> <code>str</code> <p>The name of the extractor result to be accessed.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The result of the extractor after processing the document.</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.__getstate__","title":"<code>__getstate__()</code>","text":"<p>return necessary variables for pickling, ensuring that we leave out everything that can potentiall have some sort of a lambda function in it...</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.__init__","title":"<code>__init__()</code>","text":"<p>Initializes the Pipeline instance with cache-related attributes.</p> <p>Attributes:</p> Name Type Description <code>_cache_hits</code> <code>int</code> <p>Number of cache hits during pipeline execution.</p> <code>_x_func_cache</code> <code>dict[pydoxtools.document_base.Operator, dict[str, Any]]</code> <p>Cache for operator functions to store intermediate results.</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.__repr__","title":"<code>__repr__()</code>","text":"<p>Returns:</p> Name Type Description <code>str</code> <p>A string representation of the instance.</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.__setstate__","title":"<code>__setstate__(state)</code>","text":"<p>we need to restore _x_func_cache for pickling to work...</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.config","title":"<code>config(**configuration)</code>","text":"<p>Set configuration parameters for a pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>**configuration</code> <code>Any</code> <p>A dictionary of key-value pairs representing the configuration settings for the pipeline. Each key is a string representing the name of the configuration setting, and the value is the corresponding value to be set.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>self</code> <code>Pipeline</code> <p>A reference to the current pipeline instance, allowing for method chaining.</p> Example <p>pipeline = Pipeline() pipeline.config(param1=value1, param2=value2)</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.get_configuration_names","title":"<code>get_configuration_names(pipeline)</code>  <code>cached</code> <code>classmethod</code>","text":"<p>Returns a list of names of all configuration objects for a given pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline</code> <code>str</code> <p>The name of the pipeline to retrieve configuration objects from.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list[str]</code> <p>A list of strings containing the names of all configuration objects for the   given pipeline.</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.non_interactive_pipeline","title":"<code>non_interactive_pipeline()</code>","text":"<p>return all non-interactive extractors/pipeline nodes</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.pipeline_docs","title":"<code>pipeline_docs()</code>  <code>classmethod</code>","text":"<p>Returns a formatted string containing the documentation for each pipeline operation in the class.</p> <p>This class method iterates through the pipeline operations, collects information about their output types and supported pipelines, and formats the documentation accordingly.</p> <p>Returns:</p> Name Type Description <code>str</code> <p>A formatted string containing the documentation for each pipeline operation, including  operation name, usage, return type, and supported pipelines.</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.pipeline_graph","title":"<code>pipeline_graph(image_path=None, document_logic_id='*')</code>  <code>classmethod</code>","text":"<p>Generates a visualization of the defined pipelines and optionally saves it as an image.</p> <p>Parameters:</p> Name Type Description Default <code>image_path</code> <code>str | pathlib.Path</code> <p>File path for the generated image. If provided, the                                        generated graph will be saved as an image.</p> <code>None</code> <code>document_logic_id</code> <code>str</code> <p>The document logic ID for which the pipeline graph should                                be generated. Defaults to \"current\".</p> <code>'*'</code> <p>Returns:</p> Name Type Description <code>AGraph</code> <p>A PyGraphviz AGraph object representing the pipeline graph. This object can be     visualized or manipulated using PyGraphviz functions.</p> Notes <p>This method requires the NetworkX and PyGraphviz libraries to be installed.</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.pre_cache","title":"<code>pre_cache()</code>","text":"<p>Pre-caches the results of all extractors that have caching enabled.</p> <p>This method iterates through the defined extractors and calls each one with caching enabled, storing the results for faster access in future calls.</p> <p>Returns:</p> Name Type Description <code>self</code> <p>The instance of the class, allowing for method chaining.</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.run_pipeline","title":"<code>run_pipeline(exclude=None)</code>","text":"<p>Runs all extractors defined in the pipeline for testing or pre-caching purposes.</p> <p>!!IMPORTANT!!!  This function should normally not be used as the pipeline is lazily executed anyway.</p> <p>This method iterates through the defined extractors and calls each one, ensuring that the extractor logic is functioning correctly and caching the results if required.</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.run_pipeline_fast","title":"<code>run_pipeline_fast()</code>","text":"<p>run pipeline, but exclude long-running calculations</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.to_dict","title":"<code>to_dict(*args, **kwargs)</code>","text":"<p>Returns a dictionary that accumulates the properties given in args or with a mapping in *kwargs.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>str</code> <p>A variable number of strings, each representing a property name.</p> <code>()</code> <code>**kwargs</code> <code>dict</code> <p>A dictionary mapping property names (values) to custom keys (keys) for the              returned dictionary.</p> <code>{}</code> Note <p>This function currently only supports properties that do not require any arguments, such as \"full_text\". Properties like \"answers\" that return a function requiring arguments cannot be used with this function.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary with the accumulated properties and their values, using either the   property names or custom keys as specified in the input arguments.</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.to_json","title":"<code>to_json(*args, **kwargs)</code>","text":"<p>Returns a dictionary that accumulates the properties given in args or with a mapping in *kwargs, and dumps the output as JSON.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>str</code> <p>A variable number of strings, each representing a property name.</p> <code>()</code> <code>**kwargs</code> <code>dict</code> <p>A dictionary mapping property names (values) to custom keys (keys) for the              returned dictionary.</p> <code>{}</code> Note <p>This function currently only supports properties that do not require any arguments, such as \"full_text\". Properties like \"answers\" that return a function requiring arguments cannot be used with this function.</p> <p>Returns:</p> Name Type Description <code>str</code> <p>A JSON-formatted string representing the accumulated properties and their values, using  either the property names or custom keys as specified in the input arguments.</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.to_yaml","title":"<code>to_yaml(*args, **kwargs)</code>","text":"<p>Returns a dictionary that accumulates the properties given in args or with a mapping in *kwargs, and dumps the output as YAML.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>str</code> <p>A variable number of strings, each representing a property name.</p> <code>()</code> <code>**kwargs</code> <code>dict</code> <p>A dictionary mapping property names (values) to custom keys (keys) for the              returned dictionary.</p> <code>{}</code> Note <p>This function currently only supports properties that do not require any arguments, such as \"full_text\". Properties like \"answers\" that return a function requiring arguments cannot be used with this function.</p> <p>Returns:</p> Name Type Description <code>str</code> <p>A YAML-formatted string representing the accumulated properties and their values, using  either the property names or custom keys as specified in the input arguments.</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.x","title":"<code>x(operator_name)</code>","text":"<p>Calls an extractor from the defined pipeline and returns the result.</p> <p>Parameters:</p> Name Type Description Default <code>operator_name</code> <code>str</code> <p>The name of the extractor to be called.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The result of the extractor after processing the document.</p> <p>Raises:</p> Type Description <code>operators.OperatorException</code> <p>If an error occurs while executing the extractor.</p> Notes <p>The extractor's parameters can be overridden using args and *kwargs.</p>"},{"location":"reference/#pydoxtools.document_base.Pipeline.x_all","title":"<code>x_all()</code>","text":"<p>Retrieves the results of all extractors defined in the pipeline.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the results of all extractors, with keys as the extractor   names and values as the corresponding results.</p>"},{"location":"reference/#pydoxtools.operators","title":"<code>pydoxtools.operators</code>","text":"<p>This module simply gathers all operators from across the board to make them easier to access.</p>"}]}