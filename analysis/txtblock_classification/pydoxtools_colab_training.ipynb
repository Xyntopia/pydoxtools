{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gfrmJvSONmBS"
   },
   "source": [
    "synchronize your pydoxtools directory:\n",
    "\n",
    "- rclone sync pydoxtools/ xyntopia_gdrive:/pydoxtools -P --size-only --fast-list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LzWIvGjIWeOo"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#import os\n",
    "#assert os.environ['COLAB_TPU_ADDR'], 'Make sure to select TPU from Edit > Notebook settings > Hardware accelerator'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a1uyZN6_WHiA"
   },
   "outputs": [],
   "source": [
    "#import torch\n",
    "#import torch_xla\n",
    "#import torch_xla.core.xla_model as xm\n",
    "#\n",
    "#t = torch.randn(2, 2, device=xm.xla_device())\n",
    "#print(t.device)\n",
    "#print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OfHZ2kprTjuE"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IpfYEVIhZ6he"
   },
   "source": [
    "## setup pydoxtools code access, data & gdrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7dgIb3slMjOs"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "roTdjgU1_DD3"
   },
   "outputs": [],
   "source": [
    "PDX_DIR=\"/content/pydoxtools\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hrwTZAVnguJk"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install pytorch-lightning faker evaluate transformers datasets accelerate nvidia-ml-py3 sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sljq7wNmMpDh"
   },
   "outputs": [],
   "source": [
    "!ls \"/content/gdrive/My Drive/pydoxtools/pydoxtools\"\n",
    "!rm -r $PDX_DIR\n",
    "!cp -r \"/content/gdrive/My Drive/pydoxtools/pydoxtools\" $PDX_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TfzfmVfq1jVZ"
   },
   "source": [
    "The main issue when using google colab for our training is, that we only have python version 3.9 available, while pydoxtools currently needs 3.10 to function.and\n",
    "\n",
    "because of this we are trying to only use the relevant parts of the library which is mainly the data generation part which is used for\n",
    "the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M1mr_xOn2fEK"
   },
   "outputs": [],
   "source": [
    "!rm $PDX_DIR/__init__.py\n",
    "!touch $PDX_DIR/__init__.py\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_TRbPGU3RaTD"
   },
   "source": [
    "finally...  run pydoxtools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HbAWiShuTxHp"
   },
   "outputs": [],
   "source": [
    "# make sure we set the environment variable before loading pydoxtools for the first time\n",
    "%env TRAINING_DATA_DIR=/content/gdrive/MyDrive/pydoxtools/training_data\n",
    "from pydoxtools import random_data_generators, training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m0ircFG6WaHg"
   },
   "source": [
    "## prepare training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ghNUEyG04OB7"
   },
   "outputs": [],
   "source": [
    "bg = random_data_generators.TextBlockGenerator.std_generator()\n",
    "bg.classmap, bg.classmap_inv, bg.num_generators, bg.class_gen, bg.gen_mapping, bg.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EpjrS5FKZ2bX"
   },
   "outputs": [],
   "source": [
    "bg.gen_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6RSpKOEaqh5W"
   },
   "outputs": [],
   "source": [
    "bg.single(1000, convert_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q4OnANpHZiKP"
   },
   "outputs": [],
   "source": [
    "df,y = training.load_labeled_text_block_data(classmap=bg.classmap_inv)\n",
    "df[\"label\"]=y\n",
    "df = df.rename(columns={\"txt\":\"text\"}).drop(columns=\"filename\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "72JsDSeaci9f"
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BpEZgTBrm9HD"
   },
   "outputs": [],
   "source": [
    "df.label.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AbR78dTJ3tFd"
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset, Features, Value, ClassLabel\n",
    "import random\n",
    "\n",
    "dataset_size=200000\n",
    "\n",
    "# generate datasets for finetuning\n",
    "def my_gen():\n",
    "    seed=random.randint(0,10000000)\n",
    "    for i in range(1, dataset_size):\n",
    "        text, label = bg.single(i+seed, convert_labels=True)\n",
    "        yield {\"label\": label, \"text\":text}\n",
    "\n",
    "class_names = [\"address\", \"unknown\"]\n",
    "features = Features({'text': Value('string'), 'label': ClassLabel(names=class_names)})\n",
    "\n",
    "dataset = Dataset.from_generator(my_gen, features=features)\n",
    "dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rxKppLhadEKU"
   },
   "outputs": [],
   "source": [
    "val_dataset = Dataset.from_pandas(df,features=features) # this will already convert our labels!\n",
    "val_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PrHf7iWiRx5G"
   },
   "outputs": [],
   "source": [
    "dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HARgcFnOkE9d"
   },
   "outputs": [],
   "source": [
    "dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bgsenNBIh67I"
   },
   "outputs": [],
   "source": [
    "model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "# model_name=\"bert-base-cased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eQ8f2fOXRDV4"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "def tokenize_function(batch):\n",
    "    tokenized_batch = tokenizer(batch['text'], padding=\"max_length\", truncation=True)\n",
    "    #print(batch[\"label\"])\n",
    "    #tokenized_batch[\"labels\"] = [bg.classmap_inv[label] for label in batch[\"label\"]]\n",
    "    return tokenized_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "37qmK5At5cjb"
   },
   "outputs": [],
   "source": [
    "train_dataset = dataset.map(tokenize_function, batched=True)\n",
    "validation_dataset = val_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "phUX3LpERmYY"
   },
   "outputs": [],
   "source": [
    "# select smaller subdataset\n",
    "#train_dataset = tokenized_datasets.shuffle(seed=42).select(range(1000))\n",
    "#eval_dataset = tokenized_datasets.shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YTsm0q7EqxLb"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "56kmpOwDrABc"
   },
   "outputs": [],
   "source": [
    "#model = AutoModelForSequenceClassification.from_pretrained(, num_labels=2).to(\"cuda\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EvoSpUGZta9D"
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HQm0-7u_tmKQ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dyUgKsiEttmG"
   },
   "outputs": [],
   "source": [
    "f1_metric = evaluate.load(\"f1\")\n",
    "acc = evaluate.load(\"accuracy\")\n",
    "rec = evaluate.load(\"recall\")\n",
    "prec = evaluate.load(\"precision\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Jdo6tr6twOp"
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "79GZRfIIumLS"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        **f1_metric.compute(predictions=predictions, references=labels),\n",
    "        **acc.compute(predictions=predictions, references=labels),\n",
    "        **rec.compute(predictions=predictions, references=labels),\n",
    "        **prec.compute(predictions=predictions, references=labels)\n",
    "    }\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    metrics = classification_report(predictions, labels, output_dict=True)\n",
    "    label=str(bg.classmap_inv['address'])\n",
    "    return {\n",
    "        \"address.f1\":metrics[label][\"f1-score\"],\n",
    "        \"accuracy\":metrics[\"accuracy\"],\n",
    "        \"address.precision\":metrics[label][\"precision\"],\n",
    "        \"address.recall\":metrics[label][\"recall\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XpEFOn2waQ8W"
   },
   "outputs": [],
   "source": [
    "#!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WPmDvctuYpsF"
   },
   "source": [
    "## run actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HgSjn496WWmG"
   },
   "outputs": [],
   "source": [
    "#metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fzzadAstajcu"
   },
   "outputs": [],
   "source": [
    "default_args = {\n",
    "    \"output_dir\": \"/content/gdrive/MyDrive/models/txtblock\",\n",
    "    \"evaluation_strategy\": \"steps\",\n",
    "    \"num_train_epochs\": 10,\n",
    "    \"log_level\": \"info\",\n",
    "    #\"label_names\": \"label\",\n",
    "    \"logging_strategy\": \"steps\",\n",
    "    \"logging_steps\": 200, \n",
    "    \"save_strategy\":\"steps\",\n",
    "    \"save_steps\": 200,\n",
    "    \"report_to\": \"none\",\n",
    "    \"metric_for_best_model\":\"address.f1\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9P5rbXIPuuWC"
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=44,\n",
    "    #gradient_accumulation_steps=4,\n",
    "    #gradient_checkpointing=True,\n",
    "    **default_args)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=validation_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "#result = trainer.train()\n",
    "#print_summary(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YDR8f0pLicyp"
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "64eATEegt2gh"
   },
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oliXF-iHbQAs"
   },
   "outputs": [],
   "source": [
    "#%load_ext tensorboard\n",
    "#%tensorboard --logdir logs"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
